<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="选择大于努力">
<meta property="og:type" content="website">
<meta property="og:title" content="凯_kaiii">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="凯_kaiii">
<meta property="og:description" content="选择大于努力">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="凯">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>凯_kaiii</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">凯_kaiii</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">暂无</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/" class="post-title-link" itemprop="url">目标重识别综述阅读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-23 20:48:09 / 修改时间：20:53:57" itemprop="dateCreated datePublished" datetime="2023-04-23T20:48:09+08:00">2023-04-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="目标重识别论文阅读笔记"><a href="#目标重识别论文阅读笔记" class="headerlink" title="目标重识别论文阅读笔记"></a>目标重识别论文阅读笔记</h1><h2 id="Deep-Learning-for-Person-Re-identification-A-Survey-and-Outlook"><a href="#Deep-Learning-for-Person-Re-identification-A-Survey-and-Outlook" class="headerlink" title="Deep Learning for Person Re-identification: A Survey and Outlook"></a>Deep Learning for Person Re-identification: A Survey and Outlook</h2><h3 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h3><p>行人重识别（以下简称reid）问题是在没有重叠场景的摄像机拍摄画面下，对目标行人进行检索。</p>
<p>现阶段的reid问题主要分为两大类：closed-world和open-world。说人话就是，closed-world重在研究，在各种面向研究的假设的基础上进行研究，主要是从一大堆行人的bounding box图片中去检索目标行人，而open-world重在“落地”，主要是直接从视频中去检索目标行人，或者是偏向无监督、弱监督学习。</p>
<h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><p><strong>不同视角、参差不齐的低分辨率图像、光照变化、姿态不同、遮挡情况、异构数据、复杂的相机环境、背景环境、不可靠的边缘框生成</strong>都会对ReID任务造成影响和挑战。实际部署时，摄像头的变化、Gallery十分巨大、数据要求高、对网络的泛化能力要求高、外表特征的变化等也是影响很大的因素。</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li><strong>原始数据收集</strong>：从处于不同环境的不同地方的摄像机获取原始视频数据。这些数据包含大量的背景杂波。</li>
<li><strong>边界框（Bounding Box）生成</strong>：通过行人检测或跟踪算法从原始视频数据中提取包含行人图像的边界框。在大规模应用中不可能手动裁剪所有行人图像。</li>
<li><strong>训练数据标注</strong>：对于区分行人任务来说，图像标注必不可少。</li>
<li><strong>模型构建和训练</strong>：已经开发了广泛运用的模型，重点在于特征表示学习、度量学习或两者结合。</li>
<li><strong>测试阶段</strong>：给定一个query和一组gallery，使用上一阶段训练完毕的模型进行行人特征提取，计算query图像和gallery图像的相似度进行排序。</li>
</ol>
<p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQmFsYWJvbw==,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="img"></p>
<p>即closed-world和open-world ReID之间的区别可总结如下：</p>
<ul>
<li><strong>单模态和异构数据</strong></li>
<li><strong>边界框生成和原始图像/视频</strong></li>
<li><strong>丰富的标签数据和不可用/有限的标签</strong></li>
<li><strong>正确标签和噪声标签</strong></li>
<li><strong>query是否存在于gallery中</strong></li>
</ul>
<h3 id="closed-world-ReID介绍以及方法总览"><a href="#closed-world-ReID介绍以及方法总览" class="headerlink" title="closed-world ReID介绍以及方法总览"></a>closed-world ReID介绍以及方法总览</h3><h4 id="closed-wrold假设"><a href="#closed-wrold假设" class="headerlink" title="closed-wrold假设"></a>closed-wrold假设</h4><ul>
<li>通过单模态可见光摄像机捕获行人</li>
<li>已经给出行人bounding box</li>
<li>有足够的标注好的训练数据。用于监督训练</li>
<li>标签通常是正确的</li>
<li>query行人必须出现在图库中</li>
</ul>
<h4 id="特征表示学习"><a href="#特征表示学习" class="headerlink" title="特征表示学习"></a>特征表示学习</h4><h4 id="全局表征学习"><a href="#全局表征学习" class="headerlink" title="全局表征学习"></a>全局表征学习</h4><p>从每个人的图像中提取特征向量，直接将行人图片送入网络进行特征的提取。</p>
<h4 id="局部表征学习"><a href="#局部表征学习" class="headerlink" title="局部表征学习"></a>局部表征学习</h4><p>将行人的图片进行分块，使用网络对每一个块进行特征提取，最后将所有的特征结合起来</p>
<h4 id="辅助表征学习"><a href="#辅助表征学习" class="headerlink" title="辅助表征学习"></a>辅助表征学习</h4><p>在网络中加入一些辅助性对目标进行描述的元素，例如外观描述，视角描述、区域信息等。</p>
<h4 id="基于视频的表征学习"><a href="#基于视频的表征学习" class="headerlink" title="基于视频的表征学习"></a>基于视频的表征学习</h4><p>输入为由多张图片组成的行人的视频序列，其具有丰富的外表和时域信息。</p>
<p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/image-20230309134909484.png" alt="image-20230309134909484"></p>
<h4 id="度量学习"><a href="#度量学习" class="headerlink" title="度量学习"></a>度量学习</h4><p>度量学习目前的主要工作集中以及体现于特征学习中的loss函数的设计，目前最常用的三种loss为：<strong>identity loss</strong>、<strong>verification loss</strong>、<strong>triplet loss</strong>以及其的变种。</p>
<p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/image-20230309135813177.png" alt="image-20230309135813177" style="zoom:50%;"></p>
<h5 id="identity-loss"><a href="#identity-loss" class="headerlink" title="identity loss"></a>identity loss</h5><p>将行人重识别的训练过程视为图像分类问题，将每个人视作一个独立的类别，通过类比于图像分类的方式进行重识别。这种方式其在训练过程中能较为容易训练和自动挖掘困难样本</p>
<p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/20200710212437448.png" alt="在这里插入图片描述"></p>
<h5 id="verification-loss"><a href="#verification-loss" class="headerlink" title="verification loss"></a>verification loss</h5><p>用对比损失函数或者二元损失函数来优化成对样本间关联。对比损失函数提升了成对样本距离比较，即为学习使不同类别的图像对应的特征相距较远</p>
<p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/202007102124402.png" alt="在这里插入图片描述"></p>
<p>或</p>
<p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/20200710212501363.png" alt="在这里插入图片描述"></p>
<h5 id="triplet-loss"><a href="#triplet-loss" class="headerlink" title="triplet loss"></a>triplet loss</h5><p>将ReID问题看作是检索排序问题，其主要思想可以看作同一个样本之间的距离应该小于不同的样本之间的距离</p>
<p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/2020071021245875.png" alt="在这里插入图片描述"></p>
<h4 id="数据集和评价指标"><a href="#数据集和评价指标" class="headerlink" title="数据集和评价指标"></a>数据集和评价指标</h4><p><img src="/2023/04/23/%E7%9B%AE%E6%A0%87%E9%87%8D%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E9%98%85%E8%AF%BB/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQmFsYWJvbw==,size_20,color_FFFFFF,t_70,g_se,x_16-16783434167663.png" alt="img"></p>
<h4 id="SOTA-方法解析"><a href="#SOTA-方法解析" class="headerlink" title="SOTA 方法解析"></a>SOTA 方法解析</h4><h5 id="基于图像的ReID"><a href="#基于图像的ReID" class="headerlink" title="基于图像的ReID"></a>基于图像的ReID</h5><h6 id="VAL：引入视角信息"><a href="#VAL：引入视角信息" class="headerlink" title="VAL：引入视角信息"></a>VAL：引入视角信息</h6><p>目前通过神经网络的目标重识别的识别能力已经高于人工辨识的准确度，sota数据中常常使用目标的全局特征结合局部特征进行融合，从而达到更好的效果</p>
<p>文章强调注意力机制的有效性，多损失训练的有效性</p>
<h5 id="基于视频的ReID"><a href="#基于视频的ReID" class="headerlink" title="基于视频的ReID"></a>基于视频的ReID</h5><p>时空建模对提取视频特征是十分重要的，其中包含跨多帧的注意力机制，甚至利用视频序列中的多帧可以填补被遮挡的部份。</p>
<h3 id="Open-world-ReID"><a href="#Open-world-ReID" class="headerlink" title="Open-world ReID"></a>Open-world ReID</h3><h5 id="Depth-based-Re-ID：捕获人体形状和骨骼状态，提供在光照差别大、换衣服情况下的重识别的解决方案。"><a href="#Depth-based-Re-ID：捕获人体形状和骨骼状态，提供在光照差别大、换衣服情况下的重识别的解决方案。" class="headerlink" title="Depth-based Re-ID：捕获人体形状和骨骼状态，提供在光照差别大、换衣服情况下的重识别的解决方案。"></a>Depth-based Re-ID：捕获人体形状和骨骼状态，提供在光照差别大、换衣服情况下的重识别的解决方案。</h5><h5 id="Text-To-Image-ReID：解决在语言描述和RGB图像之间的匹配上的问题，用一段语言描述来代替对目标的文字描述"><a href="#Text-To-Image-ReID：解决在语言描述和RGB图像之间的匹配上的问题，用一段语言描述来代替对目标的文字描述" class="headerlink" title="Text-To-Image ReID：解决在语言描述和RGB图像之间的匹配上的问题，用一段语言描述来代替对目标的文字描述"></a>Text-To-Image ReID：解决在语言描述和RGB图像之间的匹配上的问题，用一段语言描述来代替对目标的文字描述</h5><h5 id="Visible-Infrared-Re-ID：处理在白天可视化图片和夜晚红外图片之间的跨模态匹配问题，解决低光照问题"><a href="#Visible-Infrared-Re-ID：处理在白天可视化图片和夜晚红外图片之间的跨模态匹配问题，解决低光照问题" class="headerlink" title="Visible-Infrared Re-ID：处理在白天可视化图片和夜晚红外图片之间的跨模态匹配问题，解决低光照问题"></a>Visible-Infrared Re-ID：处理在白天可视化图片和夜晚红外图片之间的跨模态匹配问题，解决低光照问题</h5><h5 id="Cross-Resolution-Re-ID：跨分辨率的ReID在低分辨率图片和高分辨率图片中进行匹配，处理大分辨率的变化问题"><a href="#Cross-Resolution-Re-ID：跨分辨率的ReID在低分辨率图片和高分辨率图片中进行匹配，处理大分辨率的变化问题" class="headerlink" title="Cross-Resolution Re-ID：跨分辨率的ReID在低分辨率图片和高分辨率图片中进行匹配，处理大分辨率的变化问题"></a>Cross-Resolution Re-ID：跨分辨率的ReID在低分辨率图片和高分辨率图片中进行匹配，处理大分辨率的变化问题</h5><h4 id="End-to-End-ReID"><a href="#End-to-End-ReID" class="headerlink" title="End-to-End ReID"></a>End-to-End ReID</h4><p>端到端的ReID减缓了对边缘框的需求问题，直接利用原始的视频信息、图像信息进行计算，得出对应的目标ID在视频中的位置</p>
<h4 id="ReID-in-Raw-Images-Videos"><a href="#ReID-in-Raw-Images-Videos" class="headerlink" title="ReID in Raw Images/Videos"></a>ReID in Raw Images/Videos</h4><p>该任务需要同一个模型同时完成人物检测和ReID任务，由于两个主要部件的侧重点有所不同，所以是一个有挑战性的任务</p>
<h4 id="Multi-camera-Tracking"><a href="#Multi-camera-Tracking" class="headerlink" title="Multi-camera Tracking"></a>Multi-camera Tracking</h4><p>该任务与MTMCT（multi-person, multi-camera tracking）近似，可根据基于图的连接、多目标多摄像机跟踪与重识别之间的相关性进行优化解决。</p>
<h4 id="Semi-supervised-and-Unsupervised-Re-ID"><a href="#Semi-supervised-and-Unsupervised-Re-ID" class="headerlink" title="Semi-supervised and Unsupervised Re-ID"></a>Semi-supervised and Unsupervised Re-ID</h4><h4 id="Noise-Robust-Re-ID"><a href="#Noise-Robust-Re-ID" class="headerlink" title="Noise-Robust Re-ID"></a>Noise-Robust Re-ID</h4><h4 id="Open-set-Re-ID-and-Beyond"><a href="#Open-set-Re-ID-and-Beyond" class="headerlink" title="Open-set Re-ID and Beyond"></a>Open-set Re-ID and Beyond</h4><p>Open-set ReID通常被视为目标验证问题，辨别两个人员图像是否属于同一个目标。对于该问题，Adversarial PersonNet (APN) 共同学习GAN模块和Re-ID特征提取器。然而该问题依旧有非常大的提升空间，例如更高的识别率和更低的错误率。</p>
<h5 id="Re-ID组"><a href="#Re-ID组" class="headerlink" title="Re-ID组"></a>Re-ID组</h5><p>它的目的是将人以群体而不是个人的形式联系起来。早期的研究主要集中在利用稀疏字典学习或协方差描述子聚集进行组表示提取。最近，应用图卷积网络，将群表示为图。在端到端人搜索和个体再识别中也应用了群体相似性来提高准确性。然而，群体Re-ID仍然具有挑战性，因为群体变异比个体更复杂。</p>
<h5 id="动态多摄像机网络"><a href="#动态多摄像机网络" class="headerlink" title="动态多摄像机网络"></a>动态多摄像机网络</h5><p>动态更新多摄像机网络是另一个具有挑战性的问题，需要对新的摄像机或探头进行模型适配。引入一种人在循环增量学习方法来更新Re-ID模型，适应不同探测库的表示。早期的研究也将主动学习应用于多摄像头网络的连续Re-ID。引入了一种基于稀疏非冗余代表选择的连续自适应方法。设计了一种传递推理算法来开发基于测地线流核的最佳源摄像机模型。密集人群和社会关系中的多种环境约束(如摄像机拓扑)被集成到开放世界的人Re-ID系统中。在实际的动态多摄像机网络中，摄像机的模型自适应和环境因素是至关重要的。此外，如何将深度学习技术应用于动态多摄像机网络的研究还较少。</p>
<h3 id="对ReID技术的总览和展望"><a href="#对ReID技术的总览和展望" class="headerlink" title="对ReID技术的总览和展望"></a>对ReID技术的总览和展望</h3><h4 id="mINP-A-New-Evaluation-Metric-for-Re-ID"><a href="#mINP-A-New-Evaluation-Metric-for-Re-ID" class="headerlink" title="mINP: A New Evaluation Metric for Re-ID"></a>mINP: A New Evaluation Metric for Re-ID</h4><h4 id="单-跨模态重新识别的新基线-AGW"><a href="#单-跨模态重新识别的新基线-AGW" class="headerlink" title="单/跨模态重新识别的新基线 AGW"></a>单/跨模态重新识别的新基线 AGW</h4><h4 id="尚未调查的未决问题"><a href="#尚未调查的未决问题" class="headerlink" title="尚未调查的未决问题"></a>尚未调查的未决问题</h4><p>Open-set Re-ID、overlapping camera、same time、based on video </p>
<h2 id="Person-Re-identification-A-Retrospective-on-Domain-Specific"><a href="#Person-Re-identification-A-Retrospective-on-Domain-Specific" class="headerlink" title="Person Re-identification A Retrospective on Domain Specific"></a>Person Re-identification A Retrospective on Domain Specific</h2><p>Re-ID的应用场景：智能视频监控、机器人、人机交互、自动视觉监视系统等</p>
<p>Re-ID遇到的问题：遮挡、位姿方差、背景杂波、不对中、尺度差异、照明方差、视点方差、低分辨率和跨域或泛化。</p>
<p>该文从遮挡、位姿方差、背景杂波等六个方面总结了在该领域上做得最好的CNN、Attention、Self-Attention的论文。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-23 20:45:36 / 修改时间：20:53:44" itemprop="dateCreated datePublished" datetime="2023-04-23T20:45:36+08:00">2023-04-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="AGW-A-New-Baseline-for-Single-Cross-Modality-Re-ID"><a href="#AGW-A-New-Baseline-for-Single-Cross-Modality-Re-ID" class="headerlink" title="AGW: A New Baseline for Single-/Cross-Modality Re-ID"></a>AGW: A New Baseline for Single-/Cross-Modality Re-ID</h1><h1 id="AGW-A-New-Baseline-for-Single-Cross-Modality-Re-ID-1"><a href="#AGW-A-New-Baseline-for-Single-Cross-Modality-Re-ID-1" class="headerlink" title="AGW: A New Baseline for Single-/Cross-Modality Re-ID"></a>AGW: A New Baseline for Single-/Cross-Modality Re-ID</h1><p>其为综述Deep Learning for Person Re-identification:A Survey and Outlook 中提出的方法</p>
<p> AGW是在BagTricks的基础之上进行设计研究的，其主要包括以下三个主要的提升组件：</p>
<ul>
<li>Non-local Attention (Att) Block</li>
<li>Generalized-mean (GeM) Pooling.</li>
<li>Weighted Regularization Triplet (WRT) loss</li>
</ul>
<h2 id="Non-local-Attention-Att-Block"><a href="#Non-local-Attention-Att-Block" class="headerlink" title="Non-local Attention (Att) Block"></a>Non-local Attention (Att) Block</h2><p> 注意力的概念在ReID的学习中起到至关重要的作用，使用强大的非局部注意力块来获得各个位置特征的加权和。公式如下：$z_i = W_z ∗ φ(x_i) + x_i $，其中$W_z$是需要学习的权重矩阵，$φ()$表示非局部的操作，$+x_i$构建了一个残差策略。详情参见《Non-local neural networks》</p>
<h2 id="Generalized-mean-GeM-Pooling"><a href="#Generalized-mean-GeM-Pooling" class="headerlink" title="Generalized-mean (GeM) Pooling."></a>Generalized-mean (GeM) Pooling.</h2><p>ReID任务可视为细粒度的实例检索，广泛使用的max-pooling或average-pooling无法捕获领域特定的鉴别特征。所以针对该问题采用可学习的池化层，称为Generalized-mean (GeM) Pooling，公式如下:</p>
<p><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/image-20230418144447925.png" alt="image-20230418144447925"></p>
<p>$p_k$是一个池化超参数，可以在反向传播过程中学习，$p_k→∞$时近似最大池化，在$p_k = 1$时近似平均池化。详情参见《Fine-tuning cnn image retrieval with no human annotation》。可视为在最低维度上，对每个元素的p次方求均值再开p次方。</p>
<h2 id="Weighted-Regularization-Triplet-WRT-loss"><a href="#Weighted-Regularization-Triplet-WRT-loss" class="headerlink" title="Weighted Regularization Triplet (WRT) loss"></a>Weighted Regularization Triplet (WRT) loss</h2><p>除了使用基于softmax的交叉熵之外，还使用了另一个加权正则化三元组损失。<br><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/20201026220254980.png" alt="在这里插入图片描述"><br>避免引入了margin参数，类似于《Multi-similarity loss with general pair weighting for deep metric learning》</p>
<h2 id="完整流程如下所示"><a href="#完整流程如下所示" class="headerlink" title="完整流程如下所示"></a>完整流程如下所示</h2><p><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70#pic_center.png" alt="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70"></p>
<p><strong>AGW在跨模态行人重识别中的效果：</strong><br><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70#pic_center-16818009779205.png" alt="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/" class="post-title-link" itemprop="url">Bag of Tricks and A Strong Baseline for Deep Person Re-identification.md</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-23 20:45:36 / 修改时间：20:51:25" itemprop="dateCreated datePublished" datetime="2023-04-23T20:45:36+08:00">2023-04-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Bag-of-Tricks-and-A-Strong-Baseline-for-Deep-Person-Re-identification"><a href="#Bag-of-Tricks-and-A-Strong-Baseline-for-Deep-Person-Re-identification" class="headerlink" title="Bag of Tricks and A Strong Baseline for Deep Person Re-identification"></a>Bag of Tricks and A Strong Baseline for Deep Person Re-identification</h1><p>针对的问题：目前先进的基于深度神经网络的人员重识别技术设计了复杂的网络结构和串联多分支特征。</p>
<p>本文收集并评估了一些有效的训练技巧，通过对技巧的结合，模型仅使用全局特征即达到在Market1501上95.4％的rank-1。</p>
<p>作者认为，一个算法的baseline是十分重要的，对发表在顶会上的算法的baseline进行调查之后发现，顶级会议文章所选用的baseline效果大都较差。因此，作者使用一些训练策略更改了baseline。</p>
<p>本文的研究目的总结如下：</p>
<ul>
<li>调查了许多发表在顶级会议上的作品，发现其中大多数都是在糟糕的baseline上扩展的</li>
<li>对于学术界，我们希望为研究人员提供一个强有力的基线，以实现更高的准确性。</li>
<li>对于社区，我们希望给评论者一些参考，什么技巧会影响ReID模型的性能。我们建议，在比较不同方法的性能时，评审人员需要考虑这些技巧。</li>
<li>对于行业来说，我们希望提供一些有效的技巧，在不消耗太多额外的情况下获得更好的模型</li>
</ul>
<p>本文研究了六个技巧，使准确率在Market1501上达到了94.5％的rank1和85.9％的mAP，本文的主要贡献如下：</p>
<ul>
<li><p>收集了一些有效的训练技巧并设计了一种新型颈结构，命名为BNNeck。且两个广泛使用的数据集上评估了每个技巧的改进。</p>
</li>
<li><p>我们提供了强大的ReID基线。值得一提的是，该结果是利用ResNet50骨干网提供的全局特征获得的。据我们所知，这是全局特性在亲自ReID中获得的最佳性能。</p>
</li>
<li><p>作为补充，我们评估了图像大小和批量大小的数量对ReID模型性能的影响。</p>
</li>
</ul>
<h2 id="标准-Re-ID-baseline"><a href="#标准-Re-ID-baseline" class="headerlink" title="标准 Re-ID baseline"></a>标准 Re-ID baseline</h2><ol>
<li><p>在ImageNet上使用预训练的参数初始化ResNet50，并将全连接层的维数更改为N。N表示训练数据集中的身份数。</p>
</li>
<li><p>我们随机抽取每个人的P个身份和K张图像，构成一个训练批次。最后批大小为B = P×K。在本文中，我们设P = 16, K = 4。</p>
</li>
<li><p>我们将每张图像调整为256 × 128像素，并将调整后的图像填充为10个零值像素。然后随机裁剪成256 × 128的矩形图像。</p>
</li>
<li><p>每幅图像以0.5概率水平翻转。</p>
</li>
<li><p>每张图像解码为[0,1]中的32位浮点原始像素值。然后分别减去0.485,0.456,0.406，除以0.229,0.224,0.225，归一化RGB通道。</p>
</li>
<li><p>该模型输出ReID特征f和ID预测logits p。</p>
</li>
<li><p>ReID特征f用于计算triplet loss。ID预测logits p用于计算交叉熵损失。triplet loss的边际m设置为0.3。</p>
</li>
<li><p>采用Adam方法对模型进行优化。初始学习率设置为0.00035，在第40 epoch和第70 epoch分别降低到初始学习率的0.1。总共有120个训练阶段。</p>
</li>
</ol>
<h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><p><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230417202016054.png" alt="image-20230417202016054"></p>
<p>使用的训练技巧如下所示：</p>
<ul>
<li><code>Warmup Learning Rate</code>：学习率对模型的性能表现有很大的影响。在实践中，如下所示，使用10个epoch线性增加学习速率，从$3.5\times10^{-5}$到$3.5\times10^{-4}$。在第40 epoch和第70 epoch，学习率分别衰减到$3.5\times10^{-5}$和$3.5\times10^{-6}$。即第t时代的学习率lr(t)计算为:</li>
</ul>
<p><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230417202745696.png" alt="image-20230417202745696"></p>
<p><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230417202823803.png" alt="image-20230417202823803"></p>
<ul>
<li><p><code>Random Erasing Augmentation</code>:在ReID任务中，图片中的人常被其他物体遮挡，为解决该问题并提高系统的泛化性，使用随机擦除增强方案。在实际使用中，对于mini-batch中的图片I，其被随机擦除的概率为$p_e$，即保持不变的概率为$1-p_e$，REA在图片$I$中随机选择尺寸大小为$(W_e,H_e)$的矩形区域$I_e$，并将其填充为随机的数值。假设图像I和区域$I_e$的面积分别为$S = W × H$和$S_e = W_e × H_e$，使用$r_e = S_e/S$为擦除矩形区域的面积比。此外，区域$I_e$的纵横比在$r_1$和$r_2$之间随机初始化。REA随机初始化一个点$P=(x_e, y_e)$。如果$x_e + W_e≤W$,$ y_e + H_e≤H$，则设区域$I_e = (x_e, y_e, x_e + W_e, y_e + H_e)$为所选矩形区域。否则，重复上述过程，直到选择合适的$I_e$。对于所选的擦除区域$I_e$, $I_e$​中的每个像素都被赋值为区域I的均值，本文中，设置超参数如下所示：$p = 0.5$, $0.02 &lt;S_e &lt; 0.4$, $r1 = 0.3$, $r2 = 3.33$,</p>
</li>
<li><p><code>Label Smoothing</code>:在标准的ReID任务中，ID Embedding是ReID的一个基础组件，其输出图片的ID预测。标准的交叉熵损失的计算如下所示。</p>
<p><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230417205337361.png" alt="image-20230417205337361"></p>
<p>但是由于测试集的人员ID在训练集中未曾出现，所以防止ReID模型过度拟合训练ID较为重要，针对该问题，使用标签平滑（LS）方案，对应公式如下所示：</p>
<p><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230417205726034.png" alt="image-20230417205726034"></p>
<p>其中$\epsilon$为一个较小的常数，其使模型在训练集上不那么武断、不那么自信，在本研究中，设置$\epsilon$为0.1。</p>
</li>
<li><p><code>Last Stride</code>:由于更高的空间分辨率所带来的特征的粒度就越丰富。故增加特征图的大小可以较明显的增强特征表述。假设输入的图片初始尺寸为$256\times128$，经过ResNet50网络进行特征提取之后，输出的特征图尺寸为$8\times4$，如果将其最终一步的stride由2更改为1，对应的输出的特征图尺寸为$16\times8$，从而实现特征更为丰富空间尺寸更大的特征图，并能带来显著的改善。</p>
</li>
<li><p><code>BNNeck</code>:前人的许多ReID相关的工作将ID loss和triplet loss相结合，从而联合训练ReID模型。标准的联合训练方式中，ID loss和 triplet loss 约束相同的特征f，但是这两个损失的目标在嵌入空间是不同的。大量前置的研究发现，分类损失其实是在特征空间学习几个超平面，把不同类别的特征分配到不同的子空间里面（类比于SVM分类器中的超平面）。并且从人脸的SphereFace到ReID的SphereReID等工作都显示，把特征归一化到超球面，然后再优化分类损失会更好。triplet loss适合在自由的欧式空间里约束。我们经常观察到，如果把feature归一化到超球面上然后再用triplet loss优化网络的话，通常性能会比不约束的时候要差。我们推断是因为，如果把特征约束到超球面上，特征分布的自由区域会大大减小，triplet loss把正负样本对推开的难度增加。而对于分类超平面，如果把特征约束到超球面上，分类超平面还是比较清晰的。对于标准的Baseline，一个可能发生的现象是，ID loss和triplet loss不会同步收敛。通常会发现一个loss一直收敛下降，另外一个loss在某个阶段会出现先增大再下降的现象。也就是说这两个task在更新的过程中梯度方向可能不一致。<br>针对该问题，希望找个一种方式，使得triplet loss能够在自由的欧式空间里约束feature，而ID loss可以在一个超球面附近约束feature，于是乎就出现了以下的BNNeck。BNNeck的原理也很简单，网络global pooling得到的feature是在欧式空间里的，我们直接连接triplet loss，我们把这个feature记作$f_t$ 。然后这个feature经过一个BN层得到$ f_i$，经过BN层的归一化之后，batch里面$f_i$的各个维度都被拉到差不多，最后近似地在超球面附近分布。<br><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230417210658881.png" alt="image-20230417210658881"><br>最后特征的分布可以大致认为如下分布：从而感性的感受到ID loss和 Triplet loss的区别以及BNNeck的用途。</p>
<p><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230418101628876.png" alt="image-20230418101628876"></p>
</li>
<li><p><code>Center Loss</code>:Triplet loss的公式为$L_{Tri} = [d_p − d_n + α]_+$，其中$d_p$为正例之间的距离度量，$d_n$为负例之间的距离度量，$\alpha$为triplet loss的余量，文章中设置为0.3 。然而Triplet loss值考虑了正例与负例之间的差值，但没有考虑正例和负例的绝对值。故引入Center Loss，其学习每个类的深层特征的中心，并惩罚深层特征与对应类中心之间的距离，其表达式为</p>
<ul>
<li><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230418110411492.png" alt="image-20230418110411492"></li>
<li>其中$y_j$为第一个mini-batch中第j个图像的标签，$c_{y_j}$为深层特征的第一级中心，B为batch size。其有效的描述了类内变化，增加了类间的紧凑型。</li>
<li>最终的Loss表述为：$L=L_{ID}+L_{Triplet}+\beta L_C$，其中$\beta$为center loss的平衡系数，被设置为0.0005.</li>
</ul>
</li>
</ul>
<h2 id="试验效果"><a href="#试验效果" class="headerlink" title="试验效果"></a>试验效果</h2><p><img src="/2023/04/23/Bag%20of%20Tricks%20and%20A%20Strong%20Baseline%20for%20Deep%20Person%20Re-identification.md/image-20230418135011856.png" alt="image-20230418135011856"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">c++中的ffmpeg源码学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-12 09:36:52 / 修改时间：09:38:05" itemprop="dateCreated datePublished" datetime="2023-04-12T09:36:52+08:00">2023-04-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="c-中的ffmpeg使用"><a href="#c-中的ffmpeg使用" class="headerlink" title="c++中的ffmpeg使用"></a>c++中的ffmpeg使用</h2><h3 id="c-中ffmpeg的环境配置"><a href="#c-中ffmpeg的环境配置" class="headerlink" title="c++中ffmpeg的环境配置"></a>c++中ffmpeg的环境配置</h3><p>工程配置的CMakeLists.txt的一个可用案例如下所示：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">2.8</span>)</span><br><span class="line"><span class="keyword">project</span>(ffmpeg_project)</span><br><span class="line"></span><br><span class="line"><span class="comment">#以使用的rk3588s为例，以下两个set按照自己ffmpeg的安装目录修改</span></span><br><span class="line"><span class="keyword">set</span>(FFMPEG_LIBS_DIR /lib/aarch64-linux-gnu)</span><br><span class="line"><span class="keyword">set</span>(FFMPEG_HEADERS_DIR /usr/local/<span class="keyword">include</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;FFMPEG_HEADERS_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="variable">$&#123;FFMPEG_LIBS_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">set</span>(FFMPEG_LIBS libavcodec.so libavformat.so libswscale.so libavdevice.so libavutil.so)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(ffmpeg_test main.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> <span class="variable">$&#123;FFMPEG_LIBS&#125;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="c-中头文件包含ffmpeg实例"><a href="#c-中头文件包含ffmpeg实例" class="headerlink" title="c++中头文件包含ffmpeg实例"></a>c++中头文件包含ffmpeg实例</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> &#123;</span><br><span class="line">	<span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;libavcodec/avcodec.h&gt;</span></span></span><br><span class="line">	<span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;libavformat/avformat.h&gt;</span></span></span><br><span class="line">	<span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;libavutil/avutil.h&gt;</span></span></span><br><span class="line">	<span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;libavutil/opt.h&gt;</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在引入ffmpeg的头文件的时候，需要使用<code>extern &quot;C&quot;</code>将头文件包含。<code>extern &quot;c&quot;</code> 的主要作用就是为了能够正确实现C++代码调用其他C语言代码。加上 extern “c” 后，会指示编译器这部分的代码按C语言，而不是C++的方式进行编译。而ffmpeg的各个头文件都是使用c进行开发运行的，具体解释可见<a target="_blank" rel="noopener" href="https://blog.csdn.net/QTVLC/article/details/83962280">链接</a>。</p>
<h3 id="c-中使用ffmpeg的大体流程"><a href="#c-中使用ffmpeg的大体流程" class="headerlink" title="c++中使用ffmpeg的大体流程"></a>c++中使用ffmpeg的大体流程</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/leixiaohua1020/article/details/42658139#comments_25910979">详情可见雷神博客</a></p>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hVR09QSUdT,size_16,color_FFFFFF,t_70.png" alt="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hVR09QSUdT,size_16,color_FFFFFF,t_70"></p>
<p>在使用c++调用ffmpeg进行音视频处理过程中的大体流程按照<code>常见使用方法的ffmpeg音视频转换流程</code>所述。</p>
<h3 id="c-中使用ffmpeg的常用结构体"><a href="#c-中使用ffmpeg的常用结构体" class="headerlink" title="c++中使用ffmpeg的常用结构体"></a>c++中使用ffmpeg的常用结构体</h3><p>结构体之间关系如下所示：</p>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/Center.jpeg" alt="img"></p>
<p>FFMPEG中结构体很多。最关键的结构体可以分成以下几类：</p>
<ul>
<li>解协议（http,rtsp,rtmp,mms）</li>
</ul>
<blockquote>
<p>AVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。URLProtocol存储输入视音频使用的封装格式。每种协议都对应一个URLProtocol结构。（注意：FFMPEG中文件也被当做一种协议“file”）</p>
</blockquote>
<ul>
<li>解封装（flv,avi,rmvb,mp4）</li>
</ul>
<blockquote>
<p>AVFormatContext主要存储视音频封装格式中包含的信息；AVInputFormat存储输入视音频使用的封装格式。每种视音频封装格式都对应一个AVInputFormat 结构。</p>
</blockquote>
<ul>
<li>解码（h264,mpeg2,aac,mp3）</li>
</ul>
<blockquote>
<p>每个AVStream存储一个视频/音频流的相关数据；每个AVStream对应一个AVCodecContext，存储该视频/音频流使用解码方式的相关数据；每个AVCodecContext中对应一个AVCodec，包含该视频/音频对应的解码器。每种解码器都对应一个AVCodec结构。</p>
</blockquote>
<ul>
<li>存数据</li>
</ul>
<blockquote>
<p>视频的话，每个结构一般是存一帧；音频可能有好几帧<br>解码前数据：AVPacket<br>解码后数据：AVFrame</p>
</blockquote>
<ul>
<li><p><code>AVFormatContext</code>:封装格式上下文结构体，也是统领<strong>全局</strong>的结构体，保存了视频文件封装格式相关信息，是负责储存数据的结构体。</p>
<ul>
<li><p><code>AVInputFormat</code>:每种封装格式（例如<code>FLV</code>,<code>MKV</code>, <code>MP4</code>, <code>AVI</code>）对应一个该结构体。同理如<code>AVOutputFormat</code>。其保存在<code>AVFormatContext</code>中，主要被ffmpeg内部使用调用。</p>
</li>
<li><p>通过使用下述函数装载解封装器</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avformat_open_input</span><span class="params">(AVFormatContext **ps, <span class="keyword">const</span> <span class="keyword">char</span> *filename, AVInputFormat *fmt, AVDictionary **options)</span></span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><code>AVIOContext</code>:主要负责解协议，封装协议相关的过程。在整个过程中负责对例如rtmp udp进行解协议。</p>
</li>
<li><code>AVStream</code>:视频文件中每个视频（音频）流对应一个该结构体。</li>
<li><p><code>AVCodecContext</code>:编解码器上下文结构体，保存了视频（音频）编解码相关信息。</p>
<ul>
<li><code>AVCodec</code>:每种视频（音频）编解码器(例如H.264解码器)对应一个该结构体。其保存于<code>AVCodecContext</code>中，使用<code>avcodec_find_decoder(AVCodecID id)</code>装载解码器</li>
</ul>
</li>
<li><p><code>AVFrame</code>:存储一帧解码后像素（采样）数据。</p>
</li>
<li><code>AVPacket</code>:存储一帧压缩编码数据。</li>
</ul>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5byA5rC05aSq54Or,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center.png" alt="在这里插入图片描述"></p>
<h3 id="解码过程中常用函数的解析"><a href="#解码过程中常用函数的解析" class="headerlink" title="解码过程中常用函数的解析"></a>解码过程中常用函数的解析</h3><p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/1426134989_1189.jpg" alt="1426134989_1189" style="zoom:200%;"></p>
<h4 id="av-register-all"><a href="#av-register-all" class="headerlink" title="av_register_all()"></a>av_register_all()</h4><p><code>av_register_all()</code>注册所有解复用、解码等，将各个类别串成一个链表。在目前使用的ffmpeg4.2及以上的版本里面可不用该函数。其代码整个流程为首先确定有没有进行初始化，如果没有初始化，就调用avcodec_register_all()注册编解码器。函数的调用关系如下所示：</p>
<ul>
<li>在新版本的ffmpeg中，所有的解复用器，协议，复用器等被组织为一个全局静态数组，该数组在执行./configure命令的时候根据配置生成</li>
</ul>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/SouthEast.jpeg" alt="img"></p>
<h4 id="avformat-alloc-context"><a href="#avformat-alloc-context" class="headerlink" title="avformat_alloc_context()"></a>avformat_alloc_context()</h4><p><code>avformat_alloc_context()</code>主要负责AVFormatContext的初始化，主要功能为分配内存以及设置其中某些项的值为默认值。</p>
<blockquote>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150303154603565.png" alt="img"></p>
<h4 id="avformat-open-input"><a href="#avformat-open-input" class="headerlink" title="avformat_open_input()"></a>avformat_open_input()</h4><p>avformat_open_input()主要负责打开多媒体数据，并获得一些数据相关的信息。</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avformat_open_input</span><span class="params">(AVFormatContext **ps, <span class="keyword">const</span> <span class="keyword">char</span> *filename, AVInputFormat *fmt, AVDictionary **options)</span></span>;</span><br><span class="line"></span><br><span class="line">ps：函数调用成功之后处理过的AVFormatContext结构体。</span><br><span class="line">file：打开的视音频流的URL。</span><br><span class="line">fmt：强制指定AVFormatContext中AVInputFormat的。这个参数一般情况下可以设置为<span class="literal">NULL</span>，这样FFmpeg可以自动检测AVInputFormat。</span><br><span class="line">dictionay：附加的一些选项，一般情况下可以设置为<span class="literal">NULL</span>。</span><br><span class="line">当函数执行成功时，返回值大于等于<span class="number">0</span>，可以通过判断返回值与<span class="number">0</span>的关系从而判断是否打开多媒体数据成功。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150304201149635.jpeg" alt="img"></p>
<p>函数首先对输入进来的<code>AVFormatContext</code>指针进行容错检查，如有无进行初始化等操作，然后针对一些格式进行特殊处理。核心处理流程有两步。</p>
<ul>
<li>一为使用<code>init_input</code>函数，打开输入的视频数据并且探测视频的格式。<a target="_blank" rel="noopener" href="https://blog.csdn.net/leixiaohua1020/article/details/44064715">详细解释链接</a><ul>
<li>FFmpeg内部判断封装格式的原理实际上是对每种AVInputFormat给出一个分数，满分是100分，越有可能正确的AVInputFormat给出的分数就越高。最后选择分数最高的AVInputFormat作为推测结果。<ul>
<li>如果AVInputFormat中包含read_probe()，就调用read_probe()函数获取匹配分数（这一方法如果结果匹配的话，一般会获得AVPROBE_SCORE_MAX的分值，即100分）。如果不包含该函数，就使用av_match_ext()函数比较输入媒体的扩展名和AVInputFormat的扩展名是否匹配，如果匹配的话，设定匹配分数为AVPROBE_SCORE_EXTENSION（AVPROBE_SCORE_EXTENSION取值为50，即50分）。</li>
<li>使用av_match_name()比较输入媒体的mime_type和AVInputFormat的mime_type，如果匹配的话，设定匹配分数为AVPROBE_SCORE_MIME（AVPROBE_SCORE_MIME取值为75，即75分）。</li>
<li>如果该AVInputFormat的匹配分数大于此前的最大匹配分数，则记录当前的匹配分数为最大匹配分数，并且记录当前的AVInputFormat为最佳匹配的AVInputFormat.</li>
</ul>
</li>
</ul>
</li>
<li>二为使用<code>s-&gt;iformat-&gt;read_header()</code>，读取多媒体数据文件头，根据视音频流创建相应的AVStream。</li>
</ul>
<h4 id="avformat-find-stream-info"><a href="#avformat-find-stream-info" class="headerlink" title="avformat_find_stream_info()"></a>avformat_find_stream_info()</h4><p><code>avformat_find_stream_info</code>主要用于给每个媒体流（音频/视频）的AVStream结构体赋值，函数正常执行后返回值大于等于0。</p>
<ul>
<li><p>函数内部实现了解码器的查找，解码器的打开，视音频帧的读取，视音频帧的解码等工作。函数流程大致如下所示：</p>
<ul>
<li>查找解码器：find_decoder()</li>
<li>打开解码器：avcodec_open2()</li>
<li><p>读取完整的一帧压缩编码的数据：read_frame_internal()</p>
<ul>
<li>注：av_read_frame()内部实际上就是调用的read_frame_internal()。</li>
</ul>
</li>
<li><p>解码一些压缩编码数据：try_decode_frame()</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avformat_find_stream_info</span><span class="params">(AVFormatContext *ic, AVDictionary **options)</span></span>;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150306173746865.png" alt="img"></p>
<h4 id="avcodec-find-decoder"><a href="#avcodec-find-decoder" class="headerlink" title="avcodec_find_decoder()"></a>avcodec_find_decoder()</h4><p><code>avcodec_find_encoder()</code>用于查找FFmpeg的编码器</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">AVCodec *<span class="title">avcodec_find_encoder</span><span class="params">(<span class="keyword">enum</span> AVCodecID id)</span></span></span><br><span class="line"><span class="function"> 该id为编码器的ID，返回为查找到的编码器，</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150305163655358.png" alt="img"></p>
<p>在其中通过调用<code>AVCodec *find_encdec(enum AVCodecID id, int encoder)</code>进行编码器的搜索，该搜索遍历AVCodec结构的链表，逐一比较输入的ID和每一个编码器的ID，直到找到ID取值相同的编码器。</p>
<h4 id="avcodec-open2"><a href="#avcodec-open2" class="headerlink" title="avcodec_open2()"></a>avcodec_open2()</h4><p><code>avcodec_open2()</code>用于初始化一个视音频编解码器的AVCodecContext</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avcodec_open2</span><span class="params">(AVCodecContext *avctx, <span class="keyword">const</span> AVCodec *codec, AVDictionary **options)</span></span>;</span><br><span class="line"></span><br><span class="line">avctx：需要初始化的AVCodecContext。</span><br><span class="line">codec：输入的AVCodec</span><br><span class="line">options：一些选项。例如使用libx264编码的时候，“preset”，“tune”等都可以通过该参数设置。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150307171545202.png" alt="img"></p>
<p>函数整体工作流程如下所示：</p>
<ul>
<li>为各种结构体分配内存（通过各种av_malloc()实现）。</li>
<li>将输入的AVDictionary形式的选项设置到AVCodecContext。</li>
<li>其他一些零零碎碎的检查，比如说检查编解码器是否处于“实验”阶段。</li>
<li>如果是编码器，检查输入参数是否符合编码器的要求</li>
<li>调用AVCodec的init()初始化具体的解码器。</li>
</ul>
<h4 id="av-read-frame"><a href="#av-read-frame" class="headerlink" title="av_read_frame()"></a>av_read_frame()</h4><p><code>av_read_frame()</code>的作用是读取码流中的音频若干帧或者视频一帧。例如，解码视频的时候，每解码一个视频帧，需要先调用 av_read_frame()获得一帧视频的压缩数据，然后才能对该数据进行解码（例如H.264中一帧压缩数据通常对应一个NAL）</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">av_read_frame</span><span class="params">(AVFormatContext *s, AVPacket *pkt)</span></span>;</span><br><span class="line"></span><br><span class="line">s：输入的AVFormatContext</span><br><span class="line">pkt：输出的AVPacket</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150312025330316.jpeg" alt="img"></p>
<p>详细解析见<a target="_blank" rel="noopener" href="https://blog.csdn.net/leixiaohua1020/article/details/12678577">链接</a>，其大体思路为</p>
<ul>
<li>从对应的格式中，调用<code>ff_read_packet()</code>从相应的AVInputFormat中读取数据</li>
<li>视需求调用parse_packet()解析相应的AVPacket</li>
</ul>
<h4 id="avcodec-decode-video2"><a href="#avcodec-decode-video2" class="headerlink" title="avcodec_decode_video2()"></a>avcodec_decode_video2()</h4><p><code>avcodec_decode_video2()</code>的作用是解码一帧视频数据。输入一个压缩编码的结构体AVPacket，输出一个解码后的结构体AVFrame</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avcodec_decode_video2</span><span class="params">(AVCodecContext *avctx, AVFrame *picture, <span class="keyword">int</span> *got_picture_ptr, <span class="keyword">const</span> AVPacket *avpkt)</span></span>;</span><br></pre></td></tr></table></figure>
</blockquote>
<ul>
<li>对输入的字段进行了一系列的检查工作：例如宽高是否正确，输入是否为视频等等。</li>
<li>通过ret = avctx-&gt;codec-&gt;decode(avctx, picture, got_picture_ptr,&amp;tmp)这句代码，调用了相应AVCodec的decode()函数，完成了解码操作。</li>
<li>对得到的AVFrame的一些字段进行了赋值，例如宽高、像素格式等等。</li>
</ul>
<h3 id="编码过程中常用函数的解析"><a href="#编码过程中常用函数的解析" class="headerlink" title="编码过程中常用函数的解析"></a>编码过程中常用函数的解析</h3><p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/1426229411_4898.jpg" alt="1426229411_4898"></p>
<h4 id="av-register-all-1"><a href="#av-register-all-1" class="headerlink" title="av_register_all()"></a>av_register_all()</h4><p>该函数与解码时一样。</p>
<h4 id="avformat-alloc-output-context2"><a href="#avformat-alloc-output-context2" class="headerlink" title="avformat_alloc_output_context2()"></a>avformat_alloc_output_context2()</h4><p><code>avformat_alloc_output_context2()</code>函数可以初始化一个用于输出的AVFormatContext结构体。其</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avformat_alloc_output_context2</span><span class="params">(AVFormatContext **ctx, AVOutputFormat *oformat, <span class="keyword">const</span> <span class="keyword">char</span> *format_name, <span class="keyword">const</span> <span class="keyword">char</span> *filename)</span></span>;</span><br><span class="line"></span><br><span class="line">ctx：函数调用成功之后创建的AVFormatContext结构体。</span><br><span class="line">oformat：指定AVFormatContext中的AVOutputFormat，用于确定输出格式。如果指定为<span class="literal">NULL</span>，可以设定后两个参数（format_name或者filename）由FFmpeg猜测输出格式。</span><br><span class="line">	PS：使用该参数需要自己手动获取AVOutputFormat，相对于使用后两个参数来说要麻烦一些。</span><br><span class="line">format_name：指定输出格式的名称。根据格式名称，FFmpeg会推测输出格式。输出格式可以是“flv”，“mkv”等等。</span><br><span class="line">filename：指定输出文件的名称。根据文件名称，FFmpeg会推测输出格式。文件名称可以是“xx.flv”，“yy.mkv”等等。</span><br><span class="line">函数执行成功的话，其返回值大于等于<span class="number">0</span>。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150303220720490.png" alt="img"></p>
<p>函数执行流程可简单概括为以下两步：</p>
<ul>
<li>调用avformat_alloc_context()进行内存分配以及初始化默认的AVFormatContext。</li>
<li>如果指定了输入的AVOutputFormat，则直接将输入的AVOutputFormat赋值给AVOutputFormat的oformat。如果没有指定输入的AVOutputFormat，就需要根据文件格式名称或者文件名推测输出的AVOutputFormat。无论是通过文件格式名称还是文件名推测输出格式，都会调用一个函数av_guess_format()。<ul>
<li>在<code>av_guess_format()</code>中，使用socre记录每种输出格式的匹配程度，遍历ffmpeg中所有的AVOutputFormat并逐一计算每个输出格式的score，具体的计算流程如下所示：<ul>
<li>如果封装格式名称匹配，score增加100。匹配中使用了函数av_match_name()。</li>
<li>如果mime类型匹配，score增加10。匹配直接使用字符串比较函数strcmp()。</li>
<li>如果文件名称的后缀匹配，score增加5。匹配中使用了函数av_match_ext()。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="avio-open2"><a href="#avio-open2" class="headerlink" title="avio_open2()"></a>avio_open2()</h4><p><code>avio_open2()</code>用于打开FFmpeg的输入输出文件。</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avio_open2</span><span class="params">(AVIOContext **s, <span class="keyword">const</span> <span class="keyword">char</span> *url, <span class="keyword">int</span> flags, <span class="keyword">const</span> AVIOInterruptCB *int_cb, AVDictionary **options)</span></span>;</span><br><span class="line"></span><br><span class="line">s：函数调用成功之后创建的AVIOContext结构体。</span><br><span class="line">url：输入输出协议的地址（文件也是一种“广义”的协议，对于文件来说就是文件的路径）。</span><br><span class="line">flags：打开地址的方式。可以选择只读，只写，或者读写。取值如下。</span><br><span class="line">	AVIO_FLAG_READ：只读。</span><br><span class="line">	AVIO_FLAG_WRITE：只写。</span><br><span class="line">	AVIO_FLAG_READ_WRITE：读写。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150304132258935.png" alt="img"></p>
<p>该函数主要调用了两个函数<code>ffurl_open()</code>和<code>ffio_fdopen()</code>。</p>
<ul>
<li>ffurl_open()用于初始化URLContext<ul>
<li>ffurl_open()主要调用了2个函数：ffurl_alloc()和ffurl_connect()。<ul>
<li>ffurl_alloc()用于查找合适的URLProtocol，并创建一个URLContext</li>
<li>ffurl_connect()用于打开获得的URLProtocol。</li>
</ul>
</li>
</ul>
</li>
<li>ffio_fdopen()用于根据URLContext初始化AVIOContext。<ul>
<li>URLContext中包含的URLProtocol完成了具体的协议读写等工作。AVIOContext则是在URLContext的读写函数外面加上了一层“包装”（通过retry_transfer_wrapper()函数）。</li>
</ul>
</li>
</ul>
<h4 id="avformat-new-stream"><a href="#avformat-new-stream" class="headerlink" title="avformat_new_stream()"></a>avformat_new_stream()</h4><p><code>avformat_new_stream()</code>是初始化<code>AVStream</code>的函数。</p>
<h4 id="avcodec-find-encoder"><a href="#avcodec-find-encoder" class="headerlink" title="avcodec_find_encoder()"></a>avcodec_find_encoder()</h4><p><code>avcodec_find_encoder()</code>与解码过程中的<code>avcodec_find_decoder()</code>类似。</p>
<h4 id="avcodec-open2-1"><a href="#avcodec-open2-1" class="headerlink" title="avcodec_open2()"></a>avcodec_open2()</h4><p><code>avcodec_open2()</code>用于初始化一个视音频编解码器的AVCodecContext。</p>
<h4 id="avformat-write-header"><a href="#avformat-write-header" class="headerlink" title="avformat_write_header()"></a>avformat_write_header()</h4><p><code>avformat_write_header()</code>用于写视频文件头。</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avformat_write_header</span><span class="params">(AVFormatContext *s, AVDictionary **options)</span></span>;</span><br><span class="line"></span><br><span class="line">s：用于输出的AVFormatContext。</span><br><span class="line">options：额外的选项，目前没有深入研究过，一般为<span class="literal">NULL</span>。</span><br><span class="line">函数正常执行后返回值为<span class="number">0</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150307142222277.png" alt="img"></p>
<p>avformat_write_header()完成了以下工作：</p>
<ul>
<li>调用init_muxer()初始化复用器<ul>
<li>将传入的AVDictionary形式的选项设置到AVFormatContext</li>
<li>遍历AVFormatContext中的每个AVStream，并作如下检查：<ul>
<li>AVStream的time_base是否正确设置。如果发现AVStream的time_base没有设置，则会调用avpriv_set_pts_info()进行设置。</li>
<li>对于音频，检查采样率设置是否正确；对于视频，检查宽、高、宽高比。</li>
<li>其他一些检查</li>
</ul>
</li>
</ul>
</li>
<li>调用AVOutputFormat的write_header()，write_header()是AVOutputFormat中的一个函数指针，指向写文件头的函数。不同的AVOutputFormat有不同的write_header()的实现方法。</li>
</ul>
<h4 id="avcodec-encode-video2"><a href="#avcodec-encode-video2" class="headerlink" title="avcodec_encode_video2()"></a>avcodec_encode_video2()</h4><p><code>avcodec_encode_video2()</code>用于编码一帧视频数据</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">avcodec_encode_video2</span><span class="params">(AVCodecContext *avctx, AVPacket *avpkt, <span class="keyword">const</span> AVFrame *frame, <span class="keyword">int</span> *got_packet_ptr)</span></span>;</span><br><span class="line"> avctx：编码器的AVCodecContext。</span><br><span class="line"> avpkt：编码输出的AVPacket。</span><br><span class="line"> frame：编码输入的AVFrame。</span><br><span class="line"> got_packet_ptr：成功编码一个AVPacket的时候设置为<span class="number">1</span>。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150311222209829.png" alt="img"></p>
<p>在该函数中，主要由两个部分组成。首先调用<code>av_image_check_size()</code>检查设置的宽高等参数是否合理，然后调用AVcodec的<code>encode2()</code>调用具体的解码器。</p>
<h4 id="av-write-frame"><a href="#av-write-frame" class="headerlink" title="av_write_frame()"></a>av_write_frame()</h4><p><code>av_write_frame()</code>用于输出一帧视频数据。</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">av_write_frame</span><span class="params">(AVFormatContext *s, AVPacket *pkt)</span></span>;</span><br><span class="line">	s：用于输出的AVFormatContext。</span><br><span class="line">	pkt：等待输出的AVPacket。</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2023/04/12/c-%E4%B8%AD%E7%9A%84ffmpeg%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/20150311155409612.png" alt="img"></p>
<p>该函数主要包括以下三个步骤：</p>
<ul>
<li>调用check_packet()做一些简单的检测</li>
<li>调用compute_pkt_fields2()设置AVPacket的一些属性值</li>
<li>调用write_packet()写入数据</li>
</ul>
<h4 id="av-write-trailer"><a href="#av-write-trailer" class="headerlink" title="av_write_trailer()"></a>av_write_trailer()</h4><p><code>av_write_trailer()</code>用于写视频文件尾</p>
<blockquote>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">av_write_trailer</span><span class="params">(AVFormatContext *s)</span></span>;</span><br><span class="line">	s：用于输出的AVFormatContext。</span><br></pre></td></tr></table></figure>
</blockquote>
<p>av_write_trailer()主要完成了以下两步工作：</p>
<ul>
<li>循环调用interleave_packet()以及write_packet()，将还未输出的AVPacket输出出来。</li>
<li>调用AVOutputFormat的write_trailer()，输出文件尾。</li>
</ul>
<h3 id="c-使用ffmpeg进行视频格式转换的案例（由mp4转换为flv）"><a href="#c-使用ffmpeg进行视频格式转换的案例（由mp4转换为flv）" class="headerlink" title="c++使用ffmpeg进行视频格式转换的案例（由mp4转换为flv）"></a>c++使用ffmpeg进行视频格式转换的案例（由mp4转换为flv）</h3><p>工程链接：<a target="_blank" rel="noopener" href="https://github.com/XDUwsk/ffmpeg_demo/tree/main/change_mp4_2_flv">change_mp4_2_flv</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;libavformat/avformat.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;libavutil/dict.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;libavutil/opt.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;libavutil/timestamp.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;libswscale/swscale.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;libswresample/swresample.h&quot;</span></span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//本质上ffmpeg4.2.7不需要这句话，但是加上也没有问题</span></span><br><span class="line">	<span class="built_in">av_register_all</span>();</span><br><span class="line">	<span class="comment">//avformat_network_init();</span></span><br><span class="line"> </span><br><span class="line">    AVFormatContext* ifmt_ctx = <span class="literal">NULL</span>;</span><br><span class="line">	<span class="keyword">const</span> <span class="keyword">char</span>* inputUrl = <span class="string">&quot;/home/firefly/ffmpeg_workspace/media/4.mp4&quot;</span>;</span><br><span class="line"> </span><br><span class="line">	<span class="comment">///打开输入的流</span></span><br><span class="line">	<span class="keyword">int</span> ret = <span class="built_in">avformat_open_input</span>(&amp;ifmt_ctx, inputUrl, <span class="literal">NULL</span>, <span class="literal">NULL</span>);</span><br><span class="line">	<span class="keyword">if</span> (ret != <span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;Couldn&#x27;t open input stream.\n&quot;</span>);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"> </span><br><span class="line">	<span class="comment">//查找流信息</span></span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">avformat_find_stream_info</span>(ifmt_ctx, <span class="literal">NULL</span>) &lt; <span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">&quot;Couldn&#x27;t find stream information.\n&quot;</span>);</span><br><span class="line">		<span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">	&#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//输出的文件</span></span><br><span class="line">    AVOutputFormat *ofmt = <span class="literal">NULL</span>;</span><br><span class="line">    AVFormatContext *ofmt_ctx = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* out_filename = <span class="string">&quot;4_out.flv&quot;</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">avformat_alloc_output_context2</span>(&amp;ofmt_ctx, <span class="literal">NULL</span>, <span class="literal">NULL</span>, out_filename);</span><br><span class="line">    <span class="keyword">if</span> (!ofmt_ctx) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> stream_mapping_size = ifmt_ctx-&gt;nb_streams;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//为数组分配内存</span></span><br><span class="line">    <span class="keyword">int</span>* stream_mapping = (<span class="keyword">int</span> *)<span class="built_in">av_mallocz_array</span>(stream_mapping_size, <span class="built_in"><span class="keyword">sizeof</span></span>(*stream_mapping));</span><br><span class="line">    <span class="keyword">if</span> (!stream_mapping) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">int</span> stream_index = <span class="number">0</span>;</span><br><span class="line">    ofmt = ofmt_ctx-&gt;oformat;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; ifmt_ctx-&gt;nb_streams; i++) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//输出的流</span></span><br><span class="line">        AVStream* out_stream = <span class="literal">NULL</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//输入的流 视频、音频、字幕等</span></span><br><span class="line">        AVStream* in_stream = ifmt_ctx-&gt;streams[i];</span><br><span class="line">        AVCodecParameters* in_codecpar = in_stream-&gt;codecpar;</span><br><span class="line">        <span class="keyword">if</span> (in_codecpar-&gt;codec_type != AVMEDIA_TYPE_AUDIO &amp;&amp; in_codecpar-&gt;codec_type != AVMEDIA_TYPE_VIDEO &amp;&amp; in_codecpar-&gt;codec_type != AVMEDIA_TYPE_SUBTITLE) </span><br><span class="line">        &#123;</span><br><span class="line">            stream_mapping[i] = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        stream_mapping[i] = stream_index++;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//创建一个新的流</span></span><br><span class="line">        out_stream = <span class="built_in">avformat_new_stream</span>(ofmt_ctx, <span class="literal">NULL</span>); </span><br><span class="line">        <span class="keyword">if</span> (!out_stream) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//复制输入的流信息到输出流中</span></span><br><span class="line">        ret = <span class="built_in">avcodec_parameters_copy</span>(out_stream-&gt;codecpar, in_codecpar);</span><br><span class="line">        <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        out_stream-&gt;codecpar-&gt;codec_tag = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> (!(ofmt-&gt;flags &amp; AVFMT_NOFILE)) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//打开输出文件</span></span><br><span class="line">        ret = <span class="built_in">avio_open</span>(&amp;ofmt_ctx-&gt;pb, out_filename, AVIO_FLAG_WRITE); </span><br><span class="line">        <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//写入头</span></span><br><span class="line">    ret = <span class="built_in">avformat_write_header</span>(ofmt_ctx, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    AVPacket pkt;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) </span><br><span class="line">    &#123;</span><br><span class="line">        AVStream* in_stream = <span class="literal">NULL</span>;</span><br><span class="line">        AVStream* out_stream = <span class="literal">NULL</span>;</span><br><span class="line"> </span><br><span class="line">        <span class="comment">//从输入流中读取数据到pkt中</span></span><br><span class="line">        ret = <span class="built_in">av_read_frame</span>(ifmt_ctx, &amp;pkt);</span><br><span class="line">        <span class="keyword">if</span> (ret &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"> </span><br><span class="line">        in_stream = ifmt_ctx-&gt;streams[pkt.stream_index];</span><br><span class="line">        <span class="keyword">if</span> (pkt.stream_index &gt;= stream_mapping_size || stream_mapping[pkt.stream_index] &lt; <span class="number">0</span>) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">av_packet_unref</span>(&amp;pkt);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        pkt.stream_index = stream_mapping[pkt.stream_index];</span><br><span class="line">        out_stream = ofmt_ctx-&gt;streams[pkt.stream_index];</span><br><span class="line"> </span><br><span class="line">        <span class="comment">/* copy packet */</span></span><br><span class="line">        pkt.pts = <span class="built_in">av_rescale_q_rnd</span>(pkt.pts, in_stream-&gt;time_base, out_stream-&gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));</span><br><span class="line">        pkt.dts = <span class="built_in">av_rescale_q_rnd</span>(pkt.dts, in_stream-&gt;time_base, out_stream-&gt;time_base, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));</span><br><span class="line">        pkt.duration = <span class="built_in">av_rescale_q</span>(pkt.duration, in_stream-&gt;time_base, out_stream-&gt;time_base);</span><br><span class="line">        pkt.pos = <span class="number">-1</span>;</span><br><span class="line"> </span><br><span class="line">        ret = <span class="built_in">av_interleaved_write_frame</span>(ofmt_ctx, &amp;pkt);</span><br><span class="line">        <span class="keyword">if</span> (ret &lt; <span class="number">0</span>) </span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">fprintf</span>(stderr, <span class="string">&quot;Error muxing packet\n&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">av_packet_unref</span>(&amp;pkt);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//写文件尾</span></span><br><span class="line">    <span class="built_in">av_write_trailer</span>(ofmt_ctx);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//关闭</span></span><br><span class="line">    <span class="built_in">avformat_close_input</span>(&amp;ifmt_ctx);</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">if</span> (ofmt_ctx &amp;&amp; !(ofmt-&gt;flags &amp; AVFMT_NOFILE))</span><br><span class="line">        <span class="built_in">avio_closep</span>(&amp;ofmt_ctx-&gt;pb);</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">avformat_free_context</span>(ofmt_ctx);</span><br><span class="line">    <span class="built_in">av_freep</span>(&amp;stream_mapping);</span><br><span class="line">    <span class="keyword">if</span> (ret &lt; <span class="number">0</span> &amp;&amp; ret != AVERROR_EOF)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/12/ffmpeg%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/12/ffmpeg%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/" class="post-title-link" itemprop="url">ffmpeg基础了解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-12 09:36:29 / 修改时间：09:39:30" itemprop="dateCreated datePublished" datetime="2023-04-12T09:36:29+08:00">2023-04-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="ffmpeg是什么"><a href="#ffmpeg是什么" class="headerlink" title="ffmpeg是什么"></a>ffmpeg是什么</h2><p>FFmpeg是一个库和工具的集合，用于处理音频、视频、字幕和相关元数据等多媒体内容。</p>
<h2 id="ffmpeg的组成"><a href="#ffmpeg的组成" class="headerlink" title="ffmpeg的组成"></a>ffmpeg的组成</h2><p>ffmpeg由以下几个核心依赖包组成</p>
<ul>
<li><strong>libavcodec</strong> - 提供了更广泛的编码器解码器的实现。各种格式的编解码代码(如aacenc.c、aacdec.c等)都位于该目录下。</li>
<li><strong>libavformat</strong> - 实现了流协议、容器格式和基本的I/O实现。用于各种音视频封装格式的生成和解析，包括获取解码所需信息、读取音视频数据等功能。各种流媒体协议代码(如rtmpproto.c等)以及音视频格式的(解)复用代码(如flvdec.c、flvenc.c等)都位于该目录下。</li>
<li><strong>libavutil</strong> - 为核心工具包，包含一些公共的工具函数的使用库，包括算数运算，字符操作等。</li>
<li><strong>libavfilter</strong> - 提供各种音视频滤波器。</li>
<li><strong>libavdevice</strong> - 用于硬件的音视频采集、加速和显示。</li>
<li><strong>libswresample</strong> - 提供音频重采样，采样格式转换和音频混合等功能。</li>
<li><strong>libswscale</strong> - 提供原始视频的比例缩放、色彩映射转换、图像颜色空间或格式转换的功能。</li>
</ul>
<h2 id="ffmpeg用到的工具"><a href="#ffmpeg用到的工具" class="headerlink" title="ffmpeg用到的工具"></a>ffmpeg用到的工具</h2><ul>
<li><a target="_blank" rel="noopener" href="https://ffmpeg.org/ffmpeg.html">ffmpeg</a>是一个用于操作、转换和流式传输多媒体内容的命令行工具箱。</li>
<li><a target="_blank" rel="noopener" href="https://ffmpeg.org/ffplay.html">ffplay</a>是一款简约的多媒体播放器。</li>
<li><a target="_blank" rel="noopener" href="https://ffmpeg.org/ffprobe.html">ffprobe</a>是一种检查多媒体内容的简单分析工具。</li>
<li>其他小工具，如”aviocat”、”ismindex”和”qt faststart”。</li>
</ul>
<h2 id="ffmpeg的源码编译"><a href="#ffmpeg的源码编译" class="headerlink" title="ffmpeg的源码编译"></a>ffmpeg的源码编译</h2><h3 id="ffmpeg的源码下载-以ffmpeg-release-6-0为例"><a href="#ffmpeg的源码下载-以ffmpeg-release-6-0为例" class="headerlink" title="ffmpeg的源码下载    以ffmpeg release 6.0为例"></a>ffmpeg的源码下载    以<a target="_blank" rel="noopener" href="https://github.com/FFmpeg/FFmpeg/tree/release/6.0">ffmpeg release 6.0为例</a></h3><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/FFmpeg/FFmpeg/<span class="built_in">tree</span>/release/<span class="number">6</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="yasm的安装"><a href="#yasm的安装" class="headerlink" title="yasm的安装"></a>yasm的安装</h3><p>由于ffmpeg的安装过程中为了提高效率使用了汇编指令，而yasm是汇编编译器，在ffmpeg的编译过程中对其有依赖，所以需要对其提前进行下载安装。</p>
<p>linux环境下直接：</p>
<ul>
<li>下载：wget  <a target="_blank" rel="noopener" href="http://www.tortall.net/projects/yasm/releases/yasm-1.3.0.tar.gz">http://www.tortall.net/projects/yasm/releases/yasm-1.3.0.tar.gz</a></li>
<li>解压：tar zxvf yasm-1.3.0.tar.gz</li>
<li>切换路径： cd yasm-1.3.0</li>
<li>执行配置： ./configure</li>
<li>编译：make</li>
<li>安装：make install</li>
</ul>
<h3 id="ffmpeg的源码编译-1"><a href="#ffmpeg的源码编译-1" class="headerlink" title="ffmpeg的源码编译"></a>ffmpeg的源码编译</h3><p>进入ffmpeg的源码文件夹。</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/local/ffmpeg</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">vi /etc/profile</span><br><span class="line">export <span class="built_in">PATH</span>=$<span class="built_in">PATH</span>:/usr/local/ffmpeg/bin</span><br></pre></td></tr></table></figure>
<h3 id="ffmpeg的安装测试"><a href="#ffmpeg的安装测试" class="headerlink" title="ffmpeg的安装测试"></a>ffmpeg的安装测试</h3><p>在命令行中直接输入ffmpeg，得到ffmpeg相关的信息输出即可。</p>
<h2 id="常见使用方法"><a href="#常见使用方法" class="headerlink" title="常见使用方法"></a>常见使用方法</h2><p>具体详细版的ffmpeg文档可见： <a target="_blank" rel="noopener" href="https://xdsnet.gitbooks.io/other-doc-cn-ffmpeg/content/index.html">ffmpeg中文文档</a></p>
<h3 id="统一语法"><a href="#统一语法" class="headerlink" title="统一语法"></a>统一语法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg [全局选项] &#123;[输入文件选项] -i 输入文件&#125; ... &#123;[输出文件选项] 输出文件&#125; ...</span><br></pre></td></tr></table></figure>
<p>即</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg [global_options] &#123;[input_file_options] -i input_file&#125; ... &#123;[output_file_options] output_file&#125; ...</span><br></pre></td></tr></table></figure>
<h3 id="基本选项"><a href="#基本选项" class="headerlink" title="基本选项"></a>基本选项</h3><p>能力集列表</p>
<ul>
<li>-formats：列出支持的文件格式。</li>
<li>-codecs：列出支持的编解码器。</li>
<li>-decoders：列出支持的解码器。</li>
<li>-encoders：列出支持的编码器。</li>
<li>-protocols：列出支持的协议。</li>
<li>-bsfs：列出支持的比特流过滤器。</li>
<li>-filters：列出支持的滤镜。</li>
<li>-pix_fmts：列出支持的图像采样格式。</li>
<li>-sample_fmts：列出支持的声音采样格式。</li>
</ul>
<p>常用输入选项</p>
<ul>
<li>-i filename：指定输入文件名。</li>
<li>-f fmt：强制设定文件格式，需使用能力集列表中的名称(缺省是根据扩展名选择的)。</li>
<li>-ss hh:mm:ss[.xxx]：设定输入文件的起始时间点，启动后将跳转到此时间点然后开始读取数据。</li>
</ul>
<p>对于输入，以下选项通常是自动识别的，但也可以强制设定。</p>
<ul>
<li>-c codec：指定解码器，需使用能力集列表中的名称。</li>
<li>-acodec codec：指定声音的解码器，需使用能力集列表中的名称。</li>
<li>-vcodec codec：指定视频的解码器，需使用能力集列表中的名称。</li>
<li>-b:v bitrate：设定视频流的比特率，整数，单位bps。</li>
<li>-r fps：设定视频流的帧率，整数，单位fps。</li>
<li>-s WxH : 设定视频的画面大小。也可以通过挂载画面缩放滤镜实现。</li>
<li>-pix_fmt format：设定视频流的图像格式(如RGB还是YUV)。</li>
<li>-ar sample rate：设定音频流的采样率，整数，单位Hz。</li>
<li>-ab bitrate：设定音频流的比特率，整数，单位bps。</li>
<li>-ac channels：设置音频流的声道数目。</li>
</ul>
<p>常用输出选项</p>
<ul>
<li>-f fmt：强制设定文件格式，需使用能力集列表中的名称(缺省是根据扩展名选择的)。</li>
<li>-c codec：指定编码器，需使用能力集列表中的名称(编码器设定为”copy“表示不进行编解码)。</li>
<li>-acodec codec：指定声音的编码器，需使用能力集列表中的名称(编码器设定为”copy“表示不进行编解码)。</li>
<li>-vcodec codec：指定视频的编码器，需使用能力集列表中的名称(编解码器设定为”copy“表示不进行编解码)。</li>
<li>-r fps：设定视频编码器的帧率，整数，单位fps。</li>
<li>-pix_fmt format：设置视频编码器使用的图像格式(如RGB还是YUV)。</li>
<li>-ar sample rate：设定音频编码器的采样率，整数，单位Hz。</li>
<li>-b bitrate：设定音视频编码器输出的比特率，整数，单位bps。</li>
<li>-ab bitrate：设定音频编码器输出的比特率，整数，单位bps。</li>
<li>-ac channels：设置音频编码器的声道数目。</li>
<li>-an 忽略任何音频流。</li>
<li>-vn 忽略任何视频流。</li>
<li>-t hh:mm:ss[.xxx]：设定输出文件的时间长度。</li>
<li>-to hh:mm:ss[.xxx]：如果没有设定输出文件的时间长度的画可以设定终止时间点。</li>
</ul>
<h3 id="ffmpeg音视频转换流程"><a href="#ffmpeg音视频转换流程" class="headerlink" title="ffmpeg音视频转换流程"></a>ffmpeg音视频转换流程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> _______              ______________</span><br><span class="line">|       |            |              |</span><br><span class="line">| input |  demuxer   | encoded data |   decoder</span><br><span class="line">| file  | ---------&gt; | packets      | -----+</span><br><span class="line">|_______|            |______________|      |</span><br><span class="line">                                           v</span><br><span class="line">                                       _________</span><br><span class="line">                                      |         |</span><br><span class="line">                                      | decoded |</span><br><span class="line">                                      | frames  |</span><br><span class="line">                                      |_________|</span><br><span class="line">  ________             ______________      |</span><br><span class="line">|        |           |              |      |</span><br><span class="line">| output | &lt;-------- | encoded data | &lt;----+</span><br><span class="line">| file   |   muxer   | packets      |   encoder</span><br><span class="line">|________|           |______________|</span><br></pre></td></tr></table></figure>
<p><code>ffmpeg</code>调用<code>libavformat</code>库(含分离器<code>demuxer</code>)读取输入文件，分离出各类编码的数据包(流)。编码数据包通过解码器解码出非压缩的数据帧(raw视频/PCM格式音频…)，这些数据帧可以被滤镜进一步处理。经过滤镜处理的数据被重新编码为新的数据包(流)，然后经过混合器混合(例如按一定顺序和比例把音频数据包和视频数据包交叉组合)，写入到输出文件。</p>
<h3 id="滤镜处理-Filtering"><a href="#滤镜处理-Filtering" class="headerlink" title="滤镜处理(Filtering)"></a>滤镜处理(Filtering)</h3><p>在上述音视频转换流程中，decoder得到原始音视频数据之后，可以使用<code>libavfilter</code>库中的滤镜进行处理，滤镜之间可以组合使用<code>filtergraphs</code> ，对于ffmpeg而言，滤镜分为<code>简单滤镜</code>和<code>复合滤镜</code>。</p>
<h4 id="简单滤镜"><a href="#简单滤镜" class="headerlink" title="简单滤镜"></a>简单滤镜</h4><p>简单滤镜即为只有一个输入和输出的滤镜，且滤镜两边的数据为同一类型的数据，可以理解为从raw data到encoder处理之前简单附加的一步。其具体流程可如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> _________                        ______________</span><br><span class="line">|         |                      |              |</span><br><span class="line">| decoded |                      | encoded data |</span><br><span class="line">| frames  |\                     | packets      |</span><br><span class="line">|_________| \                  /||______________|</span><br><span class="line">             \   __________   /</span><br><span class="line">  simple      \ |          | /  encoder</span><br><span class="line">  filtergraph  \| filtered |/</span><br><span class="line">                | frames   |</span><br><span class="line">                |__________|</span><br></pre></td></tr></table></figure>
<p>tips：滤镜改变的不止可以为帧内容，还可以是帧属性。例如帧率的变化，尺寸的变化等。对应于帧内容并不发生改变。</p>
<h4 id="复合滤镜"><a href="#复合滤镜" class="headerlink" title="复合滤镜"></a>复合滤镜</h4><p>不为简单滤镜的行为均可视为复合滤镜，例如多个输入多个输出的场景，示意图如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> _________</span><br><span class="line">|         |</span><br><span class="line">| input 0 |\                    __________</span><br><span class="line">|_________| \                  |          |</span><br><span class="line">             \   _________    /| output 0 |</span><br><span class="line">              \ |         |  / |__________|</span><br><span class="line"> _________     \| complex | /</span><br><span class="line">|         |     |         |/</span><br><span class="line">| input 1 |----&gt;| filter  |\</span><br><span class="line">|_________|     |         | \   __________</span><br><span class="line">               /| graph   |  \ |          |</span><br><span class="line">              / |         |   \| output 1 |</span><br><span class="line"> _________   /  |_________|    |__________|</span><br><span class="line">|         | /</span><br><span class="line">| input 2 |/</span><br><span class="line">|_________|</span><br></pre></td></tr></table></figure>
<p>复合滤镜由<code>-filter_complex</code>选项进行设定。<strong>注意</strong>这是一个全局选项，因为一个复合滤镜必然是不能只关联到一个单一流或者文件的。<code>-lavfi</code>选项等效于<code>-filter_complex</code></p>
<p>一个复合滤镜的简单例子就是<code>overlay</code>滤镜，它从两路输入中，把一个视频叠加到一个输出上。对应的类似音频滤镜是<code>amix</code>。</p>
<h4 id="流拷贝"><a href="#流拷贝" class="headerlink" title="流拷贝"></a>流拷贝</h4><p>流拷贝(Stream copy)是一种对指定流数据仅仅进行复制的<code>拷贝(copy)</code>模式。这种情况下<code>ffmpeg</code>不会对指定流进行解码和编码步骤，而仅仅是分离和混合数据包。这种模式常用于文件包装格式的转换或者修改部分元数据信息，这个过程简单图示如下：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_______              ______________            ________</span><br><span class="line">|       |            |              |          |        |</span><br><span class="line">| input |  demuxer   | encoded data |  muxer   | output |</span><br><span class="line">| file  | ---------&gt; | packets      | -------&gt; | file   |</span><br><span class="line">|_______|            |______________|          |________|</span><br></pre></td></tr></table></figure>
</blockquote>
<p>因为这种模式下不存在解码和编码过程，所以也特别快，而且不会造成新的质量损失。然而这也使得这样的模式不能适合很多工作需求，例如这个模式下不能使用大量的滤镜了，因为滤镜仅能对未压缩(编码)的数据进行处理。</p>
<h3 id="流"><a href="#流" class="headerlink" title="流"></a>流</h3><h4 id="4-1流处理"><a href="#4-1流处理" class="headerlink" title="4.1流处理"></a>4.1流处理</h4><p>默认情况下，<code>ffmpeg</code>把输入文件每种类型(视频、音频和字幕)仅仅采用一个流转换输出到输出文件中，就是把<strong>最好</strong>效果的流进行输出：</p>
<ul>
<li>对于视频，它是具有最高分辨率的流</li>
<li>对于音频，它是具有最多频道的流</li>
<li>对于字幕，它是第一个找到的字幕流，但有一个警告。输出格式的默认字幕编码器可以是基于文本的，也可以是基于图像的，并且仅选择相同类型的字幕流</li>
<li>在几个相同类型的流速率相等的情况下，选择具有最低索引的流。</li>
</ul>
<p>当然，你可以禁用默认设置，而采用<code>-vn/-an/-sn</code>选项进行专门的指定，如果要进行完全的手动控制，则是以<code>-map</code>选项，它将禁止默认值而选用指定的配置。</p>
<h4 id="4-1流处理-1"><a href="#4-1流处理-1" class="headerlink" title="4.1流处理"></a>4.1流处理</h4><p>流处理独立于流选择，下面描述的字幕除外。流处理通过<code>-codec</code>选项进行设置，该选项寻址到特定输出文件内的流。特别是，<code>-codec</code>在流选择过程之后被ffmpeg应用，因此不影响后者。如果没有为流类型指定<code>-codec</code>选项，ffmpeg将选择输出文件muxer注册的默认编码器。</p>
<p>对于字幕存在例外。如果为输出文件指定了字幕编码器，则将包括找到任何类型的第一个字幕流，如文本或图像。 ffmpeg不验证指定的编码器是否可以转换所选的流，或者转换的流是否在输出格式中是可接受的。这通常也适用：当用户手动设置编码器时，流选择过程不能检查编码流是否可以复用到输出文件中。如果不能，则ffmpeg将中止，并且所有输出文件都将无法处理。</p>
<h4 id="4-2例子"><a href="#4-2例子" class="headerlink" title="4.2例子"></a>4.2例子</h4><p>假设以下三个输入文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">input file &#x27;A.avi&#x27;</span><br><span class="line">      stream 0: video 640x360</span><br><span class="line">      stream 1: audio 2 channels</span><br><span class="line"> </span><br><span class="line">input file &#x27;B.mp4&#x27;</span><br><span class="line">      stream 0: video 1920x1080</span><br><span class="line">      stream 1: audio 2 channels</span><br><span class="line">      stream 2: subtitles (text)</span><br><span class="line">      stream 3: audio 5.1 channels</span><br><span class="line">      stream 4: subtitles (text)</span><br><span class="line"> </span><br><span class="line">input file &#x27;C.mkv&#x27;</span><br><span class="line">      stream 0: video 1280x720</span><br><span class="line">      stream 1: audio 2 channels</span><br><span class="line">      stream 2: subtitles (image)</span><br></pre></td></tr></table></figure>
<h5 id="示例：自动流选择"><a href="#示例：自动流选择" class="headerlink" title="示例：自动流选择"></a>示例：自动流选择</h5><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i A.avi -i B.mp4 out1.mkv out2.wav -map <span class="number">1</span>:a -c:a <span class="built_in">copy</span> out3.mov</span><br></pre></td></tr></table></figure>
<p>指定了三个输出文件，对于前两个out1 out2，由于未设置<code>-map</code>选项，因此ffmpeg将自动为这两个文件选择流。<br>out1.mkv是一个Matroska容器文件，接受视频，音频和字幕流，因此ffmpeg将尝试选择每种类型中的一种。<br>对于视频，它将从B.mp4中选择流 stream 0 ，其在所有输入视频流中具有最高分辨率。<br>对于音频，它将从B.mp4中选择流 stream 3 ，因为它具有最多的通道。<br>对于字幕，它将从B.mp4中选择流 stream 2 ，这是A.avi和B.mp4中的第一个字幕流。<br>out2.wav只接受音频流，因此只选择来自B.mp4的stream 3。<br>out3.mov，由于设置了<code>-map</code>选项，因此不会进行自动流选择。 <code>-map 1:a</code>选项将从第二个输入B.mp4中选择所有音频流。此输出文件中不包含其他流。<br>对于前两个输出，将对所有包含的流进行转码。选择的编码器将是每种输出格式注册的默认编码器，可能与所选输入流的编解码器不匹配。<br>对于第三个输出，<code>-c:a copy</code>意为使用指定音视频编码中的所有音频流编解码器，设置为<code>copy</code>，因此不会发生以及不可能发生解码 - 过滤 - 编码操作。所选流的数据包应从输入文件传送，并在输出文件中复用。</p>
<h5 id="示例：自动字幕选择"><a href="#示例：自动字幕选择" class="headerlink" title="示例：自动字幕选择"></a>示例：自动字幕选择</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i C.mkv out1.mkv -c:s dvdsub -an out2.mkv</span><br></pre></td></tr></table></figure>
<p>尽管out1.mkv是Matroska容器文件，它接受字幕流，但只能选择视频和音频流。 C.mkv的字幕流是基于图像的，并且Matroska复用器的默认字幕编码器是基于文本的，因此字幕的转码操作预计会失败，因此不选择该流。 然而，在out2.mkv中，在命令中<code>-c:s dvdsub</code>指定字幕编码器，因此，除了视频流之外，还选择字幕流。 <code>-an</code>的存在禁用out2.mkv的音频流选择。</p>
<h3 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h3><p>所有的数值选项，如果没有特殊定义，则需要一个接受一个字符串代表一个数作为输入，这可能跟着一个单位量词首字母，例如<code>&quot;k&quot;</code>,<code>&quot;m&quot;</code>或<code>&quot;G&quot;</code></p>
<p>如果<code>i</code>是附加到SI单位的首字母，完整的字母将被解释为一个2的幂数单位，这是基于1024而不是1000的，添加<code>B</code>的SI单位则是再将此值乘以8。例如<code>KB</code>，<code>MiB</code>，<code>G</code>和<code>B</code></p>
<p>对于选项中不带参数的布尔选项，即把相应的值设置为<code>true</code>，它们可以添加<code>no</code>设置为false，例如<code>nofoo</code>就相当于<code>foo false</code> 。</p>
<h4 id="流说明-限定-符"><a href="#流说明-限定-符" class="headerlink" title="流说明(限定)符"></a>流说明(限定)符</h4><ul>
<li>很多选项是作用于单独的流的，例如码率(bitrate)或者编码(codec)，流说明符就是精确的为每个流指定相应的选项。</li>
<li>一个流说明符是一个以冒号分隔的字符串，其中分隔出的部分是附加选项，例如<code>-codec:a:1 ac3</code>表示编码器是对第2音频流以ac3编码。</li>
<li>一个流说明符可能匹配多个流，则该选项是所有匹配项的选项，例如<code>-b:a 128k</code>表示所有的音频流都是128k的码率。</li>
<li>一个空的流说明符匹配所有的流，例如<code>-codec copy</code>或者<code>-codec: copy</code>表示所有的流都不进行再次编码(包括视频和音频)</li>
</ul>
<p>可能的流说明符有：</p>
<ul>
<li><strong><code>stream_index</code></strong>:匹配流的索引，例如<code>-threads:1 4</code>表示对2号流采用4个线程处理</li>
<li><strong><code>stream_type[:stream_index]</code></strong>:<code>stream_type</code>有<code>v</code>表示视频，<code>a</code>表示音频，<code>s</code>表示字幕，<code>d</code>表示数据和<code>t</code>表示附加/附件等可能，如果<code>stream_index</code>同时被指定，则匹配该索引对于的该类型的流。例如<code>-codec:v:0 h264</code>表示第1视频流是h.264编码。</li>
<li><strong><code>p:program_id[:stream_index]</code></strong>:如果<code>stream_index</code>被指定，则表示被<code>program_id</code>指定的程序仅作用于<code>stream_index</code>所指流，否则将作用于所有流。</li>
<li><strong><code>#stream_id</code>或者<code>i:stream_id</code></strong>：匹配<code>stream_id</code>所指流(MPEG-TS中的PID)</li>
<li><strong><code>m:key[:value]</code></strong>:匹配在元数据中以标签<code>key</code>=<code>value</code>值的流，如果<code>value</code>没有设置，则匹配所有。</li>
<li><strong><code>u</code></strong>：匹配不能被配置的流，这时编码器必须被定义且有必要的视频维度或者音频采样率之类的信息。<strong>注意</strong>，<code>ffmpeg</code>匹配由元数据标识的状态仅对于输入文件有效。</li>
</ul>
<h4 id="常规选项"><a href="#常规选项" class="headerlink" title="常规选项"></a>常规选项</h4><p>这些常规选项也可以用在<code>ffmpeg</code>项目中其他<code>ff*</code>工具，例如<code>ffplayer</code></p>
<ul>
<li><p><code>-L</code>：显示授权协议</p>
</li>
<li><p><code>-h，-？，-help，--help[arg]</code>:显示帮助，一个附加选项可以指定帮助显示的模式，如果没有参数，则是基本选项(没有特别声明)说明被显示，下面是参数定义</p>
<ul>
<li><code>long</code>：在基本选项说明基础上增加高级选项说明</li>
<li><code>full</code>：输出完整的选项列表，包括编(解)码器，分离器混合器以及滤镜等等的共享和私有选项</li>
<li><code>decoder=decoder_name</code>：输出指定解码器名的详细信息。可以使用<code>-decoders</code>来获取当前支持的所有解码器名</li>
<li><code>encoder=encoder_name</code>：输出指定编码器名的详细信息。可以使用<code>-encoders</code>来获取当前支持的所有编码器名</li>
<li><code>demuxer=demuxer_name</code>：输出指定分离器名详细信息。可以使用<code>-formats</code>来获取当前支持的所有分离器和混合器</li>
<li><code>muxer=muxer_name</code>：输出指定混合器名详细信息。可以使用<code>-formats</code>来获取当前支持的所有分离器和混合器</li>
<li><code>filter=filter_name</code>：输出指定滤镜名的详细信息。可以使用<code>-filters</code>来获取当前支持的所有滤镜</li>
</ul>
</li>
<li><p><code>-version</code>：显示版</p>
</li>
<li><p><code>-buildconf</code> : 显示构建选项</p>
</li>
<li><p><code>-formats</code>：显示所有有效的格式(包括设备)</p>
</li>
<li><p><code>-devices</code>：显示有效设备</p>
</li>
<li><p><code>-codecs</code>：显示所有已支持的编码(libavcodec中的)</p>
</li>
<li><p><code>-decoders</code>：显示所有有效解码器</p>
</li>
<li><p><code>-encoders</code>：显示所有有效的编码器</p>
</li>
<li><p><code>-bsfs</code>：显示有效的数据流(bitstream)滤镜</p>
</li>
<li><p><code>-protocols</code>：显示支持的协议</p>
</li>
<li><p><code>-filters</code>：显示libavfilter中的滤镜</p>
</li>
<li><p><code>-pix_fmts</code>：显示有效的像素(pixel)格式</p>
</li>
<li><p><code>-sample_fmts</code>：显示有效的实例格式</p>
</li>
<li><p><code>-layouts</code>：显示信道名字和信道布局</p>
</li>
<li><p><code>-colors</code>：显示注册的颜色名</p>
</li>
<li><p><code>-sources device[,opt1=val1[,opt2=val]...]</code>：显示自动识别的输入设备源。一些设备可能需要提供一些系统指派的源名字而不能自动识别。返回的列表不能认为一定是完整的(即有可能还有设备没有列出来)</p>
<p><code>ffmpeg -sources pulse,server=192.168.0.4</code></p>
</li>
<li><p><code>-sinks device[,opt1=val1[,opt2=val]...]</code>:显示自动识别的输出设备。一些设备可能需要提供一些系统指派的源名字而不能自动识别。返回的列表不能认为一定是完整的(即有可能还有设备没有列出来)</p>
<p><code>ffmpeg -sinks pulse,server=192.168.0.4</code></p>
</li>
<li><p><code>-loglevel [repeat+]loglevel 或者 -v [repeat+]loglevel</code>：设置日志层次。如果附加有<code>repeat+</code>则表示从第一条非压缩行到达到最后消息n次之间的行将被忽略。<code>&quot;repeat&quot;</code>也可以一直使用，如果没有现有日志层级设置，则采用默认日志层级。如果有多个日志层级参数被获取，使用<code>&quot;repeat&quot;</code>不改变当前日志层级。日志层级是一个字符串或数值，有以下可能值：</p>
<ul>
<li><p><code>quiet,-8</code>，什么都不输出，是无声的</p>
</li>
<li><p><code>panic,0</code>，仅显示造成进程失败的致命错误，它当前不能使用</p>
</li>
<li><p><code>fatal,8</code>仅仅显示致命错误，这些错误使得处理不能继续</p>
</li>
<li><p><code>error,16</code>显示所有的错误，包括可以回收的错误(进程还可以继续的)</p>
</li>
<li><p><code>warning,24</code>显示所有警告和错误，任何错误或者意外事件相关信息均被显示</p>
</li>
<li><p><code>info,32</code>显示过程中的信息，还包括警告和错误，则是默认值</p>
</li>
<li><p><code>verbose,40</code>类似<code>info</code>，但更冗长</p>
</li>
<li><p><code>debug,48</code>显示所有，包括调试信息</p>
</li>
<li><p><code>trace,56</code></p>
<p>默认的日志输出是stderr设备，如果在控制台支持颜色，则错误和警告标记的颜色将被显示处理，默认日志的颜色设置可以由环境变量的<code>AV_LOG_FORCE_NOCOLOR</code>或者<code>NO_COLOR</code>或者环境变量<code>AV_LOG_RORCE_COLOR</code>覆盖。环境变量<code>NO_COLOR</code>不推荐使用，因为其已经不被新版本支持。</p>
</li>
</ul>
</li>
<li><p><code>-report</code>：复制所有命令行和控制台输出到当前目录下名为<code>program-YYYMMDD-HHMMSS.log</code>文件中。这常用于报告bug，所以一般会同时设置<code>-loglevel verbose</code></p>
<p>设置环境变量<code>FFREPORT</code>可以起到相同的效果。如果值是一个以<code>：</code>分隔的关键值对，则将影响到报告效果。值中的特殊符号或者分隔符<code>：</code>必须被转义(参考ffmepg-utils手册中”引用逃逸”(“Quoting and escaping”)章节)。以下是选项值范围：</p>
<ul>
<li><p>file：设置报告文件名字，<code>%p</code>被扩展为程序名字，<code>%t</code>是时间码，<code>%%</code>表示一个字符<code>%</code></p>
</li>
<li><p>level：用数字设定日志信息详略程度(参考<code>-longlevel</code>)</p>
<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`FFREPORT=file=ffreport.log:level=32 ffmpeg -i input output`</span><br></pre></td></tr></table></figure>
<p>会把日志信息输出到环境变量定义的文件中， 内容包括简要过程信息，警告和错误。</p>
</li>
</ul>
</li>
<li><p><code>-hide_banner</code>：禁止打印输出banner。所有FFmpeg工具使用中常规都会在前面显示一些版权通知、编译选项和库版本等，这个选项可以禁止这部分的显示。</p>
</li>
<li><p><code>cpuflags flags(global)</code>：允许设置或者清除cpu标志性和。当前这个选项主要还是测试特性，不要使用，除非你明确需要：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -cpuflags -sse+mmx ... </span><br><span class="line">ffmpeg -cpuflags mmx ... </span><br><span class="line">ffmpeg -cpuflags 0 ...</span><br></pre></td></tr></table></figure>
<p>可能的选项参数有：</p>
<ul>
<li><p>x86</p>
<ul>
<li>mmx</li>
<li>mmxext</li>
<li>sse</li>
<li>sse2</li>
<li>sse2slow</li>
<li>sse3</li>
<li>atom</li>
<li>sse4.1</li>
<li>sse4.2</li>
<li>avx</li>
<li>avx2</li>
<li>xop</li>
<li>fma3</li>
<li>fma4</li>
<li>3dnow</li>
<li>3dnowext</li>
<li>bmi1</li>
<li>bmi2</li>
<li>cmov</li>
</ul>
</li>
<li><p>ARM</p>
<ul>
<li>armv5te</li>
<li>armv6</li>
<li>armv6t2</li>
<li>vfp</li>
<li>vfpv3</li>
<li>neon</li>
<li>setend</li>
</ul>
</li>
<li><p>AArch64</p>
<ul>
<li>armv8</li>
<li>vfp</li>
<li>neon</li>
</ul>
</li>
<li><p>PowerPC</p>
<ul>
<li>altivec</li>
</ul>
</li>
<li><p>Specific Processors</p>
<ul>
<li>pentium2</li>
<li>pentium3</li>
<li>pentium4</li>
<li>k6</li>
<li>athlon</li>
<li>athlonxp</li>
<li>k8</li>
</ul>
</li>
</ul>
</li>
<li><p><code>-opencl_bench</code>：输出所有效OpenCL设备的基准测试情况。当前选项仅在编译FFmepg中打开了<code>--enable-opencl</code>才有效。</p>
<p>当FFmpeg指定了<code>--enable-opencl</code>编译后，这个选项还可以通过全局参数<code>-opencl_options</code>进行设定，参考OpenCL选项，在ffmpeg-utils手册中对于选项的支持情况，这包括在特定的平台设备上支持OpenCL的能力。默认，FFmpeg会运行在首选平台的首选设备上，通过设置全局的OpenCL则可以实现在选定的OpenCL设备上运行，这样就可以在更快的OpenCL设备上运行(平时节点，需要时才选用性能高但耗电的设备)</p>
<p>这个选项有助于帮助用户了解信息以进行有效配置。它将在每个设备上运行基准测试，并以性能排序所有设备，用户可以在随后调用<code>ffmpeg</code>时使用<code>-opencl_options</code>配置合适的OpenCL加速特性。</p>
<p>一般以下面的步骤使用这个参数：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -opencl_bench        </span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>注意</strong>输出中第一行的平台ID(<em>pidx</em>)和设备ID(<em>didx</em>)，然后在选择平台和设备用于命令行：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -opencl_options platform_idx=pidx:device_idx=didx ...</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
<li><p><code>opencl_options options(global)</code>:设置OpenCL环境选项，这个选项仅仅在FFmpeg编译选项中打开了<code>--enable-opencl</code>才有效。</p>
<p><em>options</em>必须是一个由<code>:</code>分隔的<code>key=value</code>键值对列表。参考OpenCL选项，在ffmpeg-utils手册中对于选项的支持情况</p>
</li>
</ul>
<h4 id="AV选项"><a href="#AV选项" class="headerlink" title="AV选项"></a>AV选项</h4><p>这些选项由特定的库提供(如libavformat，libavdevice以及libavcodec)。为了更多的了解AV选项，使用<code>-help</code>进行进一步了解。它们可以指定下面2个分类：</p>
<ul>
<li>generic(常规)：这类选项可以用于设置容器、设备、编码器、解码器等。通用选项对列在<code>AVFormatContext</code>中的容器/设备以及<code>AVCodecContext</code>中的编码有效。</li>
<li>private(私有)：这类仅对特定的容器、设备或者编码有效。私有选项由相应的 容器/设备/编码 指定(确定)。</li>
</ul>
<p>例如要在一个默认为ID3v2.4为头的MP3文件中写入ID3v2.3头，需要使用id3v2_version 私有选项来对MP3混流：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i input.flac -id3v2_version 3 out.mp3</span><br></pre></td></tr></table></figure>
</blockquote>
<p>所有编码AV选项是针对单独流的，所以必须详细指定。</p>
<p><strong>注意</strong></p>
<ol>
<li><code>-nooption</code>语法不能被用于AV选项中的布尔值项目，而必须使用<code>-option 0/-option 1</code></li>
<li>以往使用<code>v/a/s</code>命名指定每个流的AV选项语法已经不建议使用，它们很快就会失效移除。</li>
</ol>
<h4 id="主要选项"><a href="#主要选项" class="headerlink" title="主要选项"></a>主要选项</h4><ul>
<li><p><code>-f fmt (input/output)</code> :指定输入或者输出文件格式。常规可省略而使用依据扩展名的自动指定，但一些选项需要强制明确设定。</p>
</li>
<li><p><code>-i filename (input)</code>：指定输入文件</p>
</li>
<li><p><code>-y (global)</code>：默认自动覆盖输出文件，而不再询问确认。</p>
</li>
<li><p><code>-n (global)</code>:不覆盖输出文件，如果输出文件已经存在则立即退出</p>
</li>
<li><p>-<code>c[:stream_specifier] codec (input/output,per-stream)</code></p>
</li>
<li><p><code>-codec[:stream_specifier] codec (input/output,per-stream)</code> 为特定的文件选择编/解码模式，对于输出文件就是编码器，对于输入或者某个流就是解码器。选项参数中<code>codec</code>是编解码器的名字，或者是<code>copy</code>(仅对输出文件)则意味着流数据直接复制而不再编码。例如： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i INPUT -map 0 -c:v libx264 -c:a copy OUTPUT</span><br></pre></td></tr></table></figure>
<p>是使用libx264编码所有的视频流，然后复制所有的音频流。</p>
<p>再如除了特殊设置外所有的流都由<code>c</code>匹配指定： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i INPUT -map 0 -c copy -c:v:1 libx264 -c:a:137 libvorbis OUTPUT</span><br></pre></td></tr></table></figure>
<p>这将在输出文件中第2视频流按libx264编码，第138音频流按libvorbis编码，其余都直接复制输出。</p>
</li>
<li><p><code>-t duration (input/output)</code>:限制输入/输出的时间。如果是在<code>-i</code>前面，就是限定从输入中读取多少时间的数据；如果是用于限定输出文件，则表示写入多少时间数据后就停止。<code>duration</code>可以是以秒为单位的数值或者 <code>hh:mm:ss[.xxx]</code>格式的时间值。 <strong>注意</strong><code>-to</code>和<code>-t</code>是互斥的，<code>-t</code>有更高优先级。</p>
</li>
<li><p><code>-to position (output)</code>:只写入<code>position</code>时间后就停止，<code>position</code>可以是以秒为单位的数值或者 <code>hh:mm:ss[.xxx]</code>格式的时间值。 <strong>注意</strong><code>-to</code>和<code>-t</code>是互斥的，<code>-t</code>有更高优先级。</p>
</li>
<li><p><code>-fs limit_size (output)</code>:设置输出文件大小限制，单位是字节(bytes)。</p>
</li>
<li><p><code>-ss position (input/output)</code>:</p>
<ul>
<li>当在<code>-i</code>前，表示定位输入文件到<code>position</code>指定的位置。<strong>注意</strong>可能一些格式是不支持精确定位的，所以<code>ffmpeg</code>可能是定位到最接近<code>position</code>(在之前)的可定位点。当有转码发生且<code>-accurate_seek</code>被设置为启用(默认)，则实际定位点到<code>position</code>间的数据被解码出来但丢弃掉。如果是复制模式或者<code>-noaccurate_seek</code>被使用，则这之间的数据会被保留。</li>
<li>当用于输出文件时，会解码丢弃<code>position</code>对应时间码前的输入文件数据。</li>
<li><code>position</code>可以是以秒为单位的数值或者 <code>hh:mm:ss[.xxx]</code>格式的时间值</li>
</ul>
</li>
<li><p><code>-itsoffset offset (input)</code>:设置输入文件的时间偏移。<code>offset</code>必须采用时间持续的方式指定，即可以有<code>-</code>号的时间值(以秒为单位的数值或者 <code>hh:mm:ss[.xxx]</code>格式的时间值)。偏移会附加到输入文件的时间码上，意味着所指定的流会以时间码+偏移量作为最终输出时间码。</p>
</li>
<li><p><code>-timestamp date (output)</code>:设置在容器中记录时间戳。</p>
<p>date 必须是一个时间持续描述格式，即</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[(YYYY-MM-DD|YYYYMMDD)[T|t| ]]((HH:MM:SS[.m...]]])|(HHMMSS[.m...]]]))[Z]</span><br><span class="line">或者为</span><br><span class="line">now</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>-metadata[:metadata_specifier] key=value (output,per-metadata)</code>：指定元数据中的键值对。</p>
<p>流或者章的<code>metadata_specifier</code>可能值是可以参考文档中<code>-map_metadata</code>部分了解。</p>
<p>简单的覆盖<code>-map_metadata</code>可以通过一个为空的选项实现，例如：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i in.avi -metadata title=&quot;my title&quot; out.flv</span><br></pre></td></tr></table></figure>
<p>设置第1声道语言:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i INPUT -metadata:s:a:0 language=eng OUTPUT</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
<li><p><code>-taget type (output)</code>：指定目标文件类型(vcd,svcd,dvd,dv,dv50)，类型还可以前缀一个<code>pal-</code>,<code>ntsc-</code>或者<code>film-</code>来设定更具体的标准。所有的格式选项(码率、编码、缓冲尺寸)都会自动设置，而你仅仅只需要设置目标类型：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i myfile.avi -taget vcd /tmp/vcd.mpg</span><br></pre></td></tr></table></figure>
</blockquote>
<p>当然，你也可以指定一些额外的选项，只要你知道这些不会与标准冲突，如：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i myfile.avi -target vcd -bf 2 /tmp/vcd.mpg</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
<li><p><code>-dframes number (output)</code>:设定指定<code>number</code>数据帧到输出文件，这是<code>-frames:d</code>的别名。</p>
</li>
<li><p><code>frames[:stream_specifier] framecount (output,per-stream)</code>:在指定计数帧后停止写入数据。</p>
</li>
<li><p><code>-q[:stream_specifier] q (output,per-stream)</code></p>
</li>
<li><p><code>-qscale[:stream_specifier] q (output,per-stream)</code></p>
<p>使用固定的质量品质(VBR)。用于指定<code>q|qscale</code>编码依赖。如果<code>qscale</code>没有跟<code>stream_specifier</code>则只适用于视频。其中值<code>q</code>取值在0.01-255,越小质量越好。</p>
</li>
<li><p><code>-filter[:stream_specifier] filtergraph (output,per-stream)</code>:创建一个由<code>filtergraph</code>指定的滤镜，并应用于指定流。</p>
<p><code>filtergraph</code>是应用于流的滤镜链图，它必须有一个输入和输出，而且流的类型需要相同。在滤镜链图中，从<code>in</code>标签指定出输入，从<code>out</code>标签出输出。要了解更多语法，请参考<code>ffmpeg－filters</code>手册。</p>
<p>参考<code>－filter_complex</code>选项以了解如何建立多个输入／输出的滤镜链图。</p>
</li>
<li><p><code>－filter_script［：stream_specifier］ filename (output，per－stream)</code>：这个选项类似于<code>－filter</code>，只是这里的参数是一个文件名，它的内容将被读取用于构建滤镜链图。</p>
</li>
<li><p><code>－pre［：stream_specifier］ preset_name (output，per－stream)</code>：指定预设名字的流(单个或者多个)。</p>
</li>
<li><p><code>－stats (global)</code>：输出编码过程／统计，这是系统默认值，如果你想禁止，则需要采用<code>－nostats</code>。</p>
</li>
<li><p><code>－progress url (global)</code>：发送友好的处理过程信息到<code>url</code>。处理过程信息是一种键值对(key=value)序列信息，它每秒都输出，或者在一次编码结束时输出。信息中最后的一个键值对表明了当前处理进度。</p>
</li>
<li><p><code>-stdin</code>:允许标准输入作为交互。在默认情况下除非标准输入作为真正的输入。要禁用标准输入交互，则你需要显式的使用<code>-nostdin</code>进行设置。禁用标准输入作为交互作用是有用的，例如FFmpeg是后台进程组，它需要一些相同的从shell开始的调用(<code>ffmpeg ... &lt;/dev/null</code>)。</p>
</li>
<li><p><code>-debug_ts (global)</code>：打印时间码信息，默认是禁止的。这个选项对于测试或者调试是非常有用的特性，或者用于从一种格式切换到另外的格式(包括特性)的时间成本分析，所以不用于脚本处理中。还可以参考<code>-fdebug ts</code>选项。</p>
</li>
<li><p><code>-attach filename (output)</code>：把一个文件附加到输出文件中。这里只有很少文件类型能被支持，例如使用Matroska技术为了渲染字幕的字体文件。附件作为一种特殊的流类型，所以这个选项会添加一个流到文件中，然后你就可以像操作其他流一样使用每种流选项。在应用本选项时，附件流须作为最后一个流(例如根据<code>-map</code>映射流或者自动映射时需要注意)。<strong>注意</strong>对于<code>Matroska</code>你也可以在元数据标签中进行类型设定： &gt; ffmpeg -i INPUT -attach DejaVuSans.ttf -metadata:s:2 mimetype=application/x-truetype-font out.mkv</p>
</li>
</ul>
<p>(这时要访问到附件流，则就是访问输出文件中的第3个流)</p>
<ul>
<li><p><code>-dump_attachment[:stream_specifier] filename (input,per-stream)</code>：从输入文件中解出指定的附件流到文件filename： &gt; ffmpeg -dump_attachment:t:0 out.ttf -i INPUT</p>
<p>如果想一次性把所有附件都解出来，则 &gt; ffmpeg -dump_attachment:t “” -i INPUT</p>
<p>技术说明：附件流是作为编码扩展数据来工作的，所以其他流数据也能展开，而不仅仅是这个附件属性。</p>
</li>
<li><p><code>-noautorotate</code>：禁止自动依据文件元数据旋转视频。</p>
</li>
</ul>
<h4 id="视频-video-选项"><a href="#视频-video-选项" class="headerlink" title="视频(video)选项"></a>视频(video)选项</h4><ul>
<li><p><code>-vframes number (output)</code>：设置输出文件的帧数，是<code>-frames:v</code>的别名。</p>
</li>
<li><p><code>-r[:stream_specifier] fps (input/output,per-stream)</code>：设置帧率(一种Hz值，缩写或者分数值)。</p>
<p>在作为输入选项时，会忽略文件中存储的时间戳和时间戳而产生的假设恒定帧率<code>fps</code>，即强制按设定帧率处理视频产生(快进/减缓效果)。这不像<code>-framerate</code>选项是用来让一些输入文件格式如image2或者v412(兼容旧版本的FFmpeg)等，要注意这一点区别，而不要造成混淆。</p>
<p>作为输出选项时，会复制或者丢弃输入中个别的帧以满足设定达到<code>fps</code>要求的帧率。</p>
</li>
<li><p><code>-s[:stream_specifier] size (input/output,per-stream)</code>：设置帧的尺寸。</p>
<p>当作为输入选项时，是私有选项<code>video_size</code>的缩写，一些文件没有把帧尺寸进行存储，或者设备对帧尺寸是可以设置的，例如一些采集卡或者raw视频数据。</p>
<p>当作为输出选项是，则相当于<code>scale</code>滤镜作用在滤镜链图的最后。请使用<code>scale</code>滤镜插入到开始或者其他地方。</p>
<p>数据的格式是<code>wxh</code>，即<code>宽度值X高度值</code>，例如<code>320x240</code>，(默认同源尺寸)</p>
</li>
<li><p><code>aspect[:stream_specifier] aspect (output,per-stream)</code>：指定视频的纵横比(长宽显示比例)。<code>aspect</code>是一个浮点数字符串或者<code>num:den</code>格式字符串(其值就是num/den)，例如”4:3”,”16:9”,”1.3333”以及”1.7777”都是常用参数值。</p>
<p>如果还同时使用了<code>-vcodec copy</code>选项，它将只影响容器级的长宽比，而不是存储在编码中的帧纵横比。</p>
</li>
<li><p><code>-vn (output)</code>：禁止输出视频</p>
</li>
<li><p><code>-vcodec codec (output)</code>：设置视频编码器，这是<code>-codec:v</code>的一个别名。</p>
</li>
<li><p><code>-pass[:stream_specifier] n (output,per-stream)</code>:选择当前编码数(1或者2)，它通常用于2次视频编码的场景。第一次编码通常把分析统计数据记录到1个日志文件中(参考<code>-passlogfile</code>选项)，然后在第二次编码时读取分析以精确要求码率。在第一次编码时通常可以禁止音频，并且把输出文件设置为<code>null</code>，在windows和类unix分别是:</p>
<blockquote>
<p>ffmpeg -i foo.mov -c:v libxvid -pass 1 -an -f rawvideo -y NUL ffmpeg -i foo.mov -c:v libxvid -pass 1 -an -f rawvideo -y /dev/null</p>
</blockquote>
</li>
<li><p><code>-passlogfile[:stream_specifier] prefix (output,per-stream)</code>：设置2次编码模式下日志文件存储文件前导，默认是”ffmepg2pass”，则完整的文件名就是”PREFIX-N.log”，其中的N是指定的输出流序号(对多流输出情况)</p>
</li>
<li><p><code>-vf filtergraph (output)</code>：创建一个<code>filtergraph</code>的滤镜链并作用在流上。它实为<code>-filter:v</code>的别名，详细参考<code>-filter</code>选项。</p>
</li>
</ul>
<h4 id="高级视频选项"><a href="#高级视频选项" class="headerlink" title="高级视频选项"></a>高级视频选项</h4><ul>
<li><p><code>-pix_fmt[:stream_specifier] format (input/output,per-stream)</code>：设置像素格式。使用<code>-pix_fmts</code>可以显示所有支持的像素格式。如果设置的像素格式不能被选中(启用)，则ffmpeg会输出一个警告和并选择这个编码最好(兼容)的像素格式。如果<code>pix_fmt</code>前面前导了一个<code>+</code>字符，ffmepg会在要求的像素格式不被支持时退出，这也意味着滤镜中的自动转换也会被禁止。如果<code>pix_fmt</code>是单独的<code>+</code>，则ffmpeg选择和输入(或者滤镜通道)一样的像素格式作为输出，这时自动转换也会被禁止。</p>
</li>
<li><p><code>-sws_flags flags (input/output)</code>:选择<code>SwScaler</code>放缩标志量。</p>
</li>
<li><p><code>-vdt n</code>：丢弃的门限设置。</p>
</li>
<li><p><code>-rc_override[:stream_specifier] override (output,per-stream)</code>:在特定时间范围内的间隔覆盖率，<code>override</code>的格式是”int\int\int”。其中前两个数字是开始帧和结束帧，最后一个数字如果为正则是量化模式，如果为负则是品质因素。</p>
</li>
<li><p><code>-ilme</code>：支持交错编码(仅MPEG-2和MPEG-4)。如果你的输入是交错的，而且你想保持交错格式，又想减少质量损失，则选此项。另一种方法是采用<code>-deinterlace</code>对输入流进行分离，但会引入更多的质量损失。</p>
</li>
<li><p><code>-psnr</code>：计算压缩帧的<code>PSNR</code></p>
</li>
<li><p><code>-vstats</code>：复制视频编码统计分析到日志文件<code>vstats_HHMMSS.log</code></p>
</li>
<li><p><code>-vstats_file file</code>:复制视频编码统计分析到<code>file</code>所指的日志文件中。</p>
</li>
<li><p><code>-top[:stream_specifier] n (output,per-stream)</code>: 指明视频帧数据描述的起点。<code>顶部=1/底部=0/自动=-1</code>(以往CRT电视扫描线模式)</p>
</li>
<li><p><code>-dc precision</code>：Intra_dc_precision值。</p>
</li>
<li><p><code>-vtag fourcc/tag (output)</code>:是<code>-tag:v</code>的别名，强制指定视频标签/fourCC (FourCC全称Four-Character Codes，代表四字符代码 (four character code), 它是一个32位的标示符，其实就是typedef unsigned int FOURCC;是一种独立标示视频数据流格式的四字符代码。)</p>
</li>
<li><p><code>-qphist (global)</code>：显示<code>QP</code>直方图。</p>
</li>
<li><p><code>-vbsf bitstream_filter</code>：参考<code>-bsf</code>以进一步了解。</p>
</li>
<li><p><code>-force_key_frames[:stream_specifier] time[,time...] (output,per-stream)</code> ：(见下)</p>
</li>
<li><p><code>-force_key_frames[:stream_specifier] expr:expr (output,per-stream)</code>：强制时间戳位置帧为关键帧，更确切说是从第一帧起每设置时间都是关键帧(即强制关键帧率)。</p>
<p>如果参数值是以<code>expr:</code>前导的，则字符串<code>expr</code>为一个表达式用于计算关键帧间隔数。关键帧间隔值必须是一个非零数值。</p>
<p>如果一个时间值是”<code>chapters</code> [delta]”则表示文件中从<code>delta</code>章开始的所有章节点计算以秒为单位的时间，并把该时间所指帧强制为关键帧。这个选项常用于确保输出文件中所有章标记点或者其他点所指帧都是关键帧(这样可以方便定位)。例如下面的选项代码就可以使“第5分钟以及章节chapters-0.1开始的所有标记点都成为关键帧”：</p>
<blockquote>
<p>-force_key_frames 0:05:00,chapters-0.1</p>
</blockquote>
<p>其中表达式<code>expr</code>接受如下的内容：</p>
<ul>
<li><p><code>n</code>：当前帧序数，从0开始计数</p>
</li>
<li><p><code>n_forced</code>：强制关键帧数</p>
</li>
<li><p><code>prev_forced_n</code>：之前强制关键帧数，如果之前还没有强制关键帧，则其值为<code>NAN</code></p>
</li>
<li><p><code>prev_forced_t</code>：之前强制关键帧时间，如果之前还没有强制关键帧则为<code>NAN</code></p>
</li>
<li><p><code>t</code>：当前处理到的帧对应时间。</p>
<p>例如要强制每5秒一个关键帧：</p>
<blockquote>
<p>-force_key_frames expr:gte(t,n_forced*5)</p>
</blockquote>
<p>从13秒后每5秒一个关键帧：</p>
<blockquote>
<p>-force_key_frames expr:if(isnan(prev_forced_t),gte(t,13),gte(t,prev_forced_t+5))</p>
</blockquote>
<p><strong>注意</strong>设置太多强制关键帧会损害编码器前瞻算法效率，采用固定<code>GOP</code>选项或采用一些近似设置可能更高效。</p>
</li>
</ul>
</li>
<li><p><code>-copyinkf[:stream_specifier] (output,per-stream)</code>:流复制时同时复制非关键帧。</p>
</li>
<li><p><code>-hwaccel[:stream_specifier] hwaccel (input,per-stream)</code>：使用硬件加速解码匹配的流。允许的<code>hwaccel</code>值为：</p>
<ul>
<li><p><code>none</code>：没有硬件加速(默认值)</p>
</li>
<li><p><code>auto</code>：自动选择硬件加速</p>
</li>
<li><p><code>vda</code>：使用Apple的VDA硬件加速</p>
</li>
<li><p><code>vdpau</code>：使用VDPAU(Video Decode and Presentation API for Unix，类unix下的技术标准)硬件加速</p>
</li>
<li><p><code>dxva2</code>：使用DXVA2 (DirectX Video Acceleration，windows下的技术标准) 硬件加速。</p>
<p>这个选项可能并不能起效果(它依赖于硬件设备支持和选择的解码器支持)</p>
<p><strong>注意</strong>：很多加速方法(设备)现在并不比现代CPU快了，而且额外的<code>ffmpeg</code>需要拷贝解码的帧(从GPU内存到系统内存)完成后续处理(例如写入文件)，从而造成进一步的性能损失。所以当前这个选项更多的用于测试。</p>
</li>
</ul>
</li>
<li><p><code>-hwaccel_device:[:stream_specifier] hwaccel_device (input,per-stream)</code>：选择一个设备用于硬件解码加速。这个选项必须同时指定了<code>-hwaccel</code>才可能生效。它也依赖于指定的设备对于特定编码的解码加速支持性能。</p>
<ul>
<li><code>vdpau</code>：对应于<code>VDPAU</code>，在<code>X11</code>(类Unix)显示/屏幕 上的，如果这个选项值没有选中，则必须在<code>DISPLAY</code>环境变量中有设置。</li>
<li><code>dxva2</code>：对应于<code>DXVA2</code>，这个是显示硬件(卡)的设备号，如果没有指明，则采用默认设备(对于多个卡时)。</li>
</ul>
</li>
</ul>
<h4 id="音频选项"><a href="#音频选项" class="headerlink" title="音频选项"></a>音频选项</h4><ul>
<li><code>-aframes number (output)</code>：设置<code>number</code>音频帧输出，是<code>-frames:a</code>的别名</li>
<li><code>-ar[:stream_specifier] freq (input/output,per-stream)</code>:设置音频采样率。默认是输出同于输入。对于输入进行设置，仅仅通道是真实的设备或者raw数据分离出并映射的通道才有效。对于输出则可以强制设置音频量化的采用率。</li>
<li><code>-aq q (output)</code>：设置音频品质(编码指定为VBR)，它是<code>-q:a</code>的别名。</li>
<li><code>-ac[:stream_specifier] channels (input/output,per-stream)</code>：设置音频通道数。默认输出会有输入相同的音频通道。对于输入进行设置，仅仅通道是真实的设备或者raw数据分离出并映射的通道才有效。</li>
<li><code>-an (output)</code>：禁止输出音频</li>
<li><code>-acode codec (input/output)</code>：设置音频解码/编码的编/解码器，是<code>-codec:a</code>的别名</li>
<li><code>-sample_fmt[:stream_specifier] sample_fmt (output,per-stream)</code>:设置音频样例格式。使用<code>-sample_fmts</code>可以获取所有支持的样例格式。</li>
<li><code>-af filtergraph (output)</code>：对音频使用<code>filtergraph</code>滤镜效果，其是<code>-filter:a</code>的别名，参考<code>-filter</code>选项。</li>
</ul>
<h4 id="高级音频选项"><a href="#高级音频选项" class="headerlink" title="高级音频选项"></a>高级音频选项</h4><ul>
<li><code>-atag fourcc/tag (output)</code>：强制音频标签/fourcc。这个是<code>-tag:a</code>的别名。</li>
<li><code>-absf bitstream_filter</code>：要深入了解参考<code>-bsf</code></li>
<li><code>-guess_layout_max channels (input,per-stream)</code>:如果音频输入通道的布局不确定，则尝试猜测选择一个能包括所有指定通道的布局。例如：通道数是2，则<code>ffmpeg</code>可以认为是2个单声道，或者1个立体声声道而不会认为是6通道或者5.1通道模式。默认值是总是试图猜测一个包含所有通道的布局，用0来禁用。</li>
</ul>
<h4 id="字幕选项"><a href="#字幕选项" class="headerlink" title="字幕选项"></a>字幕选项</h4><ul>
<li><code>-scodec codec (input/output)</code>：设置字幕解码器，是<code>-codec:s</code>的别名。</li>
<li><code>-sn (output)</code>：禁止输出字幕</li>
<li><code>-sbsf bitstream_filter</code>：深入了解请参考<code>-bsf</code></li>
</ul>
<h4 id="高级字幕选项"><a href="#高级字幕选项" class="headerlink" title="高级字幕选项"></a>高级字幕选项</h4><ul>
<li><p><code>-fix_sub_duration</code>：修正字幕持续时间。对每个字幕根据接下来的数据包调整字幕流的时间常数以防止相互覆盖(第一个没有完下一个就出来了)。这对很多字幕解码来说是必须的，特别是DVB字幕，因为它在原始数据包中只记录了一个粗略的估计值，最后还以一个空的字幕帧结束。</p>
<p>这个选项可能失败，或者出现夸张的持续时间或者合成失败，这是因为数据中有非单调递增的时间戳。</p>
<p><strong>注意</strong>此选项将导致所有数据延迟输出到字幕解码器，它会增加内存消耗，并引起大量延迟。</p>
</li>
<li><p><code>-canvas_size size</code>：设置字幕渲染区域的尺寸(位置)</p>
</li>
</ul>
<h4 id="高级选项"><a href="#高级选项" class="headerlink" title="高级选项"></a>高级选项</h4><ul>
<li><p><code>-map [-]input_file_id[:stream_specifier][,sync_file_id[:stream_specifier]] | [linklabel] (output)</code>：设定一个或者多个输入流作为输出流的源。每个输入流是以<code>input_file_id</code>序数标记的输入文件和<code>input_stream_id</code>标记的流序号共同作用指明，它们都以0起始计数。如果设置了<code>sync_file_id:stream_specifier</code>，则把这个输入流作为同步信号参考。</p>
<p>命令行中的第一个<code>-map</code>选项指定了输出文件中第一个流的映射规则(编号为0的流，0号流)，第二个则指定1号流的，以此类推。</p>
<p>如果在流限定符前面有一个<code>-</code>标记则表明创建一个“负”映射，这意味着禁止该流输出，及排除该流。</p>
<p>一种替代的形式是在复合滤镜中利用<code>[linklabel]</code>来进行映射(参看<code>-filter_complex</code>选项)。其中的<code>linklabel</code>必须是输出滤镜链图中已命名的标签。</p>
<p>例子：映射第一个输入文件的所有流到输出文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i INPUT -map 0 output</span><br></pre></td></tr></table></figure>
<p>又如，如果在输入文件中有两路音频流，则这些流的标签就是”0:0”和”0:1”，你可以使用<code>-map</code>来选择某个输出，例如： &gt; ffmpeg -i INPUT -map 0:1 out.wav</p>
<p>这将只把输入文件中流标签为”0:1”的音频流单独输出到out.wav中。</p>
<p>再如，从文件a.mov中选择序号为2的流(流标签0:2)，以及从b.mov中选择序号为6的流(流标签1:6)，然后共同复制输出到out.mov需要如下写: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -i a.mov -i b.mov -c copy -map 0:2 -map 1:6 out.mov</span><br><span class="line">选择所有的视频和第三个音频流则是:</span><br><span class="line">ffmpeg -i INPUT -map 0:v -map:a:2 OUTPUT</span><br><span class="line">选择所有的流除了第二音频流外的流进行输出是：</span><br><span class="line">ffmpeg -i INPUT -map 0 -map -0:a:1 OUTPUT</span><br><span class="line">选择输出英语音频流:</span><br><span class="line">ffmpeg -i INPUT -map 0:m:language:eng OUTPUT</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>应用了该选项将自动禁用默认的映射。</p>
</li>
<li><p><code>-ignore_unknown</code>：如果流的类型未知则忽略，而不进行复制。</p>
</li>
<li><p><code>-copy_unknown</code>：复制类型未知的流。</p>
</li>
<li><p><code>-map_channel [input_file_id.stream_specifier.channel_id|-1][:output_file_id.stream_specifier]</code>:从输入文件中指定映射一个通道的音频到输出文件指定流。如果<code>output_file_id.stream_specifier</code>没有设置，则音频通道将映射到输出文件的所有音频流中。</p>
<p>使用<code>-1</code>插入到<code>input_file_id.stream_specifier.chnnel_id</code>会映射一个静音通道</p>
<p>例如<code>INPUT</code>是一个立体声音频文件，你可以分别选择两个音频通道(下面实际上对于输入是交换了2个音频通道顺序进行输出)： &gt; ffmpeg -i INPUT -map_channel 0.0.1 -map_channel 0.0.0 OUTPUT</p>
<p>如果你想静音第一个通道，而只保留第二通道，则可使用: &gt; ffmpeg -i INPUT -map_channel -1 -map_channel 0.0.1 OUTPUT</p>
<p>以<code>-map_channel</code>选项指定的顺序在输出文件中输出音频流通道布局，即第一个<code>-map_channel</code>对应输出中第一个音频流通道，第二个对应第二个音频流通道，以此类推(只有一个则是单声道，2个是立体声)。联合使用<code>-ac</code>与<code>-map_channel</code>，而且在输入的<code>-map_channel</code>与<code>-ac</code>不匹配(例如只有2个<code>-map_channel</code>，又设置了<code>-ac 6</code>)时将使指定音频流通道提高增益。</p>
<p>你可以详细的对每个输入通道指派输出以分离整个输入文件，例如下面就把有<code>INPUT</code>文件中的两个音频分别输出到两个输出文件中(OUTPUT_CH0 和 OUTPUT_CH1 )： &gt; ffmpeg -i INPUT -map_channel 0.0.0 OUTPUT_CH0 -map_channel 0.0.1 OUTPUT_CH1</p>
<p>下面的例子则把一个立体声音频的两个音频通道分离输出到两个相互独立的流(相当于两个单声道了)中(但还是放置在同一个输出文件中): &gt; ffmpeg -i stereo.wav -map 0:0 -map 0:0 -map_channel 0.0.0:0.0 -map_channel 0.0.1:0.1 -y out.ogg</p>
<p><strong>注意</strong>当前一个输出流仅能与一个输入通道连接，既你不能实现利用<code>-map_channel</code>把多个输入的音频通道整合到不同的流中(从同一个文件或者不同文件)或者是混合它们成为单独的流，例如整合2个单声道形成立体声是不可能的。但是分离一个立体声成为2个独立的单声道是可行的。</p>
<p>如果你需要类似的应用，你需要使用<code>amerge</code>滤镜，例如你需要整合一个媒体(这里是input.mkv)中的2个单声道成为一个立体声通道(保持视频流不变)，你需要采用下面的命令: &gt; ffmpeg -i input.mkv -filter_complex “[0:1] [0:2] amerge” -c:a pcm_s16le -c:v copy output.mkv</p>
</li>
<li><p><code>-map_metadata[:metadata_spec_out] infile[:metadata_spec_in] (output,per-metadata)</code>：在下一个输出文件中从<code>infile</code>读取输出元数据信息。<strong>注意</strong>这里的文件索引也是以0开始计数的，而不是文件名。参数<code>metadata_spec_in/out</code>指定的元数据将被复制，一个元数据描述可以有如下的信息块:</p>
<ul>
<li><p><code>g</code>:全局元数据，这些元数据将作用于整个文件</p>
</li>
<li><p><code>s[:stream_spec]</code>:每个流的元数据，<code>steam_spec</code>的介绍在<code>流指定</code>章节。如果是描述输入流，则第一个被匹配的流相关内容被复制，如果是输出元数据指定，则所有匹配的流相关信息被复制到该处。</p>
</li>
<li><p><code>c:chapter_index</code>:每个章节的元数据，<code>chapter_index</code>也是以0开始的章节索引。</p>
</li>
<li><p><code>p:program_index</code>：每个节目元数据，<code>program_index</code>是以0开始的节目索引</p>
<p>如果元数据指定被省略，则默认是全局的。</p>
<p>默认全局元数据会从第一个输入文件每个流每个章节依次复制(流/章节)，这种默认映射会因为显式创建了任意的映射而失效。一个负的文件索引就可以禁用默认的自动复制。</p>
<p>例如从输入文件的第一个流复制一些元数据作为输出的全局元数据 &gt; ffmpeg -i in.ogg -map_metadata 0:s:0 out.mp3</p>
<p>与上相反的操作，例如复制全局元数据给所有的音频流 &gt; ffmpeg -i in.mkv -map_metadata:s:a 0:g out.mkv</p>
<p><strong>注意</strong>这里简单的<code>0</code>在这里能正常工作是因为全局元数据是默认访问的。</p>
</li>
</ul>
</li>
<li><p><code>-map_chapters input_file_index (output)</code>:从输入文件中复制由<code>input_file_index</code>指定的章节的内容到输出。如果没有匹配的章节，则复制第一个输入文件至少一章内容(第一章)。使用负数索引则禁用所有的复制。</p>
</li>
<li><p><code>-benchmark (global)</code>：在编码结束后显示基准信息。则包括CPU使用时间和最大内存消耗，最大内存消耗是不一定在所有的系统中被支持，它通常以显示为0表示不支持。</p>
</li>
<li><p><code>-benchmark_all (global)</code>:在编码过程中持续显示基准信息，则包括CPU使用时间(音频/视频 的 编/解码)</p>
</li>
<li><p><code>-timelimit duration (global)</code>:ffmpeg在编码处理了<code>duration</code>秒后退出。</p>
</li>
<li><p><code>-dump (global)</code>：复制每个输入包到标准输出设备</p>
</li>
<li><p><code>-hex (global)</code>:复制包时也复制荷载信息</p>
</li>
<li><p><code>-re (input)</code>：以指定帧率读取输入。通常用于模拟一个硬件设备，例如在直播输入流(这时是读取一个文件)。不应该在实际设备或者在直播输入中使用(因为这将导致数据包的丢弃)。默认<code>ffmpeg</code>会尽量以最高可能的帧率读取。这个选项可以降低从输入读取的帧率，这常用于实时输出(例如直播流)。</p>
</li>
<li><p><code>-loop_input</code>：循环输入流。当前它仅作用于图片流。这个选项主要用于FFserver自动化测试。这个选项现在过时了，应该使用<code>-loop 1</code>。</p>
</li>
<li><p><code>-loop_output number_of_times</code>：重复播放<code>number_of_times</code>次。这是对于GIF类型的动画(0表示持续重复而不停止)。这是一个过时的选项，用<code>-loop</code>替代。</p>
</li>
<li><p><code>-vsync parameter</code>：视频同步方式。为了兼容旧，常被设置为一个数字值。也可以接受字符串来作为描述参数值，其中可能的值是:</p>
<ul>
<li><p><code>0,passthrough</code>:每个帧都通过时间戳来同步(从解复用到混合)。</p>
</li>
<li><p><code>1，cfr</code>：帧将复制或者降速以精准达到所要求的恒定帧速率。</p>
</li>
<li><p><code>2，vfr</code>：个别帧通过他们的时间戳或者降速以防止2帧具有相同的时间戳</p>
</li>
<li><p><code>drop</code>：直接丢弃所有的时间戳，而是在混合器中基于设定的帧率产生新的时间戳。</p>
</li>
<li><p><code>-1，auto</code>：根据混合器功能在1或者2中选择，这是默认值。</p>
<p><strong>注意</strong>时间戳可以通过混合器进一步修改。例如<code>avoid_negative_ts</code>被设置时。</p>
<p>利用<code>-map</code>你可以选择一个流的时间戳作为凭据，它可以对任何视频或者音频 不改变或者重新同步持续流到这个凭据。</p>
</li>
</ul>
</li>
<li><p><code>-frame_drop_threshold parameter</code>：丢帧的阀值，它指定后面多少帧内可能有丢帧。在帧率计数时1.0是1帧，默认值是1.1。一个可能的用例是避免在混杂的时间戳或者需要增加精准时间戳的情况下确立丢帧率。</p>
</li>
<li><p><code>-async samples_per_second</code>：音频同步方式。”拉伸/压缩”音频以匹配时间戳。参数是每秒最大可能的音频改变样本。<code>-async 1</code>是一种特殊情况指只有开始时校正，后续不再校正。</p>
<p><strong>注意</strong>时间戳还可以进一步被混合器修改。例如<code>avoid_negative_ts</code>选项被指定时</p>
<p>已不推荐这个选项，而是用<code>aresample</code>音频滤波器代替。</p>
</li>
<li><p><code>-copyts</code>：不处理输入的时间戳，保持它们而不是尝试审核。特别是不会消除启动时间偏移值。</p>
<p><strong>注意</strong>根据<code>vsync</code>同步选项或者特定的混合器处理流程(例如格式选项<code>avoid_negative_ts</code>被设置)输出时间戳会忽略匹配输入时间戳(即使这个选项被设置)</p>
</li>
<li><p><code>-start_at_zero</code>：当使用<code>-copyts</code>,位移输入时间戳作为开始时间0.这意味着使用该选项，同时又设置了<code>-ss</code>，例如<code>-ss 50</code>则输出中会从50秒开始加入输入文件时间戳。</p>
</li>
<li><p>```<br>-copytb mode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">：指定当流复制时如何设置编码时间基准。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>mode</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  参数是一个整数值，可以有如下可能：</span><br><span class="line"></span><br><span class="line">  - `1`表示使用分离器时间基准，从分离器中复制时间戳到编码中。复制可变帧率视频流时需要避免非单调递增的时间戳。</span><br><span class="line">  - `0`表示使用解码器时间基准，使用解码器中获取的时间戳作为输出编码基准。</span><br><span class="line">  - `-1`尝试自动选择，只要能产生一个正常的输出，这是默认值。</span><br><span class="line"></span><br><span class="line">- `-shortest (output)`：完成编码时最短输入端。</span><br><span class="line"></span><br><span class="line">- `-dts_delta_threshold`：时间不连续增量阀值。</span><br><span class="line"></span><br><span class="line">- `-muxdelay seconds (input)`：设置最大 解复用-解码 延迟。参数是秒数值。</span><br><span class="line"></span><br><span class="line">- `-maxpreload seconds (input)`：设置初始的 解复用-解码延迟，参数是秒数值。</span><br><span class="line"></span><br><span class="line">- `-streamid output-stream-index:new-value (output)`:强制把输出文件中序号为output-stream-id的流命名为new-value的值。这对应于这样的场景：在存在了多输出文件时需要把一个流分配给不同的值。例如设置0号流为33号流，1号流为36号流到一个mpegts格式输出文件中(这相当于对流建立链接/别名)：</span><br><span class="line">  </span><br><span class="line">  &gt; ffmpeg -i infile -streamid 0:33 -streamid 1:36 out.ts</span><br><span class="line"></span><br><span class="line">- `-bsf[:stream_specifier] bitstream_filters (output,per-stream)`：为每个匹配流设置bit流滤镜。`bitstream_filters`是一个逗号分隔的bit流滤镜列表。可以使用`-bsfs`来获得当前可用的bit流滤镜。</span><br><span class="line"></span><br><span class="line">  &gt; ffmpeg -i h264.mp4 -c:v copy -bsf:v h264_mp4toannexb -an out.h264 ffmpeg -i file.mov -an -vn -bsf:s mov2textsub -c:s copy -f rawvideo sub.txt</span><br><span class="line"></span><br><span class="line">- `-tag[:stream_specifier codec_tag (input/output,per-stream`：为匹配的流设置标签/fourcc。</span><br><span class="line"></span><br><span class="line">- `-timecode hh:mm:ssSEDff`:指定时间码，这里`SEP`如果是`:`则不减少时间码，如果是`;`或者`.`则可减少。</span><br><span class="line"></span><br><span class="line">  &gt; ffmpeg -i input.mpg -timecode 01:02:03.04 -r 30000/1001 -s ntsc output.mpg</span><br><span class="line"></span><br><span class="line">- `-filter_complex filtergraph (global)`：定义一个复合滤镜，可以有任意数量的输入/输出。最简单的滤镜链图至少有一个输入和一个输出，且需要相同类型。参考`-filter`以获取更多信息(更有价值)。`filtergraph`用来指定一个滤镜链图。关于`滤镜链图的语法`可以参考`ffmpeg-filters`相关章节。</span><br><span class="line"></span><br><span class="line">  其中输入链标签必须对应于一个输入流。filtergraph的具体描述可以使用`file_index:stream_specifier`语法(事实上这同于`-map`)。如果`stream_specifier`匹配到了一个多输出流，则第一个被使用。滤镜链图中一个未命名输入将匹配链接到的输入中第一个未使用且类型匹配的流。</span><br><span class="line"></span><br><span class="line">  使用`-map`来把输出链接到指定位置上。未标记的输出会添加到第一个输出文件。</span><br><span class="line"></span><br><span class="line">  **注意**这个选项参数在用于`-lavfi`源时不是普通的输入文件。 &gt; ffmpeg -i video.mkv -i image.png -filter_complex &#x27;[0:v][1:v]overlay[out]&#x27; -map &#x27;[out]&#x27; out.mkv</span><br><span class="line"></span><br><span class="line">  这里`[0:v]`是第一个输入文件的第一个视频流，它作为滤镜的第一个(主要的)输入，同样，第二个输入文件的第一个视频流作为滤镜的第二个输入。</span><br><span class="line"></span><br><span class="line">  假如每个输入文件只有一个视频流，则我们可以省略流选择标签，所以上面的内容在这时等价于:</span><br><span class="line"></span><br><span class="line">  &gt; ffmpeg -i video.mkv -i image.png -filter_complex &#x27;overlay[out]&#x27; -map &#x27;[out]&#x27; out.mkv</span><br><span class="line"></span><br><span class="line">  此外，在滤镜是单输出时我们还可以进一步省略输出标签，它会自动添加到输出文件，所以进一步简写为:</span><br><span class="line"></span><br><span class="line">  &gt; ffmpeg -i video.mkv -i image.png -filter_complex &#x27;overlay&#x27; out.mkv</span><br><span class="line"></span><br><span class="line">  利用`lavfi`生成5秒的 红`color`(色块):</span><br><span class="line"></span><br><span class="line">  &gt; ffmpeg -filter_complex &#x27;color=c=red&#x27; -t 5 out.mkv</span><br><span class="line"></span><br><span class="line">- `-lavfi filtergraph (global)`：定义一个复合滤镜，至少有一个输入和/或输出，等效于`-filter_complex`。</span><br><span class="line"></span><br><span class="line">- `-filter_complex_script filename (global)`：这个选项类似于`-filter_complex`，唯一不同就是它的参数是文件名，会从这个文件中读取复合滤镜的定义。</span><br><span class="line"></span><br><span class="line">- `-accurate_seek (input)`：这个选项会启用/禁止输入文件的精确定位(配合`-ss`)，它默认是启用的，即可以精确定位。需要时可以使用`-noaccurate_seek`来禁用，例如在复制一些流而转码另一些的场景下。</span><br><span class="line"></span><br><span class="line">- `-seek_timestamp (input)`：这个选项配合`-ss`参数可以在输入文件上启用或者禁止利用时间戳的定位。默认是禁止的，如果启用，则认为`-ss`选项参数是正式的时间戳，而不是由文件开始计算出来的偏移。这一般用于具有不是从0开始时间戳的文件，例如一些传输流(直播下)。</span><br><span class="line"></span><br><span class="line">- `-thread_queue_size size (input)`：这个选项设置可以从文件或者设备读取的最大排队数据包数量。对于低延迟高速率的直播流，如果不能及时读取，则出现丢包，所以提高这个值可以避免出现大量丢包现象。</span><br><span class="line"></span><br><span class="line">- `-override_ffserver (global)`:对`ffserver`的输入进行指定。使用这个选项`ffmpeg`可以把任意输入映射给`ffserver`并且同时控制很多编码可能。如果没有这个选项，则`ffmpeg`仅能根据`ffserver`所要求的数据进行传输。</span><br><span class="line"></span><br><span class="line">  这个选项应用场景是`ffserver`需要一些特性，但文件/设备不提供，这时可以利用`ffmpeg`作为中间处理环节控制后输出到`ffserver`到达所需要求。</span><br><span class="line"></span><br><span class="line">- `-sdp_file file (global)`：输出`sdp`信息到文件`file`。它会在至少一个输出不是`rtp`流时同时输出`sdp`信息。</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  -discard (input)</span><br></pre></td></tr></table></figure>
<p>：允许丢弃特定的流或者分离出的流上的部分帧，但不是所有的分离器都支持这个特性。</p>
<ul>
<li><code>none</code>：不丢帧</li>
<li><code>default</code>：丢弃无效帧</li>
<li><code>noref</code>：丢弃所有非参考帧</li>
<li><code>bidir</code>：丢弃所有双向帧</li>
<li><code>nokey</code>：丢弃所有非关键帧</li>
<li><code>all</code>：丢弃所有帧</li>
</ul>
</li>
<li><p><code>-xerror (global)</code>:在出错时停止并退出</p>
</li>
</ul>
<p>作为一个特殊的例外，你可以把一个位图字幕(bitmap subtitle)流作为输入，它将转换作为同于文件最大尺寸的视频(如果没有视频则是720x576分辨率)。<strong>注意</strong>这仅仅是一个特殊的例外的临时解决方案，如果在<code>libavfilter</code>中字幕处理方案成熟后这样的处理方案将被移除。</p>
<p>例如需要为一个储存在DVB-T上的MPEG-TS格式硬编码字幕，而且字幕延迟1秒： &gt; ffmpeg -i input.ts -filter_complex \ ‘[#0x2ef] setpts=PTS+1/TB [sub] ; [#0x2d0] [sub] overlay’ \ -sn -map ‘#0x2dc’ output.mkv</p>
<p>(0x2d0, 0x2dc 以及 0x2ef 是MPEG-TS 的PIDs，分别指向视频、音频和字幕流，一般作为MPEG-TS中的0:0,0:3和0：7是实际流标签)</p>
<h4 id="预设文件"><a href="#预设文件" class="headerlink" title="预设文件"></a>预设文件</h4><p>一个预设文件是选项/值对的序列(option=value)，每行都是一个选项/值对， 用于指定一系列的选项，而这些一般很难在命令行中指定(限于命令行的一些限制，例如长度限制)。以<code>#</code>开始的行是注释，会被忽略。一般<code>ffmpeg</code>会在目录树中检查<code>presets</code>子目录以获取预设文件。</p>
<p>有两种类型的预设文件:ffpreset 和 avpreset。</p>
<h5 id="ffpreset类型预设文件"><a href="#ffpreset类型预设文件" class="headerlink" title="ffpreset类型预设文件"></a>ffpreset类型预设文件</h5><p>采用<code>ffpreset</code>类型预设文件主要包含<code>vpre</code>、<code>apre</code>、<code>spre</code>和<code>fpre</code>选项。其中<code>fpre</code>选项的参数可以代替预设的名称作为输入预设文件名，以用于任何一种编码格式。对于<code>vpre</code>、<code>apre</code>和<code>spre</code>选项参数会指定一个预设定文件用于当前编码格式以替代(作为)同类项的预订选项。</p>
<p>选用预设文件传递<code>vpre</code>、<code>apre</code>和<code>spre</code>的参数<code>arg</code>有下面一些搜索应用规则：</p>
<ul>
<li>将在目录<code>$FFMPEG_DATADIR</code>(如果设置了)和<code>$HOME/.ffmpeg</code>目录和配置文件中定义的数据目录(一般是<code>PREFIX/share/ffmpeg</code>)，以及<code>ffpresets</code>所在的执行文件目录下ffmpeg搜索对应的预定义文件<code>arg.ffpreset</code>，例如参数是<code>libvpx-1080p</code>,则对应于文件<code>libvpx-1080p.ffpreset</code></li>
<li>如果没有该文件，则进一步在前述目录下搜索<code>codec_name-arg.ffpreset</code>文件，如果找到即应用。例如选择了视频编码器<code>-vcodec libvpx</code>和<code>-vpre 1080p</code>则对应的预设文件名是<code>libvpx-1080p.ffpreset</code></li>
</ul>
<h5 id="avpreset类型预设文件"><a href="#avpreset类型预设文件" class="headerlink" title="avpreset类型预设文件"></a>avpreset类型预设文件</h5><p><code>avprest</code>类型预设文件以<code>pre</code>选项引入。他们工作方式类似于<code>ffpreset</code>类型预设文件(即也是选项值对序列)，但只对于特定编码器选项，因此一些 选项值 对于不适合的编码器是无效的。根据<code>pre</code>的参数<code>arg</code>查找预设文件基于如下规则：</p>
<ul>
<li>首先搜索<code>$AVCONV_DATADIR</code>所指目录(如果定义了)，其次搜索<code>$HOME/.avconv</code>目录，然后搜索执行文件所在目录(通常是<code>PREFIX/share/ffmpeg</code>)，在其下查找<code>arg.avpreset</code>文件。第一个匹配的文件被应用。</li>
<li>如果查找不到，如果还同步还指定了编码(如<code>-vcodec libvpx</code>)再以前面目录顺序，以<code>codec_name-arg.avpreset</code>再次查找文件。例如对于有选项<code>-vcodec libvpx</code>和<code>-pre 1080p</code>将搜索<code>libvpx-1080p.avpreset</code></li>
<li>如果还没有找到，将在当前目录下搜索<code>arg.avpreset</code>文件</li>
</ul>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><h4 id="视频和音频抓取"><a href="#视频和音频抓取" class="headerlink" title="视频和音频抓取"></a>视频和音频抓取</h4><p>如果你指定了输入格式和设备，ffmpeg可以直接抓取视频和音频：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -f oss -i /dev/dsp -f video4linux2 -i /dev/video0 /tmp/out.mpg</span><br></pre></td></tr></table></figure>
</blockquote>
<p>或者采用ALSA音频源(单声道，卡的id是1)替代OSS:</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -f alsa -ac 1 -i hw:1 -f video4linux2 -i /dev/video0 /tmp/out.mpg</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>注意</strong>对于不同的视频采集卡，你必须正确激活视频源和通道，例如Gerd Knorr的<code>xawtv</code>。你还需要设置正确的音频记录层次和混合模式。只有这样你才能采集到想要的视音频。</p>
<h4 id="X11显示的抓取"><a href="#X11显示的抓取" class="headerlink" title="X11显示的抓取"></a>X11显示的抓取</h4><p>可以通过ffmpeg直接抓取X11显示内容：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -f x11grab -video_size cif -framerate 25 -i :0.0+10，20 /tmp/out.mpg</span><br></pre></td></tr></table></figure>
<p><code>0.0</code>是X11服务的显示屏幕号(display.screen)，定义于<code>DISPLAY</code>环境变量。10是水平偏移，20是垂直偏移</p>
</blockquote>
<h4 id="视频和音频文件格式转换"><a href="#视频和音频文件格式转换" class="headerlink" title="视频和音频文件格式转换"></a>视频和音频文件格式转换</h4><p>任何支持的文件格式或者协议都可以作为ffmpeg输入。例如：</p>
<ul>
<li><p>你可以使用YUV文件作为输入</p>
<blockquote>
<p>ffmpeg -i /tmp/test%d.Y /tmp/out.mpg</p>
</blockquote>
<p>这里可能是这样一些文件</p>
<blockquote>
<p>/tmp/test0.Y, /tmp/test0.U, /tmp/test1.V, /tmp/test1.Y, /tmp/test1.U, /tmp/test1.V, etc…</p>
</blockquote>
<p>这里Y还有对应分辨率的2个关联文件U和V。这是一种raw数据文件而没有文件头，它可以被所有的视频解码器生成。你必须利用<code>-s</code>对它指定一个尺寸而不是让ffmpeg去猜测。</p>
</li>
<li><p>你可以把raw YUV420P文件作为输入：</p>
<blockquote>
<p>ffmpeg -i /tmp/test/yuv /tmp/out.avi</p>
</blockquote>
<p>test.yuv 是一个包含raw YUV通道数据的文件。每个帧先是Y数据，然后是U和V数据。</p>
</li>
<li><p>也可以输出YUV420P类型的文件</p>
<blockquote>
<p>ffmpeg -i mydivx.avi hugefile.yuv</p>
</blockquote>
</li>
<li><p>可以设置一些输入文件和输出文件</p>
<blockquote>
<p>ffmpeg -i /tmp/a.wav -s 640x480 -i /tmp/a.yuv /tmp/a.mpg</p>
</blockquote>
<p>这将转换一个音频和raw的YUV视频到一个MPEG文件中</p>
</li>
<li><p>你也可以同时对音频或者视频进行转换</p>
<blockquote>
<p>ffmpeg -i /tmp/a.wav -ar 22050 /tmp/a.mp2</p>
</blockquote>
<p>这里把a.wav转换为MPEG音频，同时转换了采样率为22050HZ</p>
</li>
<li><p>你也可以利用映射同时编码多个格式作为输入或者输出：</p>
<blockquote>
<p>ffmpeg -i /tmp/a.wav -map 0:a -b:a 64k /tmp/a.mp2 -map 0:a -b:a 128k /tmp/b.mp2</p>
</blockquote>
<p>这将同时把a.wav以64k码率输出到a.mp2，以128k码率输出到b.mp2。 “-map file:index”指定了对于每个输出是连接到那个输入流的。</p>
</li>
<li><p>还可以转换解码VOBs：</p>
<blockquote>
<p>ffmpeg -i snatch_1.vob -f avi -c:v mpeg4 -b:v 800k -g 300 -bf 2 -c:a libmp3lame -b:a 128k snatch.avi</p>
</blockquote>
<p>这是一个典型的DVD抓取例子。这里的输入是一个VOB文件，输出是MPEG-4编码视频以及MP3编码音频的AVI文件。<strong>注意</strong>在这个命令行里使用了B-frames（B帧）是兼容DivX5的，GOP设置为300则意味着有一个内帧是适合29.97fps的输入视频。此外，音频流采用MP3编码需要运行LAME支持，它需要通过在编译是设置<code>--enable-libmp3lame</code>。这种转换设置在多语言DVD抓取转换出所需的语言音频时特别有用。</p>
<p><strong>注意</strong>要了解支持那些格式，可以采用<code>ffmpeg -formats</code></p>
</li>
<li><p>可以从一个视频扩展生成图片（序列），或者从一些图片生成视频：</p>
<ul>
<li><p>导出图片</p>
<blockquote>
<p>ffmpeg -i foo.avi -r 1 -s WxH -f image2 foo-%03d.jpeg</p>
</blockquote>
<p>这将每秒依据foo.avi生成一个图片命名为foo-001.jpeg ,foo-002.jpeg以此类推,图片尺寸是WxH定义的值。</p>
<p>如果你想只生成有限数量的视频帧，你可以进一步结合<code>-vframes</code>或者<code>-t</code>或者<code>-ss</code>选项实现。</p>
</li>
<li><p>从图片生成视频</p>
<blockquote>
<p>ffmpeg -f image2 -framerate 12 -i foo-%03d.jpeg -s WxH foo.avi</p>
</blockquote>
<p>这里的语法<code>foo-%03d.jpeg</code>指明使用3位数字来补充完整文件名，不足3位以0补齐。这类似于C语言的printf函数中的格式，但只接受常规整数作为部分。</p>
<p>当导入一个图片序列时，<code>-i</code>也支持shell的通配符模式(内置的)，这需要同时选择image2的特性选项<code>-pattern_type glob</code>：例如下面就利用了所有匹配<code>foo-*.jpeg</code>的图片序列创建一个视频：</p>
<blockquote>
<p>ffmpeg -f image2 -pattern_type glob -framerate 12 -i ‘foo-*.jpeg’ -s WxH foo.avi</p>
</blockquote>
</li>
</ul>
</li>
<li><p>你可以把很多相同类型的流一起放到一个输出中：</p>
<blockquote>
<p>ffmpeg -i test1.avi -i test2.avi -map 1:1 -map 1:0 -map 0:1 -map 0:0 -c copy -y test12.nut</p>
</blockquote>
<p>这里最后输出文件test12.nut包括了4个流，其中流的顺序完全根据前面<code>-map</code>的指定顺序。</p>
</li>
<li><p>强制为固定码率编码(CBR)输出视频：</p>
<blockquote>
<p>ffmpeg -i myfile.avi -b 4000k -minrate 4000k -maxrate 4000k -bufsize 1835k out.m2v</p>
</blockquote>
</li>
<li><p>使用<code>lambda</code>工具的4个选项<code>lmin</code>，<code>lmax</code>，<code>mblmin</code>以及<code>mblmax</code>使你能更简单的从<code>q</code>转换到<code>QP2LAMBDA</code>:</p>
<blockquote>
<p>ffmpeg -i src.ext -lmax 21*QP2LAMBDA dst.ext</p>
</blockquote>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/21/RK3588%E3%80%81ros%E3%80%81fastdeploy%E8%81%94%E5%90%88%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/21/RK3588%E3%80%81ros%E3%80%81fastdeploy%E8%81%94%E5%90%88%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE/" class="post-title-link" itemprop="url">RK3588、ros、fastdeploy联合环境设置</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-03-21 21:00:27 / 修改时间：22:16:15" itemprop="dateCreated datePublished" datetime="2023-03-21T21:00:27+08:00">2023-03-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="基础环境设置"><a href="#基础环境设置" class="headerlink" title="基础环境设置"></a>基础环境设置</h2><h3 id="ROS环境设置"><a href="#ROS环境设置" class="headerlink" title="ROS环境设置"></a>ROS环境设置</h3><p>ros与ubuntu系统紧密相连，要求在固定版本的Ubuntu系统上安装对应版本的ros系统，对应版本如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Ubuntu版本</th>
<th>ros版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>ubuntu16.04</td>
<td>ros-kinetic</td>
</tr>
<tr>
<td>ubuntu18.04</td>
<td>ros-melodic</td>
</tr>
<tr>
<td>ubuntu20.04</td>
<td>ros-noetic</td>
</tr>
<tr>
<td>Ubuntu22.04</td>
<td>ros-humble（ros2）</td>
</tr>
</tbody>
</table>
</div>
<p>以在rk3588上安装ros为例，rk3588上系统环境为ubuntu20.04，即需要安装的ros版本为ros-noetic。</p>
<p>安装类比于<a target="_blank" rel="noopener" href="https://blog.csdn.net/KIK9973/article/details/118755045">Ubuntu18.04安装Ros</a>进行ubuntu20.04下的ros安装，注意将其中的ros-melodic替换为ros-noetic。</p>
<p>核心命令为</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#设置中科大源</span><br><span class="line">sudo sh -c &#x27;. /etc/lsb-release &amp;&amp; <span class="built_in">echo</span> &quot;deb http://mirrors.ustc.edu.cn/ros/ubuntu/ `lsb_release -cs` main&quot; &gt; /etc/apt/sources.list.d/ros-latest.list&#x27;</span><br><span class="line">#设置公钥</span><br><span class="line">sudo apt-key adv --keyserver &#x27;hkp://keyserver.ubuntu.com:<span class="number">80</span>&#x27; --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654</span><br><span class="line">#更新软件包列表</span><br><span class="line">sudo apt update</span><br><span class="line">#安装ros（需要替换网址中的melodic为noetic）</span><br><span class="line">sudo apt install ros-noetic-desktop-full</span><br><span class="line">#设置环境变量</span><br><span class="line"><span class="built_in">echo</span> &quot;source /opt/ros/melodic/setup.bash&quot; &gt;&gt; ~/.bashrc</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<h3 id="FFMPEG源码编译"><a href="#FFMPEG源码编译" class="headerlink" title="FFMPEG源码编译"></a>FFMPEG源码编译</h3><p>在官方github中下载FFMPEG源码，例如版本4.2.7</p>
<p>安装依赖库</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libx264-dev </span><br><span class="line">sudo apt install libdrm-dev  </span><br></pre></td></tr></table></figure>
<p>根据需求配置configure，配置makefile</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --enable-shared --enable-gpl --enable-libx264 --enable-rkmpp --enable-version3 --enable-libdrm</span><br></pre></td></tr></table></figure>
<p>make直接编译</p>
<h3 id="OPENCV源码编译"><a href="#OPENCV源码编译" class="headerlink" title="OPENCV源码编译"></a>OPENCV源码编译</h3><p>在官方github中下载OpenCV源码，例如版本4.5.5</p>
<ul>
<li>安装依赖库</li>
</ul>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libgtk-dev</span><br><span class="line">sudo apt install libgail-dev</span><br></pre></td></tr></table></figure>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build </span><br><span class="line">cmake ..</span><br></pre></td></tr></table></figure>
<h2 id="环境依赖兼容问题"><a href="#环境依赖兼容问题" class="headerlink" title="环境依赖兼容问题"></a>环境依赖兼容问题</h2><p>ros、fastdeploy、opencv版本问题</p>
<p>解决方法：</p>
<ul>
<li><p>针对fastdeploy，自定义opencv版本进行fastdeploy的编译</p>
<ul>
<li><p>需要修改的文件路径如下所示：</p>
<ul>
<li><p><strong>/FastDeploy/CmakeLists.txt</strong></p>
<ul>
<li><pre><code class="lang-txt">#修改opencv_dir
set(OPENCV_DIRECTORY &quot;/usr/local/lib/cmake/opencv4&quot; CACHR PATH &quot;User can specify the installed opencv directory.&quot;)
</code></pre>
<p>这个地方修改之后会在后续引入opencv.cmake的时候将该参数传入，并在opencv.cmake中进行cmakelist中头文件的包含，动态链接库的链接等操作</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/04/RK3588s%E9%83%A8%E7%BD%B2%E7%9B%B8%E5%85%B3-NEW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/04/RK3588s%E9%83%A8%E7%BD%B2%E7%9B%B8%E5%85%B3-NEW/" class="post-title-link" itemprop="url">RK3588s部署相关</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-03-04 15:28:29 / 修改时间：15:35:38" itemprop="dateCreated datePublished" datetime="2023-03-04T15:28:29+08:00">2023-03-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="RK3588s部署相关"><a href="#RK3588s部署相关" class="headerlink" title="RK3588s部署相关"></a>RK3588s部署相关</h1><p>目前无人机上开发板为ROC-RK3588S-PC，为在其上进行深度学习模型的推理，需要对板载的NPU进行配置使用。为使用该NPU，需要下载<a target="_blank" rel="noopener" href="https://wiki.t-firefly.com/zh_CN/ROC-RK3588S-PC/usage_npu.html">RKNN SDK</a>，RKNN SDK为RK3588s提供编程接口，帮助用户部署使用通过RKNN-Toolkit2导出的RKNN模型。</p>
<p>下列代码输出的是rk3588机载npu的使用率，从而可以验证板载npu是否被使用</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/kernel/debug/rknpu/load</span><br></pre></td></tr></table></figure>
<p>目前3588的部署方式大致有两种，一为利用RKNN官方支持的方式进行部署，另一为利用FastDeploy，其在RKNN官方的基础上加了一层进行后续部署。</p>
<h2 id="基本组件及功能介绍"><a href="#基本组件及功能介绍" class="headerlink" title="基本组件及功能介绍"></a>基本组件及功能介绍</h2><h3 id="RKNN-Toolkit2与RKNN-Toolkit-Lite2套件介绍"><a href="#RKNN-Toolkit2与RKNN-Toolkit-Lite2套件介绍" class="headerlink" title="RKNN-Toolkit2与RKNN Toolkit Lite2套件介绍"></a><strong>RKNN-Toolkit2与RKNN Toolkit Lite2套件介绍</strong></h3><h4 id="RKNN-Toolkit2"><a href="#RKNN-Toolkit2" class="headerlink" title="RKNN-Toolkit2"></a>RKNN-Toolkit2</h4><p>RKNN-Toolkit2是为用户提供在PC平台上进行Rockchip芯片NPU模型转换、推理和性能评估的开发套件。用户通过该工具提供的Python 接口可以便捷地完成以下功能：</p>
<ol>
<li>模型转换：支持Caffe、TensorFlow、TensorFlow Lite、ONNX、DarkNet、PyTorch 等模型转为RKNN 模型，并支持RKNN 模型导入导出，RKNN 模型能够在Rockchip NPU 平台上加载使用。</li>
<li>量化功能：支持将浮点模型量化为定点模型，目前支持的量化方法为非对称量化（ asymmetric_quantized-8 及asymmetric_quantized-16 ） ， 并支持混合量化功能。</li>
<li>模型推理：能够在PC 上模拟Rockchip NPU 运行RKNN 模型并获取推理结果；或将RKNN模型分发到指定的NPU 设备上进行推理并获取推理结果。</li>
<li>性能和内存评估：将RKNN 模型分发到指定NPU 设备上运行，以评估模型在实际设备上运行时的性能和内存占用情况。</li>
<li>量化精度分析：该功能将给出模型量化前后每一层推理结果与浮点模型推理结果的余弦距离，以便于分析量化误差是如何出现的，为提高量化模型的精度提供思路。</li>
</ol>
<p><strong>即，该部分需要在linux-ubuntu的电脑上进行安装，从而完成模型的转换和模型的量化等功能</strong></p>
<h4 id="RKNN-Toolkit-Lite2"><a href="#RKNN-Toolkit-Lite2" class="headerlink" title="RKNN -Toolkit-Lite2"></a>RKNN -Toolkit-Lite2</h4><p>RKNN -Toolkit-Lite2为RKNN-Toolkit-lite2的一部分，为带有瑞芯NPU平台提供Python编程接口，帮助用户部署使用RKNN-Toolkit2导出的RKNN模型。</p>
<h4 id="rknpu2"><a href="#rknpu2" class="headerlink" title="rknpu2"></a>rknpu2</h4><p>rknpu2为带有瑞芯NPU平台提供c语言编程接口，帮助用户部署使用 RKNN-Toolkit2 导出的 RKNN 模型。</p>
<p><strong>即RKNN -Toolkit-Lite2和rknpu2分别为板载上使用python或c++调用npu的接口，需要在板子上进行安装从而完成对应的npu的调用</strong></p>
<h4 id="FastDeploy"><a href="#FastDeploy" class="headerlink" title="FastDeploy"></a>FastDeploy</h4><p>FastDeploy是百度推出的一款AI算法推理部署的工具。其为在RKNN官方的库的基础上进行二次开发及封装，从而实现更方便的一种算法部署方式。但是其由于目前依旧为develop阶段，很多接口尚未完全开发完毕。</p>
<p><strong>即FastDeploy需要在ubuntu服务器端和板载端均进行安装，目前正在快速开发中，交流群内较为活跃，遇到问题好解决</strong></p>
<h2 id="使用RKNN官方例程"><a href="#使用RKNN官方例程" class="headerlink" title="使用RKNN官方例程"></a>使用RKNN官方例程</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="服务器端环境"><a href="#服务器端环境" class="headerlink" title="服务器端环境"></a>服务器端环境</h4><p>首先在<a target="_blank" rel="noopener" href="https://wiki.t-firefly.com/zh_CN/ROC-RK3588S-PC/usage_npu.html">firefly官网中的RK3588S中的NPU使用</a>中或者<a href="wget https://bj.bcebos.com/fastdeploy/third_libs/rknpu2_device_install_1.4.0.zip">百度提供的下载链接</a>下载RKNN SDK，然后由于目前RKNN ToolKit2只支持python3.6或者python3.8，所以在linux-ubuntu的电脑运行如下代码，安装对应代码</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 创建python3.<span class="number">8</span>环境</span><br><span class="line">conda create -n rknn2 python=<span class="number">3</span>.<span class="number">8</span></span><br><span class="line">conda activate rknn2</span><br><span class="line"></span><br><span class="line"># 安装 rknn-toolkit2</span><br><span class="line">pip install numpy==<span class="number">1</span>.<span class="number">16</span>.<span class="number">6</span></span><br><span class="line">sudo apt-get install libxslt1-dev zlib1g zlib1g-dev libglib2.<span class="number">0</span>-<span class="number">0</span> libsm6 libgl1-mesa-glx libprotobuf-dev gcc g++</span><br><span class="line">pip install rknn-toolkit2/packages/rknn_toolkit2-<span class="number">1</span>.<span class="number">3</span>.<span class="number">0</span>_11912b58-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line"># 安装yaml</span><br><span class="line">pip install pyyaml</span><br></pre></td></tr></table></figure>
<h4 id="板端环境"><a href="#板端环境" class="headerlink" title="板端环境"></a>板端环境</h4><p>首先将板子从原生的安卓刷系统为ubuntu系统，然后在<a target="_blank" rel="noopener" href="https://wiki.t-firefly.com/zh_CN/ROC-RK3588S-PC/usage_npu.html">firefly官网中的RK3588S中的NPU使用</a>中下载RKNN SDK，由于目前的rknn_toolkit2_lite2只支持python3.7或者python3.9，所以输入下列代码安装rknn_toolkit2_lite2</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> rknpu2_device_install</span><br><span class="line"># RK3588运行以下代码</span><br><span class="line">sudo rknn_install_rk3588.sh</span><br></pre></td></tr></table></figure>
<h3 id="模型转换步骤"><a href="#模型转换步骤" class="headerlink" title="模型转换步骤"></a>模型转换步骤</h3><ul>
<li><p>首先通过任务的不同需求训练出神经网络</p>
</li>
<li><p>通过各类转换工具将模型转换为onnx</p>
</li>
<li><p>将onnx模型通过PKNN-Toolkit2转换为RKNN格式</p>
<ul>
<li><p>利用RKNN-Toolkit2的Python API接口导出RKNN格式的模型。操作流程如下</p>
<p>1、 创建RKNN对象，初始化RKNN SDK环境。</p>
<p>2、 调用config接口设置模型预处理参数。</p>
<p>3、 调用对应加载第3方框架接口，加载TensorFlow、Pytorch、ONNX模型。</p>
<p>4、 调用build接口构建RKNN模型。</p>
<p>5、 调用export_rknn接口导出RKNN模型</p>
</li>
</ul>
<p>对应文档查看下载包内的<strong>Rockchip_User_Guide_RKNN_Toolkit2_CN</strong></p>
</li>
</ul>
<h3 id="默认转换好的模型在RK3588s上的使用（RKNPU2）"><a href="#默认转换好的模型在RK3588s上的使用（RKNPU2）" class="headerlink" title="默认转换好的模型在RK3588s上的使用（RKNPU2）"></a>默认转换好的模型在RK3588s上的使用（RKNPU2）</h3><ul>
<li>首先针对于自身平台下载gcc交叉编译器 gcc-9.3.0-x86_64_arrch64-linux-gnu</li>
<li>然后进入/rknpu2_1.3.0/examples 文件夹</li>
<li>假定想要测试转换好了的yolov5，则进入对应的rknn_yolov5_demo文件夹</li>
<li>使用./build-linux_RK3588.sh进行编译</li>
<li>进入./install/rknn_yolov5_demo_linux 文件夹使用./rknn_yolov5_demo ./model/RK3588/yolov5s-640-640.rknn ./model/bus.jpg 进行测试</li>
</ul>
<h3 id="默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）"><a href="#默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）" class="headerlink" title="默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）"></a>默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）</h3><ul>
<li>利用conda 创建py36或者py38的环境</li>
<li>进入rknn-toolkit2-1.3.0/rknn_toolkit_lite2文件夹，进入package文件夹安装对应的whl</li>
<li>进入examples/inference_with_lite文件夹，使用python test.py进行测试</li>
</ul>
<h2 id="使用百度-Fast-Deploy相关工具进行部署"><a href="#使用百度-Fast-Deploy相关工具进行部署" class="headerlink" title="使用百度 Fast Deploy相关工具进行部署"></a>使用百度 Fast Deploy相关工具进行部署</h2><p>官方视频例程大都是老版本的配置，其在现有的工程下的使用比较落后。故大致步骤可和<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/education/preview/3610910">官方视频</a>内相同，但具体节点的使用方式有所差异。</p>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>对应部署的环境配置分为板载RK3588s上的环境配置和个人PC上的环境配置，对应需要配置的环境如下所示</p>
<p><img src="/2023/03/04/RK3588s%E9%83%A8%E7%BD%B2%E7%9B%B8%E5%85%B3-NEW/image-20230221221155971.png" alt="image-20230221221155971"></p>
<p>RKNN官方例程中的服务器端环境和板端环境均需要进行配置，然后针对于FastDeploy进行环境配置。</p>
<p>FastDeploy在板端的安装（c++）</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/PaddlePaddle/FastDeploy.git</span><br><span class="line"><span class="built_in">cd</span> FastDeploy</span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line"></span><br><span class="line"># Only a few key configurations are introduced here, see README.<span class="built_in">md</span> <span class="keyword">for</span> details.</span><br><span class="line"># -DENABLE_ORT_BACKEND:     Whether to enable ONNX model, default OFF</span><br><span class="line"># -DENABLE_RKNPU2_BACKEND:  Whether to enable RKNPU model, default OFF</span><br><span class="line"># -RKNN2_TARGET_SOC:        Compile the SDK board model. Enter RK356X or RK3588 with case sensitive required.</span><br><span class="line">cmake ..  -DENABLE_ORT_BACKEND=ON \</span><br><span class="line">	      -DENABLE_RKNPU2_BACKEND=ON \</span><br><span class="line">	      -DENABLE_VISION=ON \</span><br><span class="line">	      -DRKNN2_TARGET_SOC=RK3588 \</span><br><span class="line">          -DCMAKE_INSTALL_PREFIX=$&#123;PWD&#125;/fastdeploy-<span class="number">0</span>.<span class="number">0</span>.<span class="number">3</span></span><br><span class="line">make -j8</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>FastDeploy在板端的安装（python）</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/PaddlePaddle/FastDeploy.git</span><br><span class="line"><span class="built_in">cd</span> FastDeploy</span><br><span class="line"><span class="built_in">cd</span> python</span><br><span class="line"></span><br><span class="line">export ENABLE_ORT_BACKEND=ON</span><br><span class="line">export ENABLE_RKNPU2_BACKEND=ON</span><br><span class="line">export ENABLE_VISION=ON</span><br><span class="line">export RKNN2_TARGET_SOC=RK3588</span><br><span class="line">python3 setup.py build</span><br><span class="line">python3 setup.py bdist_wheel</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> dist</span><br><span class="line"></span><br><span class="line">pip3 install fastdeploy_python-<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>-cp39-cp39-linux_aarch64.whl</span><br></pre></td></tr></table></figure>
<p>个人PC环境配置及模型转换</p>
<p>本次尝试在对应远程服务器上进行环境部署，该服务器主要负责对应模型的训练，模型的转换，目前要求为linux-64bit环境。</p>
<ul>
<li><p>假定使用的是paddleDetection训练得到想要的模型，首先使用PaddleDetection中的/tools/export_model.py对训练得出的模型进行导出，将Paddle动态图转换为静态图，对应转换模型代码为</p>
<ul>
<li>```cmd<br>python ./tools/export_model.py —config configs/picodet/picodet_s_416_visdrone.yml<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 转换后的模型会存放于</span><br><span class="line"></span><br><span class="line">- ```cmd</span><br><span class="line">  ./output_inference/picodet_s_416_visdrone</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>进一步将模型从静态的paddle模型转化到onnx模型，需要使用Paddle2ONNX库，该库的安装命令使用</p>
<ul>
<li><p>```cmd<br>pip install paddle2onnx    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 对应的模型的转换，从静态paddle模型转换为onnx格式，使用命令，对应[官网](https://github.com/PaddlePaddle/Paddle2ONNX)，（在Paddle2ONNX文件夹内使用）</span><br><span class="line"></span><br><span class="line">- ```cmd</span><br><span class="line">  #静态图转onnx模型</span><br><span class="line">  paddle2onnx --model_dir picodet_s_416_coco_lcnet --model_filename model.pdmodel --params_filename model.pdiparams --save_file  picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx --enable_dev_version True</span><br><span class="line">  #固定模型输入形状，改为静态shape</span><br><span class="line">  python -m paddle2onnx.optimize \--input_modelpicodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx \--output_modelpicodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx \--input_shape_dict&quot;&#123;&#x27;image&#x27;:[1,3,416,416]&#125;&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意，若ONNX不支持对应的算子，如自适应池化层，需要对得到的模型进行输入的固定，即改为静态shape。对应的查看方式为转换为onnx格式文件之后，使用[onnx结构查询——netron][<a target="_blank" rel="noopener" href="https://netron.app/]对onnx模型结构进行查看。">https://netron.app/]对onnx模型结构进行查看。</a></p>
</li>
</ul>
</li>
<li><p>然后将得到的onnx模型转换为对应的rknn模型，需要书写转换用到的yaml文件</p>
<ul>
<li><p>转换的yaml书写要点可见<a target="_blank" rel="noopener" href="https://www.github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/rknpu2/export.md">官网</a></p>
<ul>
<li><p>大体上如下所示</p>
</li>
<li><p>```cmd<br>mean:<br>  -</p>
<pre><code>- 128.5
- 128.5
- 128.5
</code></pre><p>std:<br>  -</p>
<pre><code>- 128.5
- 128.5
- 128.5
</code></pre><p>model_path: “./scrfd_500m_bnkps_shape640x640.onnx”<br>outputs_nodes:<br>do_quantization: True<br>dataset: “./datasets.txt”<br>output_folder: “./“</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 转换用的语句为</span><br><span class="line"></span><br><span class="line">  - ```cmd</span><br><span class="line">    python tools/rknpu2/export.py --config_path tools/rknpu2/config/RK3588/picodet_s_416_coco_lcnet.yaml</span><br></pre></td></tr></table></figure>
</li>
<li><p>对应使用的为fastdeploy内的对rknpu2的export函数，得到对应的rknn模型</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RK3588s环境配置及实机程序运行"><a href="#RK3588s环境配置及实机程序运行" class="headerlink" title="RK3588s环境配置及实机程序运行"></a>RK3588s环境配置及实机程序运行</h3><ul>
<li><p>将对应的rknn模型和cfg，对应图像拷贝到RK3588板子上，并运行<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/rknpu2/python">官网</a>提供的infer代码进行推理检测</p>
<ul>
<li>```python<br>import fastdeploy as fd<br>import cv2<br>import os</li>
</ul>
</li>
</ul>
<pre><code>def parse_arguments():
    import argparse
    import ast
    parser = argparse.ArgumentParser()
    parser.add_argument(
        &quot;--model_file&quot;,
        default=&quot;./picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet_rk3588_unquantized.rknn&quot;,
        help=&quot;Path of rknn model.&quot;)
    parser.add_argument(
        &quot;--config_file&quot;,
        default=&quot;./picodet_s_416_coco_lcnet/infer_cfg.yml&quot;,
        help=&quot;Path of config.&quot;)
    parser.add_argument(
        &quot;--image&quot;,
        type=str,
        default=&quot;./000000014439.jpg&quot;,
        help=&quot;Path of test image file.&quot;)
    return parser.parse_args()


if __name__ == &quot;__main__&quot;:
    args = parse_arguments()

    model_file = args.model_file
    params_file = &quot;&quot;
    config_file = args.config_file

    # 配置runtime，加载模型
    runtime_option = fd.RuntimeOption()
    runtime_option.use_rknpu2()

    model = fd.vision.detection.PPYOLOE(
        model_file,
        params_file,
        config_file,
        runtime_option=runtime_option,
        model_format=fd.ModelFormat.RKNN)
    model.preprocessor.disable_normalize()
    model.preprocessor.disable_permute()
    model.postprocessor.apply_decode_and_nms()

    # 预测图片分割结果
    im = cv2.imread(args.image)
    result = model.predict(im)
    print(result)

    # 可视化结果
    vis_im = fd.vision.vis_detection(im, result, score_threshold=0.5)
    cv2.imwrite(&quot;visualized_result.jpg&quot;, vis_im)
    print(&quot;Visualized result save in ./visualized_result.jpg&quot;)
```
</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/" class="post-title-link" itemprop="url">Towards Data-Efficient Detection Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 21:01:24 / 修改时间：21:34:28" itemprop="dateCreated datePublished" datetime="2022-08-22T21:01:24+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Towards-Data-Efficient-Detection-Transformers"><a href="#Towards-Data-Efficient-Detection-Transformers" class="headerlink" title="Towards Data-Efficient Detection Transformers"></a>Towards Data-Efficient Detection Transformers</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>DETR在足量样本的COCO数据集上表现出了有竞争性的效果。然而我们发现许多DETR类的方法在内容数量较少的数据集上（如Cityscapes）会有明显的性能的下降。换而言之，DETR通常需要大量的数据。为了处理这个问题。我们逐步的将数据效率高的RCNN变换为代表性的DETR，分析了影响数据效率（data efficiency）的因素。试验结果表明从局部图片进行稀疏特征采样是影响的关键。基于这个观察，本文通过简单的交替 key 和 value序列在cross attention中的构造方式，用对原始模型最少的改变的方式缓解了现存DETR方法对数据需求量巨大的问题。另外，我们介绍了一个简单但有效的数据增强的方法，从而提供更丰富的监督并提高了数据效率。实验证明，我们的方法可以被很容易的应用到不同的DETR变种上去，并在较小和较大的数据集上均可提升检测效果。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>目标检测是在计算机视觉领域里面的长盛不衰的话题。最近一种新型的目标检测算法，名叫detection transformer，因为其的简单和尚可的检测效果吸引了许多的注意力。这个类别的先驱工作是DETR，其将目标检测的任务看作是直接的集合预测问题，并利用transformer直接将目标查询转换为目标对象。其实现了相对于开创性的Faster RCNN在常用的COCO数据集上更好的效果，但其具有收敛速度显著慢于基于CNN系列检测器的缺点。因为这个原因，许多随后的工作都是致力于提高DETR的收敛速度。</p>
<ul>
<li>Deformable DETR：通过efficient attention mechanism机制</li>
<li>Swin transformer：通过conditional spatial query机制</li>
<li>（SMCA）Fast convergence of detr with spatially modulated co-attention：通过regression-aware co-attention机制</li>
</ul>
<p>这些上述的方法都可以在COCO数据集上以相似的训练代价，实现相对于Faster RCNN而言更好的检测效果，证明了DETR类方法的优越性。</p>
<p>现有的工作大都认为DETR类的方法在简单性和模型效果上均优于基于CNN的目标检测器。然而本文发现，DETR只有在充足的训练数据的情况下（例如COCO2017,有118K训练数据）才能展现出其优越的性能，然而在训练数据量不是非常充足的时候，其的效果会出现明显的下降。以自动驾驶领域常用的数据集Cityscapes（约3k训练数据）为例，大部分的DETR类的方法的AP小于Faster RCNN的AP的一半。且不同的DETR类的检测器，其性能的差距在COCO数据集上是小于3AP的，但在数量较小的Cityscapes数据集上，其会存在一个明显的差距，其性能差距约有15AP。如下图所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220811161935395.png" alt="image-20220811161935395"></p>
<p>这些发现证明了DETR类的目标检测相较于CNN类的目标检测器而言，更需要大量的数量进行训练。然而带有标签的数据的获取是需要大量的时间和人力的。</p>
<p>总而言之，为了迎合目前现存的DETR对训练数据的需求，需要大量的人力和计算资源。为了应对这个问题，本文首先从实验上，通过逐步的将数据高效的Sparse RCNN转换为DETR，分析了影响DETR中影响数据效率的关键性因素。我们的发现和分析表明：</p>
<ul>
<li>稀疏的局部特征采样是影响数据效率的关键，<ul>
<li>其缓解了学习注意到特定物体的困难</li>
<li>其避免了图像像素两倍的计算复杂度</li>
<li>令利用多尺度的特征成为可能，多尺度的特征已经被证明在目标检测任务中是关键的</li>
</ul>
</li>
</ul>
<p>基于上述的观察，我们通过简单的交替 key 和 value序列在cross attention中的构造方式，提升了现存的DETR类的目标检测算法的数据效率。具体来说，我们在前一个解码器层预测的边界框的指导下，对发送到交叉注意力层的键和值特征执行稀疏采样特征，这样对原始模型的修改最少，并且没有任何专门的模块。另外，本文通过提供给DETR丰富的监督信号来缓解对数据的需求。为达到这个目的，本文提出了一种标签加强的方式，通过在标签分配的过程中重复前景物体的label去高效并简单的执行。这个方法可以被应用在不同的DETR类的方法从而提升其的数据效率。有趣的是，其依旧带来了在训练数据充足的COCO数据集上的性能提升。</p>
<p>本文的贡献如下总结所示：</p>
<ul>
<li>本文确定了DETR的数据效率的问题。虽然DETR实现了在COCO数据集上的优秀效果，其一般会在小规模的数据集上遭受到明显的性能下降。</li>
<li>本文通过从 Sparse RCNN 到 DETR 的逐步模型转换，通过实验分析了影响检测转换器数据效率的关键因素，并发现局部区域的稀疏特征采样是数据效率的关键。</li>
<li>本文通过简单的交替在cross-attention模块中key和value序列的构造方式，明显的提升了现存的DETR方法的数据效率</li>
<li>本文提出了一种简单但有效的标签增强策略，从而提供更丰富的监督信号并提升了数据效率。其可以与不同的方法融合，从而实现在不同数据集上的性能增益。</li>
</ul>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h4><p>目标检测在许多现实生活中是非常必要的，例如自动驾驶，缺陷检测和遥感。最具有代表性的目标检测的工作可以被粗略的分为两类，两阶段的Faster RCNN和一阶段的YOLO和RetinaNet。虽然上述方法有效，但上述方法一般而言是需要以来与许多人工设计（启发式算法）的先验，例如anchor generation和rule based 标签分配方式。</p>
<p>最近DETR提供了一种简单并且干净的目标检测的计算流程。其将目标检测看作是集合预测的任务，并应用transformer将稀疏的目标候选转换为目标物体。DETR的成功引爆了最近井喷的DETR类的方法，并且许多最近的工作都致力于缓解DETR的收敛速度慢的问题。</p>
<ul>
<li>DeformDETR 提出了可学习的稀疏特征采样的可变形注意力机制并聚合多尺度特征以加速模型收敛并提高模型性能。</li>
<li>CondDETR 提出从解码器嵌入中学习条件空间查询，这有助于模型快速学习定位四个末端以进行检测</li>
</ul>
<p>这些工作实现了在COCO 2017数据集上用相似的训练代价得到Faster RCNN更好的性能。这似乎表明DETR类的方法已经在简单性和性能上压制了Faster RCNN。但本文发现DETR通常需要更多的数据，并在小规模的数据集上表现比Faster RCNN要差。</p>
<h4 id="目标检测中的标签分配"><a href="#目标检测中的标签分配" class="headerlink" title="目标检测中的标签分配"></a>目标检测中的标签分配</h4><p>在目标检测中，标签分配是一个十分重要的组件。其将一个物体的ground truth与从模型中的一个预测相匹配，从而为训练提供监督信号。在DETR之前，许多的目标检测器采用的是一对多的匹配策略，其将每个ground trurh基于局部空间关系分类给多个预测框。而DETR相反，其是采用的一对一的匹配策略，将ground truth与预测框之间通过最小化全局匹配损失来进行匹配。这个标签分配方式被许多的后续的DETR方法所采用。尽管这样的分配方式具有避免了重复移除的过程的优点，但只有少量的候选目标在每次迭代的过程中被目标标签所监督。这样就会导致模型必须从大量的数据中获得足够的监督信号或需要更多论次的训练。为了解决这个问题，本文提出了一种标签增强的方式去提供更丰富的监督信号。</p>
<h4 id="视觉transformer（ViT）中的数据效率"><a href="#视觉transformer（ViT）中的数据效率" class="headerlink" title="视觉transformer（ViT）中的数据效率"></a>视觉transformer（ViT）中的数据效率</h4><p>视觉transformer正在成为特征提取器和视觉识别的CNN的替代品。尽管其具有优秀的性能表现，但其一般而言需要比CNN需要更多的数据，并依赖于大量的数据和更多轮次的训练。</p>
<ul>
<li>DeiT 通过从预训练的CNN上进行知识蒸馏，配合上更好的训练配方，从而提高了数据效率</li>
<li>Liu等人提出了一个密集的相对定位损失去提高ViT类算法的数据效率（Efficient training of visual transformers with small datasets）</li>
</ul>
<p>与之前专注于transformer主干在图像分类任务上的数据效率问题不同，本文在目标检测任务上处理DETR数据效率的问题</p>
<h3 id="RCNN类算法与DETR类算法的不同之处分析"><a href="#RCNN类算法与DETR类算法的不同之处分析" class="headerlink" title="RCNN类算法与DETR类算法的不同之处分析"></a>RCNN类算法与DETR类算法的不同之处分析</h3><p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812084137035.png" alt="image-20220812084137035"></p>
<p>上图为从SRCN（Sparse RCNN）逐渐转化为DETR的过程中，在Ciytscapes数据集上，分别在训练50 epoch和300 epoch的情况下的AP情况。</p>
<p>对上图进行分析可得，DETR一般而言相比与RCNN需要更多的数据。为了寻找影响数据效率的关键性因素，本文将数据效率高的RCNN逐步的转变为数据效率较低的DETR，从而消融不同设计的影响。相同的实验方法在ATSS和Visformer中被使用，但实验目的不同。</p>
<h4 id="检测器的选择"><a href="#检测器的选择" class="headerlink" title="检测器的选择"></a>检测器的选择</h4><p>为了从模型的转换中获得有效的结果，需要选择适当的检测器去参与实验。为了达到这个目的，本文选择Sparse RCNN和DETR作为实验模型，原因如下所示：</p>
<ul>
<li>两个模型都是在各自的领域里（RCNN类和DETR类）具有代表性的模型。所以由这两者的转换得出的结论可以推广到其他的探测器中去。</li>
<li>这两个模型在数据效率方面有巨大的差异</li>
<li>其在标签分配（label assignment）、损失函数设计（loss design），优化器选择（optimization）上具有许多的相似之处。这些相似之处可以在我们专注于核心部件的不同的时候消除没有那么重要的部件的影响。</li>
</ul>
<h4 id="Sparse-RCNN到DETR的转换"><a href="#Sparse-RCNN到DETR的转换" class="headerlink" title="Sparse RCNN到DETR的转换"></a>Sparse RCNN到DETR的转换</h4><ul>
<li>交替训练方式<ul>
<li>虽然Sparse RCNN和DETR有许多的相似之处，但其在训练策略（训练方式）上依旧有所不同。如分类损失、object query的数量，学习率和梯度剪切。本文首先通过将Sparse RCNN的训练策略用DETR的训练策略替代，我们发现Sparse RCNN用DETR的训练策略进行训练时，其在50 epoch时表现稍好，但在300epoch时表现较差。消除训练策略的差异可以帮助我们关注与影响数据效率的更核心的因素。</li>
</ul>
</li>
<li>移除FPN：<ul>
<li>多尺度特征融合已经被证明对目标检测是有效的。当CNN类的FPN neck可以实现在较小的计算代价的情况下完成多尺度特征融合，注意力机制有输入图像尺寸的平方的计算复杂度，使在DETR中对多尺度特征融合代价昂贵。因此DETR只采用了原图像经过32倍下采样的单尺度特征进行预测。在这个阶段，我们移除了FPN neck部分，并只将经过32倍下采样的特征传入检测头。模型在50epoch的情况下性能明显的下降了7.3AP</li>
</ul>
</li>
<li>引入transformer encoder：<ul>
<li>在DETR中，transformer encoder可以被认为是检测器的neck部分，其被用来处理被backbone提取出的特征。在移除了FPN neck之后，我们加入transformer encoder作为网络的neck。与在DETR中相似，backbone提取出的特征投影和位置编码同样被引入。试验结果表明AP在50epoch的时候有所下降，在300eopch的时候有所上升。我们推测其与ViT中相似，注意力机制因为其平方项的复杂度和缺少先验知识，其需要更长的训练epoch去收敛和发展其的优势。</li>
</ul>
</li>
<li>使用cross-attention替代dynamic convolutions<ul>
<li>在Sparse RCNN中的dynamic convolutions（动态卷积）和DETR中的cross-attention（互注意力）的作用相似。它们都基于图像特征的相似性自适应地将上下文聚合到候选对象。在这个步骤中，我们将dynamic convolutions替换为带有可学习的query positional embedding，其结果反直觉的表示：大量可学习的参数不一定会让模型需要更多的数据。事实上，动态卷积的70M的参数可以展现出相较于cross-attention而言更好的数据效率。</li>
</ul>
</li>
<li>对齐解码器中的dropout设置<ul>
<li>在Sparse RCNN和DETR中的decoder是非常相似的。在将dynamic convolution 用cross-attention替代之后，其可以被认为是transformer decoder。在其之间有一个轻微的不同是dropout layer在self-attention和FFN之间的使用。我们消除了这个影响。</li>
</ul>
</li>
<li>移除级联边缘框细化<ul>
<li>Sparse RCNN遵循了Cascade RCNN中的级联边缘框回归，其中每个decoder层都迭代的细化前一层做的边缘框预测。本文移除了这个步骤，模型性能有所下降。虽然级联边缘框细化没有被大多数的DETR类的检测器所使用，但其可以自然的被级联解码器所包含。</li>
</ul>
</li>
<li>移除ROIAlign<ul>
<li>Sparse RCNN和其余RCNN类的检测其相同，从感兴趣的局部区域采样特征，然后根据采样的稀疏特征进行预测。而每个DETR中的内容查询直接从全局的特征图中聚合特定于对象的信息。在这个步骤中，我们移除了Sparse RCNN中的ROIAlign，box target transformation也被移除。我们可以发现，模型的性能出现了明显的下降，在50epoch的情况下出现了8.4Ap的下降。我们推测从整个特征图上学习到局部对象区域的代价较大，所以模型需要更多的数据和训练epoch去获取局部属性。</li>
</ul>
</li>
<li>移除初始的proposals<ul>
<li>最终，DETR直接预测了目标的bounding box，RCNN类预测使用了一些初始化的先验。在这个步骤中，我们通过移除初始的proposals消除了影响。预料之外的是，这个小改变使模型性能出现了明显的下降。我们人文初始的proposals作为空间上的先验，帮助模型聚焦于局部空间信息，从而减少了从大量训练数据中学习局部性的需要</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>如上所示，从Sparse RCNN转换为DETR的结果和分析如下所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812084137035.png" alt="image-20220812084137035"></p>
<p>其在更改之后对AP影响大于5AP的本文认为是影响数据效率的关键因素，如下所示：</p>
<ol>
<li>局部稀疏特征采样</li>
<li>依赖稀疏特征采样的多尺度特征拥有可接受的计算复杂度</li>
<li>依赖于空间先验的预测</li>
</ol>
<p>其中，1和3有助于模型关注局部对象区域，减轻从大量数据中学习局部性的需求，而2有助于更全面地利用和增强图像特征，但它也依赖于稀疏特征。</p>
<p>DeformDETR是在DETR中特殊的一种，其表现出了与Sparse RCNN相比而言有可比性的数据效率。我们从Sparse RCNN到DETR的变换过程中可以对DeformDETR的数据效率进行解释：multi-scale deformable attention从图像的局部区域采样稀疏特征并利用多尺度特征。 模型的预测是相对于初始参考点的。 因此，DeformDETR 尽管没有专门设计在小型数据集上实现数据高效，但其满足了所有三个关键因素。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>本节主要利用现有的DETR类方法，在对原始的设计做最小的改变的前提下提升数据效率。</p>
<ul>
<li>本文重新对现有的DETR类算法进行了审视思考</li>
<li>基于前文的实验和分析，对现有的数据需求量巨大的DETR类模型做最少的改变并显著的提升他们的数据效率。</li>
<li>提供一种简单但有效的标签增强方法，从而为DETR提供更丰富的监督信号提升数据效率。</li>
</ul>
<h4 id="对DETR的重新审视"><a href="#对DETR的重新审视" class="headerlink" title="对DETR的重新审视"></a>对DETR的重新审视</h4><h5 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h5><p>DETR通常来说，由backbone，transformer encoder，transformer decoder，prediction head构成。</p>
<ul>
<li><p>backbone：backbone首先从输入图片中提取多尺度的特征，被称作$\{f^l\}^L_{l=1}$，其中$f^l \in R^{H ×W ×C}$。最后一个特征曾有着最小的分辨率，将其展平并嵌入以获得$z^L \in R^{S^L \times D}$，其中$S^L =H^L \times W^L$是序列长度，$D$是特征维度。相应的，位置编码嵌入被表示为$p^L \in R^{S^L\times D}$。</p>
</li>
<li><p>transformer encoder：之后单尺度序列特征被transformer编码，并获得$Z^L_e \in R^{S^L \times D}$。</p>
</li>
<li><p>transformer decoder：decoder包含了$L_d$层的decoder layers。 查询内容的嵌入表示被初始化为$q_0\in R^{N\times D}$，其中$N$是查询的数量。每个decoder层 $DecoderLayer_l$采用上一个decoder的输出$q_{l-1}$，查询位置编码$p_l$，图像序列特征$z_l$和其位置嵌入$p_l$作为输入，输出为解码序列特征。即</p>
<p>$q _l= DecoderLayer_l (q_{l−1} , p_q , z_l, p_l),= 1 . . . L_d $</p>
<p>在大多数DETR类检测器中，例如DETR和CondDETR，单尺度的图像特征被解码器所利用，因此$z_l=z^L_e$、$p_l=p^L$，其中$l=1…L_d$</p>
<ul>
<li>prediction head ：DETR的head是使用的单纯的FFN前馈网络加上softmax进行的判断</li>
</ul>
</li>
</ul>
<h5 id="标签分配"><a href="#标签分配" class="headerlink" title="标签分配"></a>标签分配</h5><p>DETR将目标检测任务视作集合预测的问题，并对每个解码器层的预测执行深度监督。在这个过程中，标签集可以被表示如下：$y=\{y_1,…,y_M,\emptyset,…,\emptyset\}$，其中$M$为前景物体的在图像中的数量，$\emptyset$(no object)被填充到标签集合里，使标签集合的大小为$N$。相应的，每个decoder的输出可以被写作$\hat y = \{\hat y\}_{i=1}$。在标签分配的过程中，DETR搜寻一个最优的$τ \in T_N$，使得下述的匹配损失最小：</p>
<script type="math/tex; mode=display">
\hat τ= argmin_{$τ \in T_N}\sum^N_iL_{ match} (y_i , \hat y_{τ (i)})</script><p>其中$L_{ match} (y_i , \hat y_{τ (i)})$为在ground truth和index为$τ (i)$的预测之间的配对损失。</p>
<h4 id="模型的提升"><a href="#模型的提升" class="headerlink" title="模型的提升"></a>模型的提升</h4><h5 id="系数特征采样"><a href="#系数特征采样" class="headerlink" title="系数特征采样"></a>系数特征采样</h5><p>根据上述RCNN类算法与DETR类算法的不同之处分析，我们分析可得局部特征采样对数据效率是非常关键的。幸运的是，在DETR中，物体位置是在每个decoder layer之后预测得出的，因此，我们可以在上一个decoder预测的bounding box的指导下不需要引入新的参数的采样局部特征。如下图所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/vz2GLTd9Ylg5q.png" alt="img"></p>
<p>虽然有更复杂的局部特征采样方法可以使用，本文只采用了最常用的RoIAlign。采样操作可以被写成如下形式：</p>
<p>$z_L = RoIAlign(z^L_e , b_{l-1}),\ \ \ l=2…L_d$</p>
<p>其中$b_{l-1}$是上一层预测得出的边缘框，$z_l^L\in R^{N\times K^2\times D}$是被采样的特征，$K$是在RoIAlign采样的特征分辨率。注意reshape操作和flatten操作在上式中被省略。类似的，可以得到对应的position embedding  $p^L_l$。</p>
<p>在DETR中的级联结构使使用逐层边界框细化来提升检测性能很自然。本文在RCNN类算法与DETR类算法的不同之处分析处也验证了迭代细化和对初始空间参考进行预测的有效性。因此，本文如CondDETR一样引进了边缘框细化和在实施过程中的初始参考点。</p>
<h5 id="结合多尺度特征"><a href="#结合多尺度特征" class="headerlink" title="结合多尺度特征"></a>结合多尺度特征</h5><p>我们的系数特征采样使DETR以较小的计算花销使用多尺度特征变得可能。为了达到这个目的，本文使用backbone从被展平和嵌入之后的高分辨率特征提取特征以得到$\{z^l\}^{L-1}_{l=1} \in \R^{S^l \times D}$，从而进行局部特征采样。然而这些特征不被transformer encoder处理。虽然可以使用更复杂的技术，这些单尺度的被RoIAlign所采样的特征被简单的拼接，从而形成我们的多尺度的特征。这些特征可以被自然的利用cross-attention机制在decoder中被融合。</p>
<p>$z^{ms}_l=[z^1_l],[z^2_l],…,[z^L_l],l=2…L_d$</p>
<p>其中$z^{ms}_l \in \R^{N \times LK^2 \times D}$为多尺度特征，$z^l_L=RoIAlign(z^l,b_{l-1}),l-1…L-1$是。对应的位置嵌入$p^{ms}_l$用相似的方式得到。解码过程和原始的DETR是相同的。唯一的区别在于$z_l=z^{ms}_l$以及$p_l=p^{ms}_l$。</p>
<h4 id="标签增强"><a href="#标签增强" class="headerlink" title="标签增强"></a>标签增强</h4><p>DETR展现出了标签分配的一对一的分配方式。尽管拥有避免重复删除过程的优点，但只有少数检测候选者在每次迭代中都被提供了一个积极的监督信号。这样会导致模型需要更大数量的数据或者更多论次的训练，从而获得足够的监督。</p>
<p>为了缓解这个问题，本文提出了一种标签增强的策略为DETR提供更丰富的监督信号，即通过在二部图匹配的过程中重复positive labels。如下图所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/kITnuhEstBzQy.png" alt="img"></p>
<p>我们为每个前景样本$y_i$重复labels $R_i$次，并使label set $N$的总长度不变。</p>
<p>$y=\{ y^1_1,y^2_1,…,y^{R_1}_1,…y^1_M,y^2_M,…,y^{R_M}_M,…,\emptyset,…,\emptyset    \}$</p>
<p>label assignment的其余公式与DETR中相同。</p>
<p>在实际操作的过程中，考虑以下两种重复策略：</p>
<ul>
<li>固定重复次数：所有positive的label都被重复相同的次数</li>
<li>固定positive采样比例：positive的labels被重复采样，从而确保有$r$个positive样本在label set中。</li>
</ul>
<p>特别的$F=N\times r$是重复标签后的预期正样本数。 我们首先将每个正标签重复 $F//M$次，然后随机抽取 $F \% M $个正标签而不重复。 默认情况下，我们使用固定重复次数策略，因为它更容易实现并且生成的标签集是确定性的。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>本文重点关注DETR的数据效率。因此，我们的大多数实验都是在 Cityscapes 和下采样 COCO 2017在内的小型数据集上进行的。具体来说，Cityscapes 数据集包含2,975 张用于训练的图像和500 张用于评估的图像。对于下采样的 COCO 2017 数据集，训练图像随机下采样0.1、0.05、0.02 和0.01，而评估集保持不变。此外，我们还验证了我们的方法在具有118K 训练图像的全尺寸 COCO 2017 数据集上的有效性。</p>
<h4 id="实施细节"><a href="#实施细节" class="headerlink" title="实施细节"></a>实施细节</h4><p>默认情况下，我们的特征采样实现为 RoIAlign，特征分辨率为4。包括三个不同的特征级别用于多尺度特征融合。我们的标签增强采用固定重复次数，并且使用阈值为0.7 的非极大值抑制(NMS)来去除重复。所有模型都训练了50 个 epoch，并且除非另有说明，否则学习率会在40 个 epoch 后衰减。在 ImageNet-1K 上预训练的 ResNet-50用作主干。为了保证足够的训练迭代次数，所有关于 Cityscapes 和下采样 COCO2017 数据集的实验都以8 的batch size进行训练。结果是使用不同的随机种子重复运行五次的平均值。我们的数据高效检测转换器仅对现有方法进行轻微修改。除非另有说明，否则我们遵循相应基线方法的原始实现细节。运行时间在 NVIDIA A100 GPU 上进行评估。</p>
<h4 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h4><h5 id="基于Cityscapes"><a href="#基于Cityscapes" class="headerlink" title="基于Cityscapes"></a>基于Cityscapes</h5><p>在本节中，我们将我们的方法与现有的DETR进行比较。 如下表所示，大多数检测变压器都存在数据效率问题。 尽管如此，通过对 CondDETR 模型进行微小更改，我们的 DE-CondDETR 能够实现与 DeformDETR 相当的数据效率。 此外，通过标签增强提供的更丰富的监督，我们的 DELA-CondDETR 超过了 DeformDETR 2.2 AP。 此外，我们的方法可以与其他检测转换器相结合，以显着提高它们的数据效率，例如，我们训练了 50 个 epoch 的 DE-DETR 和 DELA-DETR 的性能明显优于训练了 500 个 epoch 的 DETR。另外，我们的方法依旧提高了DeformDETR的数据效率。见下</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812145944038.png" alt="image-20220812145944038"></p>
<p>上表为DETR在Cityscapes上的比较，DE前缀表明使用了本文的data-efficient，LA表明使用了label增强。</p>
<hr>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812151829137.png" alt="image-20220812151829137"></p>
<p>上表为DeformDETR使用了LA之后的效果对比。</p>
<hr>
<h5 id="基于下采样的COCO2017数据集"><a href="#基于下采样的COCO2017数据集" class="headerlink" title="基于下采样的COCO2017数据集"></a>基于下采样的COCO2017数据集</h5><p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812151508333.png" alt="image-20220812151508333"></p>
<p>下采样的 COCO 2017 数据集分别包含 11,828 (10%)、5,914 (5%)、2,365 (2%) 和 1,182 (1%) 训练图像。 如上图 所示，我们的方法在很大程度上始终优于基线方法。 特别是，仅用 ∼1K 图像训练的 DELA-DETR 显着优于 DETR 基线，训练数据是训练数据的五倍。 同样，DELA-CondDETR 始终优于使用两倍数据量训练的 CondDETR 基线。</p>
<hr>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>在本节中，我们进行消融实验以更好地理解我们方法的每个组成部分。 所有消融研究都是在 DELACondDETR 和 Cityscapes 数据集上实施的，而更多基于 DELADETR 的消融研究可以在我们的附录中找到。</p>
<hr>
<h5 id="每个模块的有效性"><a href="#每个模块的有效性" class="headerlink" title="每个模块的有效性"></a>每个模块的有效性</h5><p>我们首先消融了我们方法中每个模块的作用，如下表所示。使用局部特征采样和多尺度特征融合将模型的性能分别显着提高了 8.3 和 6.4 AP。 此外，标签增强进一步将性能提高了 2.7 AP。 此外，单独使用标签增强也带来了 2.6 AP 的性能增益。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812152636147.png" alt="image-20220812152636147"></p>
<hr>
<h5 id="RoIAlign-的特征分辨率"><a href="#RoIAlign-的特征分辨率" class="headerlink" title="RoIAlign 的特征分辨率"></a>RoIAlign 的特征分辨率</h5><p>通常，RoIAlign 中较大的样本分辨率可提供更丰富的信息，从而提高检测性能。 然而，采样更大的特征分辨率也更耗时，并且增加了解码过程的计算成本。 如下表所示，当分辨率从 1 增加到 4 时，模型性能显着提高了 5.6 AP。但是，当分辨率进一步增加到 7 时，改进很小，并且增加了 FLOPs 和延迟。 为此，我们将 RoIAlign 的特征分辨率默认设置为 4。</p>
<h5 id="多尺度特征的数量"><a href="#多尺度特征的数量" class="headerlink" title="多尺度特征的数量"></a>多尺度特征的数量</h5><p>为了结合多尺度特征，我们还从主干中采样了 8 倍和 16 倍的下采样特征来构建3个不同级别的多尺度特征。 从上表可以看出，它显着提高了模型性能 6.4 AP。 然而，当我们进一步为多尺度融合添加 64 倍下采样特征时，性能下降了 0.5 AP。 默认情况下，我们使用 3 个特征级别进行多尺度特征融合。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812152843982.png" alt="image-20220812152843982"></p>
<hr>
<h5 id="标签增强的策略"><a href="#标签增强的策略" class="headerlink" title="标签增强的策略"></a>标签增强的策略</h5><p>在本节中，我们消融了提出的两种标签增强策略，即固定重复时间和固定正样本比率。 如下左表 所示，使用不同的固定重复次数可以持续提高 DE-DETR 基线的性能，但性能增益会随着重复次数的增加而降低。 因此，默认采用固定重复时间 2。 此外，如下右表 所示，虽然使用不同的比率可以提高 AP，但在正负样本比率为 1:3 时性能最佳，有趣的是，这也是Faster RCNN中最常用的正负采样比率。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812154348539.png" alt="image-20220812154348539"></p>
<h4 id="泛化到特征丰富的数据集"><a href="#泛化到特征丰富的数据集" class="headerlink" title="泛化到特征丰富的数据集"></a>泛化到特征丰富的数据集</h4><p>虽然上述实验表明，我们的方法可以在只有有限的训练数据可用时提高模型性能，但不能保证我们的方法在训练数据充足的情况下仍然有效。 为此，我们用足够多的数据在 COCO 2017 上评估了我们的方法。 从下表 中可以看出，我们的方法不会降低 COCO 2017 上的模型性能。相反，它提供了改进效果。 具体来说，DELA-DETR 和 DELA-CondDETR 分别将其相应的基线提高了 8.3 和 2.8 AP。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812153538482.png" alt="image-20220812153538482"></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>在本文中，我们确定了DETR的数据效率问题。 通过从 Sparse RCNN 到 DETR 的逐步模型转换，我们发现局部区域的稀疏特征采样是数据效率的关键。基于这些，我们通过在预测的bounding box的指导下通过简单地采样多尺度特征在对原始模型的修改最少的前提下来改进现有的检测转换器。 此外，我们提出了一种简单而有效的标签增强策略，以提供更丰富的监督，从而进一步缓解数据效率问题。 大量实验验证了我们方法的有效性。 随着Transformer在视觉任务中越来越流行，我们希望我们的工作能够激发大家探索Transformer在不同任务中的数据效率。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/Deformable%20DETR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/Deformable%20DETR/" class="post-title-link" itemprop="url">Deformable DETR</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 20:56:45 / 修改时间：21:31:44" itemprop="dateCreated datePublished" datetime="2022-08-22T20:56:45+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Deformable-DETR-：-deformable-transformers-for-End-to-End-object-detection"><a href="#Deformable-DETR-：-deformable-transformers-for-End-to-End-object-detection" class="headerlink" title="Deformable DETR ： deformable transformers for End-to-End object detection"></a>Deformable DETR ： deformable transformers for End-to-End object detection</h2><h3 id="DETR的问题"><a href="#DETR的问题" class="headerlink" title="DETR的问题"></a>DETR的问题</h3><p>是针对DETR进行改进的一篇文章，其指出DETR主要存在以下两点问题：</p>
<ol>
<li>DETR需要相较于现有的目标检测器更长的训练epoch来收敛。</li>
<li>DETR在检测小物体时准确率较低。</li>
</ol>
<p>这是由于transformer结构所引入的问题，即是transformer组件处理特征图方面的不足：transformer结构其在初始化时分配给所有特征像素的注意力权重几乎是均等的，这就造成了模型需要长时间去学习关注真正有意义的位置。其次Transformer在计算注意力权重时，伴随着高计算量与空间复杂度。特别是在编码器部分，与特征像素点的数量成平方级关系，因此难以处理高分辨率的特征。</p>
<p>deformable DETR结合deformable conv的空间稀疏采样的优势和transformer元素间建模的能力。通过添加稀疏的空间位置，避免了上述的问题，因此DETR不采用全局的注意力计算，而是只计算reference point周围一小部分点的注意力。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Deformable DETR是一种End-to-End的目标检测器，其在DETR和transformer的基础上做了改进，能够更快收敛，同时减少计算量提高精度。其核心部件为Multi-scale Deformable Attention Module（多尺度可变形注意力模块），其为一种处理图像特征图的有效的注意力机制。</p>
<h3 id="Deformable-Attention-Module"><a href="#Deformable-Attention-Module" class="headerlink" title="Deformable Attention Module"></a>Deformable Attention Module</h3><p>针对于DETR存在的问题，提出Deformable Attention Module ，其不用遍历所有的空间位置，而是与可变形卷积相似，其只注意参考点周围的一小部分关键采样点，而不是特征图的整体。其通过为每个query分配少量固定的键，可以缓解难以收敛和特征空间分辨率所带来的问题。</p>
<p>示意图如下所示：</p>
<p><img src="/2022/08/22/Deformable%20DETR/image-20220810091027439.png" alt="image-20220810091027439"></p>
<p>下面给出MultiheadAttention和DeformableAttention的计算公式</p>
<script type="math/tex; mode=display">
 MultiHeadAttn(z_q , x) =\sum ^M_{m=1}W_m[\sum_{k∈Ω_k}A_{mqk}\cdot W_m^`x_k]
\\
 DeformAttn(z_q , p_q , x) =\sum ^M_{m=1}W_m[\sum^K_{k=1}A_{mqk}\cdot W_m^`x(p_q+∆p_{mqk})]
\\
 MSDeformAttn(z_q , p_q , \{x^l\}^L_{l=1}) =\sum ^M_{m=1}W_m[\sum^L_{l=1}\sum^K_{k=1}A_{mlqk}\cdot W_m^`x^l( \phi_l ( p̂ _q )+∆p_{mlqk})]</script><p>对DeformAttn，输入特征图尺寸为$C<em>H</em>W$，$z_q$为带有内容特征的第q个查询元素，为$p_q$为一个二维参考点，公式中参数如下解释：</p>
<ul>
<li>m 表示注意力头（head）。</li>
<li>k 表示 sampled key（被采样的键）。K 表示 total sampled key number( $K \lt\lt HW$ )。</li>
<li>$ \Delta p_{mqk} $表示第 m 个注意力头中第 k 个采样点的采样偏移量。</li>
<li>$ A_{mqk} $表示第 m 个注意力头中第 k 个采样点的注意力权重 V。</li>
<li><img src="/2022/08/22/Deformable%20DETR/4b4befc3e13742eea81d3b220c04133e.png" alt="img">，Xk表示第k个采样点，Um和Vm是可学习的参数。</li>
<li>标量注意力权重 $ A_{mqk} $的取值范围为[ 0 , 1]，通过$ \sum^K_{k=1}A_{mqk}=1 $进行归一化。</li>
<li><img src="/2022/08/22/Deformable%20DETR/fc909ee8a2414a778ba1f82d7120c36a.png" alt="img">是范围不受限制的2-d实数。</li>
<li>由于$ p_q+\Delta p_{mqk} $是分数阶的，所以在计算$x(p_q+\Delta p_{mqk})$时，采用了《Deformableconvolutional networks (ICCV)》中的双线性插值。</li>
<li>$\Delta p_{mqk}$和$A_{mqk}$都是通过在查询特征$z_q$上的线性投影获得的。</li>
<li>在实现中，查询特征$z_q$被送入3MK通道的线性投影算子，其中前2MK通道编码采样偏移量为 ，其余MK通道被送入softmax算子以获得注意力权重 。</li>
</ul>
<p>可变形注意力模块是为了将卷积特征图作为 key 要素进行处理而设计的。令 $N_q$ 为 query 元素的个数，当 MK 比较小时，可变形注意力模块的复杂度为$O(2N_q C^2 + min(HW C^2 , N_q KC^2 ))$。当它应用于DETR编码器时，其中 $N_q = HW$，复杂度变为$O(HWC^2)$，与空间大小成线性复杂度。当它被用作DETR解码器中的交叉注意力模块时，其中 $N_q = N$ ( N为对象查询次数)，复杂度变为$O (NKC^2)$，这与空间大小HW无关。</p>
<h3 id="Multi-scale-Deformable-Attention-Module"><a href="#Multi-scale-Deformable-Attention-Module" class="headerlink" title="Multi-scale Deformable Attention Module"></a>Multi-scale Deformable Attention Module</h3><p>仿照其余目标检测框架中的多尺度特征，提出Multi-scale Deformable Attention Module，将可变形注意力模块扩展为多尺度，其公式依旧如下所示：</p>
<script type="math/tex; mode=display">
MultiHeadAttn(z_q , x) =\sum ^M_{m=1}W_m[\sum_{k∈Ω_k}A_{mqk}\cdot W_m^`x_k]
\\
DeformAttn(z_q , p_q , x) =\sum ^M_{m=1}W_m[\sum^K_{k=1}A_{mqk}\cdot W_m^`x(p_q+∆p_{mqk})]
\\
MSDeformAttn(z_q , p̂ _q , \{x^l\}^L_{l=1}) =\sum ^M_{m=1}W_m[\sum^L_{l=1}\sum^K_{k=1}A_{mlqk}\cdot W_m^`x^l( \phi_l ( p̂ _q )+∆p_{mlqk})]</script><p>对MSDeformAttn，$\{x^l\}^L_{l=1}$为输入的多尺度特征图，其每层的输入特征图尺寸为$C<em>H_l</em>W_l$，$z_q$为带有内容特征的第q个查询元素，$p̂ _q ∈ [0, 1]^2$二维参考点，公式中参数如下解释：</p>
<ul>
<li>m 表示注意力头（head）。</li>
<li>k 表示 sampled key（被采样的键）。K 表示 total sampled key number( K &lt;&lt; HW )。</li>
<li>$\Delta p_{mlqk}$表示第 L 个特征层和第 m 个注意力头中第 k 个采样点的采样偏移量。</li>
<li>$A_{mlqk}$表示第 L 个特征层和第 m 个注意力头中第 k 个采样点的注意力权重 V。</li>
<li><img src="/2022/08/22/Deformable%20DETR/4b4befc3e13742eea81d3b220c04133e.png" alt="img">，Xk表示第k个采样点，Um和Vm是可学习的参数。</li>
<li>标量注意力权重 $A_{mlqk}$的取值范围为[ 0 , 1]，通过$\sum^L_{l=1}\sum^K_{k=1}A_{mlqk}=1$进行归一化。</li>
<li>$p̂ _q ∈ [0, 1]^2$是归一化坐标，我们用其清晰的表示尺度公式，其中$(0,0)$表示左上角的点,$(1,1)$表示右下角的点</li>
<li>$\phi_l ( p̂ _q )$将归一化的坐标$p̂ _q $重新缩放至输入特征图的第$l$层上。</li>
<li>多尺度可变形注意力与以前的单尺度版本非常相似，只是它从多尺度特征图中采样LK 点，而不是从单尺度特征图中采样 K 点。</li>
</ul>
<p>当选$L=1,K=1，且W_m^`∈R^{C_v\times C}$固定为单位矩阵的时候，上述公式退化为deformable convolution。</p>
<h3 id="Deformable-Transformer-Encoder"><a href="#Deformable-Transformer-Encoder" class="headerlink" title="Deformable Transformer Encoder"></a>Deformable Transformer Encoder</h3><p>我们将DETR中处理特征图的Transformer注意力模块替换为提出的多尺度可变形注意力模块。编码器的输入和输出都是具有相同分辨率的多尺度特征图。</p>
<p>在编码器中，通过ResNet (transformed by a 1 × 1 convolution) 中的从 $C_3$阶段到$C_5$阶段的输出特征图中提取多尺度特征图$\{x^l\}^{L-1}_{l=1}(L = 4)$，其中$C_l$分辨率是输入图的$\frac{1}{2^l}$ 。 在最后的$C_5$级上通过 3 × 3 步长为 2 的卷积得到的最低分辨率特征图$x^L$，记为$C_6$。所有多尺度特征图的通道数为 C = 256 。注意：FPN 中自顶向下的结构没有被使用，因为我们提出的多尺度可变形注意力机制本身可以在多尺度特征图之间交换信息。多尺度特征图的构造如下图所示。注：添加FPN不会提高性能，因为本文所设计的结构能在不同层级之间交换信息，和FPN的功能相同</p>
<p><img src="/2022/08/22/Deformable%20DETR/image-20220810101839490.png" alt="image-20220810101839490"></p>
<p>在Deformable Transformer Encoder的应用中，输出是与输入具有相同分辨率的多尺度特征图。key和query均为多尺度特征图中的像素。对于每个查询像素，参考点为其本身。为了识别每个查询像素位于哪个特征级别（即属于目标物体的概率），除了位置嵌入外，我们在特征表示中添加了一个尺度级别的嵌入，记为$e_l$。不同于固定编码的位置嵌入，尺度级嵌入$\{e_l\}^L_{l=1}$随机初始化并与网络联合训练。</p>
<h3 id="Deformable-Transformer-Decoder"><a href="#Deformable-Transformer-Decoder" class="headerlink" title="Deformable Transformer Decoder"></a>Deformable Transformer Decoder</h3><p>解码器中存在交叉注意力和自注意力模块，两种类型的注意力模块的query elements都是object query。</p>
<p>在交叉注意力模块中，object query从特征图中提取特征，其中的key元素是编码器输出的特征图。</p>
<p>在自注意力模块中，object query是相互作用的，其中的key元素是object query。由于我们提出的可变形注意力模块是为了处理卷积特征图作为key元素而设计的，因此我们只将每个交叉注意力模块替换为多尺度可变形注意力模块，而自注意力模块保持不变。对于每个object query，参考点的二维归一化坐标$p̂ _q$ 通过可学习的线性投影和sigmoid函数从其对象查询嵌入中预测。</p>
<p>由于multi-scale deformable attention module提取参考点周围的图像特征，我们让检测头预测边界框作为参考点的相对偏移量，以进一步降低优化难度。将参考点作为箱体中心的初始猜测。检测头预测参考点的相对偏移量。这样，学习到的解码器注意力将与预测的边界框具有较强的相关性，这也加速了训练收敛。</p>
<p>通过将DETR中的Transformer注意力模块替换为可变形注意力模块，我们建立了一个高效、快速收敛的检测系统，称为可变形DETR 。</p>
<h3 id="多种计算方式之间关系"><a href="#多种计算方式之间关系" class="headerlink" title="多种计算方式之间关系"></a>多种计算方式之间关系</h3><p><img src="/2022/08/22/Deformable%20DETR/1858467-20220401094649223-612897571.png" alt="img"></p>
<h3 id="deformable-DETR结构示意图"><a href="#deformable-DETR结构示意图" class="headerlink" title="deformable DETR结构示意图"></a>deformable DETR结构示意图</h3><p><img src="/2022/08/22/Deformable%20DETR/1858467-20220401094708641-447017032.png" alt="img"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/VIT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/VIT/" class="post-title-link" itemprop="url">VIT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 20:56:31 / 修改时间：21:35:42" itemprop="dateCreated datePublished" datetime="2022-08-22T20:56:31+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="VIT-An-image-is-worth-16-x-16-words-Transformer-for-image-recognition-at-scale"><a href="#VIT-An-image-is-worth-16-x-16-words-Transformer-for-image-recognition-at-scale" class="headerlink" title="(VIT) An image is worth 16 x 16 words: Transformer for image recognition at scale"></a>(VIT) An image is worth 16 x 16 words: Transformer for image recognition at scale</h2><h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p>ViT是2020年Google团队提出的将Transformer应用在图像分类的模型，因其模型“简单”且效果好，可扩展性强，在数据量越大的前提下效果越好，从而成为了transformer在CV领域应用的里程碑著作。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置（即先验知识，如：卷及神经网络所默认的平移不变性等）的限制，可以在下游任务中获得较好的迁移效果。但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些。这是因为CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）（即$f(g(x))=g(f(x))$),其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。</p>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>ViT的整体结构如下图所示：</p>
<p><img src="/2022/08/22/VIT/v2-5afd38bd10b279f3a572b13cda399233_720w.jpg" alt="img"></p>
<p>整个ViT的运行流程如下所示：</p>
<ul>
<li><p>假设输入图片大小为224x224,每个patch对应的像素为16x16，则对于每张图片而言，其生成的patch数量为$（224/16）×（224/16）=196$，即生成的patch序列长度为196，每个patch的大小为$16<em>16</em>3$，每个patch的元素总量为$768$。</p>
</li>
<li><p>对于ViT而言，其整体的结构和流程都是模仿transformer和bert的所以其分为以下几个部分：</p>
<ul>
<li>Patch Embeding：上述所生成的每个patch通过投影层，投影成固定长度的向量，作为encoder的第一部分输入。其固定长度的向量的长度定义为768，所以输入的patch序列的维度为$196<em>768$，Patch Embedding的维度为$768</em>768$，最终得到的Patch Embedding的向量长度为$196*768$。</li>
<li>Position Embedding：由于将图片分为多个patch之后，每个patch经过投影的过程中不引入位置编码信息，所以仿照bert引入position enbedding部分。其位置编码可以理解为是一个有N行（输入patch序列的长度），每行有768（embedding的维度）个元素的矩阵，其第i行就代表了第i个位置所对应的Position Embedding的值。将Patch Embedding与Position Embedding的值相加，由于维度都是$196*768$，所以加之后的维度相同</li>
<li>$[cls]$:仿照bert中的$[cls]$，在196x768的基础上加一维，变成197x768，由于其具体的计算过程中是元素和元素之间两两计算，所以作者认为这样可以在计算过程中学到如何从其他元素上学到我们所需要的信息。并最终在经过Encoder部分的计算之后，取对应位置的输出进行分类。</li>
</ul>
</li>
<li>Encoder：encoder部分由$Add/Norm+多头自注意力机制+Add/Norm+MLP$组成，其过程与transformer中的一致。其输入维度为$(196+1)*768$，经过Encoder之后输入维度与输入相同。所以支持多个Encoder块进行叠加。</li>
<li>MLP Head：在MLP时，输入为197x768，并经过与bert相似的操作，将维度放大四倍再收缩回去，即变为$197<em>（768</em>4）$再缩小变为$197*768$。</li>
<li>最终选取MLP的第0个位置处的元素（即为[cls]对应的位置处的元素）进行图片分类</li>
</ul>
<p>最终的计算步骤如下图所示：</p>
<p><img src="/2022/08/22/VIT/v2-ebf697b1994598019a6a59855dc0dbed_720w.png" alt="img"></p>
<h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><h4 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h4><p>作者研究对比了多种位置编码方式：</p>
<ul>
<li>1-D的位置编码</li>
<li>2-D的位置编码</li>
<li>相对位置编码</li>
</ul>
<p>作者实验结论为：不管使用哪种位置编码方式，模型的精度都很接近，甚至不适用位置编码，模型的性能损失也没有特别大。原因可能是ViT是作用在image patch上的，而不是image pixel，对网络来说这些patch之间的相对位置信息很容易理解，所以使用什么方式的位置编码影像都不大。</p>
<p><img src="/2022/08/22/VIT/v2-99f02198921e7aed8162cd7af8a29805_720w.jpg" alt="img"></p>
<h4 id="image-presentation"><a href="#image-presentation" class="headerlink" title="image presentation"></a>image presentation</h4><p>关于使用[cls]进行学习和直接对输出的结果通过average pooling进行学习的方法，通过实验表明两者区别不大。</p>
<p><img src="/2022/08/22/VIT/v2-4a8b39b1d2dd43d1e9b16edbc38b1660_720w.jpg" alt="img"></p>
<p>文章主要为了和bert类似，所以引入[cls]进行学习</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">凯</p>
  <div class="site-description" itemprop="description">选择大于努力</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">凯</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
