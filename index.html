<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="选择大于努力">
<meta property="og:type" content="website">
<meta property="og:title" content="凯_kaiii">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="凯_kaiii">
<meta property="og:description" content="选择大于努力">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="凯">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>凯_kaiii</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">凯_kaiii</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">暂无</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/04/RK3588s%E9%83%A8%E7%BD%B2%E7%9B%B8%E5%85%B3_NEW/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/04/RK3588s%E9%83%A8%E7%BD%B2%E7%9B%B8%E5%85%B3_NEW/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-03-04 15:27:13 / 修改时间：15:20:59" itemprop="dateCreated datePublished" datetime="2023-03-04T15:27:13+08:00">2023-03-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="RK3588s部署相关"><a href="#RK3588s部署相关" class="headerlink" title="RK3588s部署相关"></a>RK3588s部署相关</h1><p>目前无人机上开发板为ROC-RK3588S-PC，为在其上进行深度学习模型的推理，需要对板载的NPU进行配置使用。为使用该NPU，需要下载<a target="_blank" rel="noopener" href="https://wiki.t-firefly.com/zh_CN/ROC-RK3588S-PC/usage_npu.html">RKNN SDK</a>，RKNN SDK为RK3588s提供编程接口，帮助用户部署使用通过RKNN-Toolkit2导出的RKNN模型。</p>
<p>下列代码输出的是rk3588机载npu的使用率，从而可以验证板载npu是否被使用</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/kernel/debug/rknpu/load</span><br></pre></td></tr></table></figure>
<p>目前3588的部署方式大致有两种，一为利用RKNN官方支持的方式进行部署，另一为利用FastDeploy，其在RKNN官方的基础上加了一层进行后续部署。</p>
<h2 id="基本组件及功能介绍"><a href="#基本组件及功能介绍" class="headerlink" title="基本组件及功能介绍"></a>基本组件及功能介绍</h2><h3 id="RKNN-Toolkit2与RKNN-Toolkit-Lite2套件介绍"><a href="#RKNN-Toolkit2与RKNN-Toolkit-Lite2套件介绍" class="headerlink" title="RKNN-Toolkit2与RKNN Toolkit Lite2套件介绍"></a><strong>RKNN-Toolkit2与RKNN Toolkit Lite2套件介绍</strong></h3><h4 id="RKNN-Toolkit2"><a href="#RKNN-Toolkit2" class="headerlink" title="RKNN-Toolkit2"></a>RKNN-Toolkit2</h4><p>RKNN-Toolkit2是为用户提供在PC平台上进行Rockchip芯片NPU模型转换、推理和性能评估的开发套件。用户通过该工具提供的Python 接口可以便捷地完成以下功能：</p>
<ol>
<li>模型转换：支持Caffe、TensorFlow、TensorFlow Lite、ONNX、DarkNet、PyTorch 等模型转为RKNN 模型，并支持RKNN 模型导入导出，RKNN 模型能够在Rockchip NPU 平台上加载使用。</li>
<li>量化功能：支持将浮点模型量化为定点模型，目前支持的量化方法为非对称量化（ asymmetric_quantized-8 及asymmetric_quantized-16 ） ， 并支持混合量化功能。</li>
<li>模型推理：能够在PC 上模拟Rockchip NPU 运行RKNN 模型并获取推理结果；或将RKNN模型分发到指定的NPU 设备上进行推理并获取推理结果。</li>
<li>性能和内存评估：将RKNN 模型分发到指定NPU 设备上运行，以评估模型在实际设备上运行时的性能和内存占用情况。</li>
<li>量化精度分析：该功能将给出模型量化前后每一层推理结果与浮点模型推理结果的余弦距离，以便于分析量化误差是如何出现的，为提高量化模型的精度提供思路。</li>
</ol>
<p><strong>即，该部分需要在linux-ubuntu的电脑上进行安装，从而完成模型的转换和模型的量化等功能</strong></p>
<h4 id="RKNN-Toolkit-Lite2"><a href="#RKNN-Toolkit-Lite2" class="headerlink" title="RKNN -Toolkit-Lite2"></a>RKNN -Toolkit-Lite2</h4><p>RKNN -Toolkit-Lite2为RKNN-Toolkit-lite2的一部分，为带有瑞芯NPU平台提供Python编程接口，帮助用户部署使用RKNN-Toolkit2导出的RKNN模型。</p>
<h4 id="rknpu2"><a href="#rknpu2" class="headerlink" title="rknpu2"></a>rknpu2</h4><p>rknpu2为带有瑞芯NPU平台提供c语言编程接口，帮助用户部署使用 RKNN-Toolkit2 导出的 RKNN 模型。</p>
<p><strong>即RKNN -Toolkit-Lite2和rknpu2分别为板载上使用python或c++调用npu的接口，需要在板子上进行安装从而完成对应的npu的调用</strong></p>
<h4 id="FastDeploy"><a href="#FastDeploy" class="headerlink" title="FastDeploy"></a>FastDeploy</h4><p>FastDeploy是百度推出的一款AI算法推理部署的工具。其为在RKNN官方的库的基础上进行二次开发及封装，从而实现更方便的一种算法部署方式。但是其由于目前依旧为develop阶段，很多接口尚未完全开发完毕。</p>
<p><strong>即FastDeploy需要在ubuntu服务器端和板载端均进行安装，目前正在快速开发中，交流群内较为活跃，遇到问题好解决</strong></p>
<h2 id="使用RKNN官方例程"><a href="#使用RKNN官方例程" class="headerlink" title="使用RKNN官方例程"></a>使用RKNN官方例程</h2><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><h4 id="服务器端环境"><a href="#服务器端环境" class="headerlink" title="服务器端环境"></a>服务器端环境</h4><p>首先在<a target="_blank" rel="noopener" href="https://wiki.t-firefly.com/zh_CN/ROC-RK3588S-PC/usage_npu.html">firefly官网中的RK3588S中的NPU使用</a>中或者<a href="wget https://bj.bcebos.com/fastdeploy/third_libs/rknpu2_device_install_1.4.0.zip">百度提供的下载链接</a>下载RKNN SDK，然后由于目前RKNN ToolKit2只支持python3.6或者python3.8，所以在linux-ubuntu的电脑运行如下代码，安装对应代码</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 创建python3.<span class="number">8</span>环境</span><br><span class="line">conda create -n rknn2 python=<span class="number">3</span>.<span class="number">8</span></span><br><span class="line">conda activate rknn2</span><br><span class="line"></span><br><span class="line"># 安装 rknn-toolkit2</span><br><span class="line">pip install numpy==<span class="number">1</span>.<span class="number">16</span>.<span class="number">6</span></span><br><span class="line">sudo apt-get install libxslt1-dev zlib1g zlib1g-dev libglib2.<span class="number">0</span>-<span class="number">0</span> libsm6 libgl1-mesa-glx libprotobuf-dev gcc g++</span><br><span class="line">pip install rknn-toolkit2/packages/rknn_toolkit2-<span class="number">1</span>.<span class="number">3</span>.<span class="number">0</span>_11912b58-cp38-cp38-linux_x86_64.whl</span><br><span class="line"></span><br><span class="line"># 安装yaml</span><br><span class="line">pip install pyyaml</span><br></pre></td></tr></table></figure>
<h4 id="板端环境"><a href="#板端环境" class="headerlink" title="板端环境"></a>板端环境</h4><p>首先将板子从原生的安卓刷系统为ubuntu系统，然后在<a target="_blank" rel="noopener" href="https://wiki.t-firefly.com/zh_CN/ROC-RK3588S-PC/usage_npu.html">firefly官网中的RK3588S中的NPU使用</a>中下载RKNN SDK，由于目前的rknn_toolkit2_lite2只支持python3.7或者python3.9，所以输入下列代码安装rknn_toolkit2_lite2</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> rknpu2_device_install</span><br><span class="line"># RK3588运行以下代码</span><br><span class="line">sudo rknn_install_rk3588.sh</span><br></pre></td></tr></table></figure>
<h3 id="模型转换步骤"><a href="#模型转换步骤" class="headerlink" title="模型转换步骤"></a>模型转换步骤</h3><ul>
<li><p>首先通过任务的不同需求训练出神经网络</p>
</li>
<li><p>通过各类转换工具将模型转换为onnx</p>
</li>
<li><p>将onnx模型通过PKNN-Toolkit2转换为RKNN格式</p>
<ul>
<li><p>利用RKNN-Toolkit2的Python API接口导出RKNN格式的模型。操作流程如下</p>
<p>1、 创建RKNN对象，初始化RKNN SDK环境。</p>
<p>2、 调用config接口设置模型预处理参数。</p>
<p>3、 调用对应加载第3方框架接口，加载TensorFlow、Pytorch、ONNX模型。</p>
<p>4、 调用build接口构建RKNN模型。</p>
<p>5、 调用export_rknn接口导出RKNN模型</p>
</li>
</ul>
<p>对应文档查看下载包内的<strong>Rockchip_User_Guide_RKNN_Toolkit2_CN</strong></p>
</li>
</ul>
<h3 id="默认转换好的模型在RK3588s上的使用（RKNPU2）"><a href="#默认转换好的模型在RK3588s上的使用（RKNPU2）" class="headerlink" title="默认转换好的模型在RK3588s上的使用（RKNPU2）"></a>默认转换好的模型在RK3588s上的使用（RKNPU2）</h3><ul>
<li>首先针对于自身平台下载gcc交叉编译器 gcc-9.3.0-x86_64_arrch64-linux-gnu</li>
<li>然后进入/rknpu2_1.3.0/examples 文件夹</li>
<li>假定想要测试转换好了的yolov5，则进入对应的rknn_yolov5_demo文件夹</li>
<li>使用./build-linux_RK3588.sh进行编译</li>
<li>进入./install/rknn_yolov5_demo_linux 文件夹使用./rknn_yolov5_demo ./model/RK3588/yolov5s-640-640.rknn ./model/bus.jpg 进行测试</li>
</ul>
<h3 id="默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）"><a href="#默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）" class="headerlink" title="默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）"></a>默认转换好的模型在RK3588s上的使用（RKNN-ToolKit2-lite）</h3><ul>
<li>利用conda 创建py36或者py38的环境</li>
<li>进入rknn-toolkit2-1.3.0/rknn_toolkit_lite2文件夹，进入package文件夹安装对应的whl</li>
<li>进入examples/inference_with_lite文件夹，使用python test.py进行测试</li>
</ul>
<h2 id="使用百度-Fast-Deploy相关工具进行部署"><a href="#使用百度-Fast-Deploy相关工具进行部署" class="headerlink" title="使用百度 Fast Deploy相关工具进行部署"></a>使用百度 Fast Deploy相关工具进行部署</h2><p>官方视频例程大都是老版本的配置，其在现有的工程下的使用比较落后。故大致步骤可和<a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/education/preview/3610910">官方视频</a>内相同，但具体节点的使用方式有所差异。</p>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>对应部署的环境配置分为板载RK3588s上的环境配置和个人PC上的环境配置，对应需要配置的环境如下所示</p>
<p><img src="/2023/03/04/RK3588s%E9%83%A8%E7%BD%B2%E7%9B%B8%E5%85%B3_NEW/image-20230221221155971.png" alt="image-20230221221155971"></p>
<p>RKNN官方例程中的服务器端环境和板端环境均需要进行配置，然后针对于FastDeploy进行环境配置。</p>
<p>FastDeploy在板端的安装（c++）</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/PaddlePaddle/FastDeploy.git</span><br><span class="line"><span class="built_in">cd</span> FastDeploy</span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line"></span><br><span class="line"># Only a few key configurations are introduced here, see README.<span class="built_in">md</span> <span class="keyword">for</span> details.</span><br><span class="line"># -DENABLE_ORT_BACKEND:     Whether to enable ONNX model, default OFF</span><br><span class="line"># -DENABLE_RKNPU2_BACKEND:  Whether to enable RKNPU model, default OFF</span><br><span class="line"># -RKNN2_TARGET_SOC:        Compile the SDK board model. Enter RK356X or RK3588 with case sensitive required.</span><br><span class="line">cmake ..  -DENABLE_ORT_BACKEND=ON \</span><br><span class="line">	      -DENABLE_RKNPU2_BACKEND=ON \</span><br><span class="line">	      -DENABLE_VISION=ON \</span><br><span class="line">	      -DRKNN2_TARGET_SOC=RK3588 \</span><br><span class="line">          -DCMAKE_INSTALL_PREFIX=$&#123;PWD&#125;/fastdeploy-<span class="number">0</span>.<span class="number">0</span>.<span class="number">3</span></span><br><span class="line">make -j8</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>FastDeploy在板端的安装（python）</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/PaddlePaddle/FastDeploy.git</span><br><span class="line"><span class="built_in">cd</span> FastDeploy</span><br><span class="line"><span class="built_in">cd</span> python</span><br><span class="line"></span><br><span class="line">export ENABLE_ORT_BACKEND=ON</span><br><span class="line">export ENABLE_RKNPU2_BACKEND=ON</span><br><span class="line">export ENABLE_VISION=ON</span><br><span class="line">export RKNN2_TARGET_SOC=RK3588</span><br><span class="line">python3 setup.py build</span><br><span class="line">python3 setup.py bdist_wheel</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> dist</span><br><span class="line"></span><br><span class="line">pip3 install fastdeploy_python-<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>-cp39-cp39-linux_aarch64.whl</span><br></pre></td></tr></table></figure>
<p>个人PC环境配置及模型转换</p>
<p>本次尝试在对应远程服务器上进行环境部署，该服务器主要负责对应模型的训练，模型的转换，目前要求为linux-64bit环境。</p>
<ul>
<li><p>假定使用的是paddleDetection训练得到想要的模型，首先使用PaddleDetection中的/tools/export_model.py对训练得出的模型进行导出，将Paddle动态图转换为静态图，对应转换模型代码为</p>
<ul>
<li>```cmd<br>python ./tools/export_model.py —config configs/picodet/picodet_s_416_visdrone.yml<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 转换后的模型会存放于</span><br><span class="line"></span><br><span class="line">- ```cmd</span><br><span class="line">  ./output_inference/picodet_s_416_visdrone</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>进一步将模型从静态的paddle模型转化到onnx模型，需要使用Paddle2ONNX库，该库的安装命令使用</p>
<ul>
<li><p>```cmd<br>pip install paddle2onnx    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 对应的模型的转换，从静态paddle模型转换为onnx格式，使用命令，对应[官网](https://github.com/PaddlePaddle/Paddle2ONNX)，（在Paddle2ONNX文件夹内使用）</span><br><span class="line"></span><br><span class="line">- ```cmd</span><br><span class="line">  #静态图转onnx模型</span><br><span class="line">  paddle2onnx --model_dir picodet_s_416_coco_lcnet --model_filename model.pdmodel --params_filename model.pdiparams --save_file  picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx --enable_dev_version True</span><br><span class="line">  #固定模型输入形状，改为静态shape</span><br><span class="line">  python -m paddle2onnx.optimize \--input_modelpicodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx \--output_modelpicodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet.onnx \--input_shape_dict&quot;&#123;&#x27;image&#x27;:[1,3,416,416]&#125;&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意，若ONNX不支持对应的算子，如自适应池化层，需要对得到的模型进行输入的固定，即改为静态shape。对应的查看方式为转换为onnx格式文件之后，使用[onnx结构查询——netron][<a target="_blank" rel="noopener" href="https://netron.app/]对onnx模型结构进行查看。">https://netron.app/]对onnx模型结构进行查看。</a></p>
</li>
</ul>
</li>
<li><p>然后将得到的onnx模型转换为对应的rknn模型，需要书写转换用到的yaml文件</p>
<ul>
<li><p>转换的yaml书写要点可见<a target="_blank" rel="noopener" href="https://www.github.com/PaddlePaddle/FastDeploy/blob/develop/docs/cn/faq/rknpu2/export.md">官网</a></p>
<ul>
<li><p>大体上如下所示</p>
</li>
<li><p>```cmd<br>mean:<br>  -</p>
<pre><code>- 128.5
- 128.5
- 128.5
</code></pre><p>std:<br>  -</p>
<pre><code>- 128.5
- 128.5
- 128.5
</code></pre><p>model_path: “./scrfd_500m_bnkps_shape640x640.onnx”<br>outputs_nodes:<br>do_quantization: True<br>dataset: “./datasets.txt”<br>output_folder: “./“</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 转换用的语句为</span><br><span class="line"></span><br><span class="line">  - ```cmd</span><br><span class="line">    python tools/rknpu2/export.py --config_path tools/rknpu2/config/RK3588/picodet_s_416_coco_lcnet.yaml</span><br></pre></td></tr></table></figure>
</li>
<li><p>对应使用的为fastdeploy内的对rknpu2的export函数，得到对应的rknn模型</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RK3588s环境配置及实机程序运行"><a href="#RK3588s环境配置及实机程序运行" class="headerlink" title="RK3588s环境配置及实机程序运行"></a>RK3588s环境配置及实机程序运行</h3><ul>
<li><p>将对应的rknn模型和cfg，对应图像拷贝到RK3588板子上，并运行<a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/FastDeploy/tree/develop/examples/vision/detection/paddledetection/rknpu2/python">官网</a>提供的infer代码进行推理检测</p>
<ul>
<li>```python<br>import fastdeploy as fd<br>import cv2<br>import os</li>
</ul>
</li>
</ul>
<pre><code>def parse_arguments():
    import argparse
    import ast
    parser = argparse.ArgumentParser()
    parser.add_argument(
        &quot;--model_file&quot;,
        default=&quot;./picodet_s_416_coco_lcnet/picodet_s_416_coco_lcnet_rk3588_unquantized.rknn&quot;,
        help=&quot;Path of rknn model.&quot;)
    parser.add_argument(
        &quot;--config_file&quot;,
        default=&quot;./picodet_s_416_coco_lcnet/infer_cfg.yml&quot;,
        help=&quot;Path of config.&quot;)
    parser.add_argument(
        &quot;--image&quot;,
        type=str,
        default=&quot;./000000014439.jpg&quot;,
        help=&quot;Path of test image file.&quot;)
    return parser.parse_args()


if __name__ == &quot;__main__&quot;:
    args = parse_arguments()

    model_file = args.model_file
    params_file = &quot;&quot;
    config_file = args.config_file

    # 配置runtime，加载模型
    runtime_option = fd.RuntimeOption()
    runtime_option.use_rknpu2()

    model = fd.vision.detection.PPYOLOE(
        model_file,
        params_file,
        config_file,
        runtime_option=runtime_option,
        model_format=fd.ModelFormat.RKNN)
    model.preprocessor.disable_normalize()
    model.preprocessor.disable_permute()
    model.postprocessor.apply_decode_and_nms()

    # 预测图片分割结果
    im = cv2.imread(args.image)
    result = model.predict(im)
    print(result)

    # 可视化结果
    vis_im = fd.vision.vis_detection(im, result, score_threshold=0.5)
    cv2.imwrite(&quot;visualized_result.jpg&quot;, vis_im)
    print(&quot;Visualized result save in ./visualized_result.jpg&quot;)
```
</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/" class="post-title-link" itemprop="url">Towards Data-Efficient Detection Transformer</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 21:01:24 / 修改时间：21:34:28" itemprop="dateCreated datePublished" datetime="2022-08-22T21:01:24+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Towards-Data-Efficient-Detection-Transformers"><a href="#Towards-Data-Efficient-Detection-Transformers" class="headerlink" title="Towards Data-Efficient Detection Transformers"></a>Towards Data-Efficient Detection Transformers</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>DETR在足量样本的COCO数据集上表现出了有竞争性的效果。然而我们发现许多DETR类的方法在内容数量较少的数据集上（如Cityscapes）会有明显的性能的下降。换而言之，DETR通常需要大量的数据。为了处理这个问题。我们逐步的将数据效率高的RCNN变换为代表性的DETR，分析了影响数据效率（data efficiency）的因素。试验结果表明从局部图片进行稀疏特征采样是影响的关键。基于这个观察，本文通过简单的交替 key 和 value序列在cross attention中的构造方式，用对原始模型最少的改变的方式缓解了现存DETR方法对数据需求量巨大的问题。另外，我们介绍了一个简单但有效的数据增强的方法，从而提供更丰富的监督并提高了数据效率。实验证明，我们的方法可以被很容易的应用到不同的DETR变种上去，并在较小和较大的数据集上均可提升检测效果。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>目标检测是在计算机视觉领域里面的长盛不衰的话题。最近一种新型的目标检测算法，名叫detection transformer，因为其的简单和尚可的检测效果吸引了许多的注意力。这个类别的先驱工作是DETR，其将目标检测的任务看作是直接的集合预测问题，并利用transformer直接将目标查询转换为目标对象。其实现了相对于开创性的Faster RCNN在常用的COCO数据集上更好的效果，但其具有收敛速度显著慢于基于CNN系列检测器的缺点。因为这个原因，许多随后的工作都是致力于提高DETR的收敛速度。</p>
<ul>
<li>Deformable DETR：通过efficient attention mechanism机制</li>
<li>Swin transformer：通过conditional spatial query机制</li>
<li>（SMCA）Fast convergence of detr with spatially modulated co-attention：通过regression-aware co-attention机制</li>
</ul>
<p>这些上述的方法都可以在COCO数据集上以相似的训练代价，实现相对于Faster RCNN而言更好的检测效果，证明了DETR类方法的优越性。</p>
<p>现有的工作大都认为DETR类的方法在简单性和模型效果上均优于基于CNN的目标检测器。然而本文发现，DETR只有在充足的训练数据的情况下（例如COCO2017,有118K训练数据）才能展现出其优越的性能，然而在训练数据量不是非常充足的时候，其的效果会出现明显的下降。以自动驾驶领域常用的数据集Cityscapes（约3k训练数据）为例，大部分的DETR类的方法的AP小于Faster RCNN的AP的一半。且不同的DETR类的检测器，其性能的差距在COCO数据集上是小于3AP的，但在数量较小的Cityscapes数据集上，其会存在一个明显的差距，其性能差距约有15AP。如下图所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220811161935395.png" alt="image-20220811161935395"></p>
<p>这些发现证明了DETR类的目标检测相较于CNN类的目标检测器而言，更需要大量的数量进行训练。然而带有标签的数据的获取是需要大量的时间和人力的。</p>
<p>总而言之，为了迎合目前现存的DETR对训练数据的需求，需要大量的人力和计算资源。为了应对这个问题，本文首先从实验上，通过逐步的将数据高效的Sparse RCNN转换为DETR，分析了影响DETR中影响数据效率的关键性因素。我们的发现和分析表明：</p>
<ul>
<li>稀疏的局部特征采样是影响数据效率的关键，<ul>
<li>其缓解了学习注意到特定物体的困难</li>
<li>其避免了图像像素两倍的计算复杂度</li>
<li>令利用多尺度的特征成为可能，多尺度的特征已经被证明在目标检测任务中是关键的</li>
</ul>
</li>
</ul>
<p>基于上述的观察，我们通过简单的交替 key 和 value序列在cross attention中的构造方式，提升了现存的DETR类的目标检测算法的数据效率。具体来说，我们在前一个解码器层预测的边界框的指导下，对发送到交叉注意力层的键和值特征执行稀疏采样特征，这样对原始模型的修改最少，并且没有任何专门的模块。另外，本文通过提供给DETR丰富的监督信号来缓解对数据的需求。为达到这个目的，本文提出了一种标签加强的方式，通过在标签分配的过程中重复前景物体的label去高效并简单的执行。这个方法可以被应用在不同的DETR类的方法从而提升其的数据效率。有趣的是，其依旧带来了在训练数据充足的COCO数据集上的性能提升。</p>
<p>本文的贡献如下总结所示：</p>
<ul>
<li>本文确定了DETR的数据效率的问题。虽然DETR实现了在COCO数据集上的优秀效果，其一般会在小规模的数据集上遭受到明显的性能下降。</li>
<li>本文通过从 Sparse RCNN 到 DETR 的逐步模型转换，通过实验分析了影响检测转换器数据效率的关键因素，并发现局部区域的稀疏特征采样是数据效率的关键。</li>
<li>本文通过简单的交替在cross-attention模块中key和value序列的构造方式，明显的提升了现存的DETR方法的数据效率</li>
<li>本文提出了一种简单但有效的标签增强策略，从而提供更丰富的监督信号并提升了数据效率。其可以与不同的方法融合，从而实现在不同数据集上的性能增益。</li>
</ul>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h4><p>目标检测在许多现实生活中是非常必要的，例如自动驾驶，缺陷检测和遥感。最具有代表性的目标检测的工作可以被粗略的分为两类，两阶段的Faster RCNN和一阶段的YOLO和RetinaNet。虽然上述方法有效，但上述方法一般而言是需要以来与许多人工设计（启发式算法）的先验，例如anchor generation和rule based 标签分配方式。</p>
<p>最近DETR提供了一种简单并且干净的目标检测的计算流程。其将目标检测看作是集合预测的任务，并应用transformer将稀疏的目标候选转换为目标物体。DETR的成功引爆了最近井喷的DETR类的方法，并且许多最近的工作都致力于缓解DETR的收敛速度慢的问题。</p>
<ul>
<li>DeformDETR 提出了可学习的稀疏特征采样的可变形注意力机制并聚合多尺度特征以加速模型收敛并提高模型性能。</li>
<li>CondDETR 提出从解码器嵌入中学习条件空间查询，这有助于模型快速学习定位四个末端以进行检测</li>
</ul>
<p>这些工作实现了在COCO 2017数据集上用相似的训练代价得到Faster RCNN更好的性能。这似乎表明DETR类的方法已经在简单性和性能上压制了Faster RCNN。但本文发现DETR通常需要更多的数据，并在小规模的数据集上表现比Faster RCNN要差。</p>
<h4 id="目标检测中的标签分配"><a href="#目标检测中的标签分配" class="headerlink" title="目标检测中的标签分配"></a>目标检测中的标签分配</h4><p>在目标检测中，标签分配是一个十分重要的组件。其将一个物体的ground truth与从模型中的一个预测相匹配，从而为训练提供监督信号。在DETR之前，许多的目标检测器采用的是一对多的匹配策略，其将每个ground trurh基于局部空间关系分类给多个预测框。而DETR相反，其是采用的一对一的匹配策略，将ground truth与预测框之间通过最小化全局匹配损失来进行匹配。这个标签分配方式被许多的后续的DETR方法所采用。尽管这样的分配方式具有避免了重复移除的过程的优点，但只有少量的候选目标在每次迭代的过程中被目标标签所监督。这样就会导致模型必须从大量的数据中获得足够的监督信号或需要更多论次的训练。为了解决这个问题，本文提出了一种标签增强的方式去提供更丰富的监督信号。</p>
<h4 id="视觉transformer（ViT）中的数据效率"><a href="#视觉transformer（ViT）中的数据效率" class="headerlink" title="视觉transformer（ViT）中的数据效率"></a>视觉transformer（ViT）中的数据效率</h4><p>视觉transformer正在成为特征提取器和视觉识别的CNN的替代品。尽管其具有优秀的性能表现，但其一般而言需要比CNN需要更多的数据，并依赖于大量的数据和更多轮次的训练。</p>
<ul>
<li>DeiT 通过从预训练的CNN上进行知识蒸馏，配合上更好的训练配方，从而提高了数据效率</li>
<li>Liu等人提出了一个密集的相对定位损失去提高ViT类算法的数据效率（Efficient training of visual transformers with small datasets）</li>
</ul>
<p>与之前专注于transformer主干在图像分类任务上的数据效率问题不同，本文在目标检测任务上处理DETR数据效率的问题</p>
<h3 id="RCNN类算法与DETR类算法的不同之处分析"><a href="#RCNN类算法与DETR类算法的不同之处分析" class="headerlink" title="RCNN类算法与DETR类算法的不同之处分析"></a>RCNN类算法与DETR类算法的不同之处分析</h3><p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812084137035.png" alt="image-20220812084137035"></p>
<p>上图为从SRCN（Sparse RCNN）逐渐转化为DETR的过程中，在Ciytscapes数据集上，分别在训练50 epoch和300 epoch的情况下的AP情况。</p>
<p>对上图进行分析可得，DETR一般而言相比与RCNN需要更多的数据。为了寻找影响数据效率的关键性因素，本文将数据效率高的RCNN逐步的转变为数据效率较低的DETR，从而消融不同设计的影响。相同的实验方法在ATSS和Visformer中被使用，但实验目的不同。</p>
<h4 id="检测器的选择"><a href="#检测器的选择" class="headerlink" title="检测器的选择"></a>检测器的选择</h4><p>为了从模型的转换中获得有效的结果，需要选择适当的检测器去参与实验。为了达到这个目的，本文选择Sparse RCNN和DETR作为实验模型，原因如下所示：</p>
<ul>
<li>两个模型都是在各自的领域里（RCNN类和DETR类）具有代表性的模型。所以由这两者的转换得出的结论可以推广到其他的探测器中去。</li>
<li>这两个模型在数据效率方面有巨大的差异</li>
<li>其在标签分配（label assignment）、损失函数设计（loss design），优化器选择（optimization）上具有许多的相似之处。这些相似之处可以在我们专注于核心部件的不同的时候消除没有那么重要的部件的影响。</li>
</ul>
<h4 id="Sparse-RCNN到DETR的转换"><a href="#Sparse-RCNN到DETR的转换" class="headerlink" title="Sparse RCNN到DETR的转换"></a>Sparse RCNN到DETR的转换</h4><ul>
<li>交替训练方式<ul>
<li>虽然Sparse RCNN和DETR有许多的相似之处，但其在训练策略（训练方式）上依旧有所不同。如分类损失、object query的数量，学习率和梯度剪切。本文首先通过将Sparse RCNN的训练策略用DETR的训练策略替代，我们发现Sparse RCNN用DETR的训练策略进行训练时，其在50 epoch时表现稍好，但在300epoch时表现较差。消除训练策略的差异可以帮助我们关注与影响数据效率的更核心的因素。</li>
</ul>
</li>
<li>移除FPN：<ul>
<li>多尺度特征融合已经被证明对目标检测是有效的。当CNN类的FPN neck可以实现在较小的计算代价的情况下完成多尺度特征融合，注意力机制有输入图像尺寸的平方的计算复杂度，使在DETR中对多尺度特征融合代价昂贵。因此DETR只采用了原图像经过32倍下采样的单尺度特征进行预测。在这个阶段，我们移除了FPN neck部分，并只将经过32倍下采样的特征传入检测头。模型在50epoch的情况下性能明显的下降了7.3AP</li>
</ul>
</li>
<li>引入transformer encoder：<ul>
<li>在DETR中，transformer encoder可以被认为是检测器的neck部分，其被用来处理被backbone提取出的特征。在移除了FPN neck之后，我们加入transformer encoder作为网络的neck。与在DETR中相似，backbone提取出的特征投影和位置编码同样被引入。试验结果表明AP在50epoch的时候有所下降，在300eopch的时候有所上升。我们推测其与ViT中相似，注意力机制因为其平方项的复杂度和缺少先验知识，其需要更长的训练epoch去收敛和发展其的优势。</li>
</ul>
</li>
<li>使用cross-attention替代dynamic convolutions<ul>
<li>在Sparse RCNN中的dynamic convolutions（动态卷积）和DETR中的cross-attention（互注意力）的作用相似。它们都基于图像特征的相似性自适应地将上下文聚合到候选对象。在这个步骤中，我们将dynamic convolutions替换为带有可学习的query positional embedding，其结果反直觉的表示：大量可学习的参数不一定会让模型需要更多的数据。事实上，动态卷积的70M的参数可以展现出相较于cross-attention而言更好的数据效率。</li>
</ul>
</li>
<li>对齐解码器中的dropout设置<ul>
<li>在Sparse RCNN和DETR中的decoder是非常相似的。在将dynamic convolution 用cross-attention替代之后，其可以被认为是transformer decoder。在其之间有一个轻微的不同是dropout layer在self-attention和FFN之间的使用。我们消除了这个影响。</li>
</ul>
</li>
<li>移除级联边缘框细化<ul>
<li>Sparse RCNN遵循了Cascade RCNN中的级联边缘框回归，其中每个decoder层都迭代的细化前一层做的边缘框预测。本文移除了这个步骤，模型性能有所下降。虽然级联边缘框细化没有被大多数的DETR类的检测器所使用，但其可以自然的被级联解码器所包含。</li>
</ul>
</li>
<li>移除ROIAlign<ul>
<li>Sparse RCNN和其余RCNN类的检测其相同，从感兴趣的局部区域采样特征，然后根据采样的稀疏特征进行预测。而每个DETR中的内容查询直接从全局的特征图中聚合特定于对象的信息。在这个步骤中，我们移除了Sparse RCNN中的ROIAlign，box target transformation也被移除。我们可以发现，模型的性能出现了明显的下降，在50epoch的情况下出现了8.4Ap的下降。我们推测从整个特征图上学习到局部对象区域的代价较大，所以模型需要更多的数据和训练epoch去获取局部属性。</li>
</ul>
</li>
<li>移除初始的proposals<ul>
<li>最终，DETR直接预测了目标的bounding box，RCNN类预测使用了一些初始化的先验。在这个步骤中，我们通过移除初始的proposals消除了影响。预料之外的是，这个小改变使模型性能出现了明显的下降。我们人文初始的proposals作为空间上的先验，帮助模型聚焦于局部空间信息，从而减少了从大量训练数据中学习局部性的需要</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>如上所示，从Sparse RCNN转换为DETR的结果和分析如下所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812084137035.png" alt="image-20220812084137035"></p>
<p>其在更改之后对AP影响大于5AP的本文认为是影响数据效率的关键因素，如下所示：</p>
<ol>
<li>局部稀疏特征采样</li>
<li>依赖稀疏特征采样的多尺度特征拥有可接受的计算复杂度</li>
<li>依赖于空间先验的预测</li>
</ol>
<p>其中，1和3有助于模型关注局部对象区域，减轻从大量数据中学习局部性的需求，而2有助于更全面地利用和增强图像特征，但它也依赖于稀疏特征。</p>
<p>DeformDETR是在DETR中特殊的一种，其表现出了与Sparse RCNN相比而言有可比性的数据效率。我们从Sparse RCNN到DETR的变换过程中可以对DeformDETR的数据效率进行解释：multi-scale deformable attention从图像的局部区域采样稀疏特征并利用多尺度特征。 模型的预测是相对于初始参考点的。 因此，DeformDETR 尽管没有专门设计在小型数据集上实现数据高效，但其满足了所有三个关键因素。</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>本节主要利用现有的DETR类方法，在对原始的设计做最小的改变的前提下提升数据效率。</p>
<ul>
<li>本文重新对现有的DETR类算法进行了审视思考</li>
<li>基于前文的实验和分析，对现有的数据需求量巨大的DETR类模型做最少的改变并显著的提升他们的数据效率。</li>
<li>提供一种简单但有效的标签增强方法，从而为DETR提供更丰富的监督信号提升数据效率。</li>
</ul>
<h4 id="对DETR的重新审视"><a href="#对DETR的重新审视" class="headerlink" title="对DETR的重新审视"></a>对DETR的重新审视</h4><h5 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h5><p>DETR通常来说，由backbone，transformer encoder，transformer decoder，prediction head构成。</p>
<ul>
<li><p>backbone：backbone首先从输入图片中提取多尺度的特征，被称作$\{f^l\}^L_{l=1}$，其中$f^l \in R^{H ×W ×C}$。最后一个特征曾有着最小的分辨率，将其展平并嵌入以获得$z^L \in R^{S^L \times D}$，其中$S^L =H^L \times W^L$是序列长度，$D$是特征维度。相应的，位置编码嵌入被表示为$p^L \in R^{S^L\times D}$。</p>
</li>
<li><p>transformer encoder：之后单尺度序列特征被transformer编码，并获得$Z^L_e \in R^{S^L \times D}$。</p>
</li>
<li><p>transformer decoder：decoder包含了$L_d$层的decoder layers。 查询内容的嵌入表示被初始化为$q_0\in R^{N\times D}$，其中$N$是查询的数量。每个decoder层 $DecoderLayer_l$采用上一个decoder的输出$q_{l-1}$，查询位置编码$p_l$，图像序列特征$z_l$和其位置嵌入$p_l$作为输入，输出为解码序列特征。即</p>
<p>$q _l= DecoderLayer_l (q_{l−1} , p_q , z_l, p_l),= 1 . . . L_d $</p>
<p>在大多数DETR类检测器中，例如DETR和CondDETR，单尺度的图像特征被解码器所利用，因此$z_l=z^L_e$、$p_l=p^L$，其中$l=1…L_d$</p>
<ul>
<li>prediction head ：DETR的head是使用的单纯的FFN前馈网络加上softmax进行的判断</li>
</ul>
</li>
</ul>
<h5 id="标签分配"><a href="#标签分配" class="headerlink" title="标签分配"></a>标签分配</h5><p>DETR将目标检测任务视作集合预测的问题，并对每个解码器层的预测执行深度监督。在这个过程中，标签集可以被表示如下：$y=\{y_1,…,y_M,\emptyset,…,\emptyset\}$，其中$M$为前景物体的在图像中的数量，$\emptyset$(no object)被填充到标签集合里，使标签集合的大小为$N$。相应的，每个decoder的输出可以被写作$\hat y = \{\hat y\}_{i=1}$。在标签分配的过程中，DETR搜寻一个最优的$τ \in T_N$，使得下述的匹配损失最小：</p>
<script type="math/tex; mode=display">
\hat τ= argmin_{$τ \in T_N}\sum^N_iL_{ match} (y_i , \hat y_{τ (i)})</script><p>其中$L_{ match} (y_i , \hat y_{τ (i)})$为在ground truth和index为$τ (i)$的预测之间的配对损失。</p>
<h4 id="模型的提升"><a href="#模型的提升" class="headerlink" title="模型的提升"></a>模型的提升</h4><h5 id="系数特征采样"><a href="#系数特征采样" class="headerlink" title="系数特征采样"></a>系数特征采样</h5><p>根据上述RCNN类算法与DETR类算法的不同之处分析，我们分析可得局部特征采样对数据效率是非常关键的。幸运的是，在DETR中，物体位置是在每个decoder layer之后预测得出的，因此，我们可以在上一个decoder预测的bounding box的指导下不需要引入新的参数的采样局部特征。如下图所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/vz2GLTd9Ylg5q.png" alt="img"></p>
<p>虽然有更复杂的局部特征采样方法可以使用，本文只采用了最常用的RoIAlign。采样操作可以被写成如下形式：</p>
<p>$z_L = RoIAlign(z^L_e , b_{l-1}),\ \ \ l=2…L_d$</p>
<p>其中$b_{l-1}$是上一层预测得出的边缘框，$z_l^L\in R^{N\times K^2\times D}$是被采样的特征，$K$是在RoIAlign采样的特征分辨率。注意reshape操作和flatten操作在上式中被省略。类似的，可以得到对应的position embedding  $p^L_l$。</p>
<p>在DETR中的级联结构使使用逐层边界框细化来提升检测性能很自然。本文在RCNN类算法与DETR类算法的不同之处分析处也验证了迭代细化和对初始空间参考进行预测的有效性。因此，本文如CondDETR一样引进了边缘框细化和在实施过程中的初始参考点。</p>
<h5 id="结合多尺度特征"><a href="#结合多尺度特征" class="headerlink" title="结合多尺度特征"></a>结合多尺度特征</h5><p>我们的系数特征采样使DETR以较小的计算花销使用多尺度特征变得可能。为了达到这个目的，本文使用backbone从被展平和嵌入之后的高分辨率特征提取特征以得到$\{z^l\}^{L-1}_{l=1} \in \R^{S^l \times D}$，从而进行局部特征采样。然而这些特征不被transformer encoder处理。虽然可以使用更复杂的技术，这些单尺度的被RoIAlign所采样的特征被简单的拼接，从而形成我们的多尺度的特征。这些特征可以被自然的利用cross-attention机制在decoder中被融合。</p>
<p>$z^{ms}_l=[z^1_l],[z^2_l],…,[z^L_l],l=2…L_d$</p>
<p>其中$z^{ms}_l \in \R^{N \times LK^2 \times D}$为多尺度特征，$z^l_L=RoIAlign(z^l,b_{l-1}),l-1…L-1$是。对应的位置嵌入$p^{ms}_l$用相似的方式得到。解码过程和原始的DETR是相同的。唯一的区别在于$z_l=z^{ms}_l$以及$p_l=p^{ms}_l$。</p>
<h4 id="标签增强"><a href="#标签增强" class="headerlink" title="标签增强"></a>标签增强</h4><p>DETR展现出了标签分配的一对一的分配方式。尽管拥有避免重复删除过程的优点，但只有少数检测候选者在每次迭代中都被提供了一个积极的监督信号。这样会导致模型需要更大数量的数据或者更多论次的训练，从而获得足够的监督。</p>
<p>为了缓解这个问题，本文提出了一种标签增强的策略为DETR提供更丰富的监督信号，即通过在二部图匹配的过程中重复positive labels。如下图所示：</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/kITnuhEstBzQy.png" alt="img"></p>
<p>我们为每个前景样本$y_i$重复labels $R_i$次，并使label set $N$的总长度不变。</p>
<p>$y=\{ y^1_1,y^2_1,…,y^{R_1}_1,…y^1_M,y^2_M,…,y^{R_M}_M,…,\emptyset,…,\emptyset    \}$</p>
<p>label assignment的其余公式与DETR中相同。</p>
<p>在实际操作的过程中，考虑以下两种重复策略：</p>
<ul>
<li>固定重复次数：所有positive的label都被重复相同的次数</li>
<li>固定positive采样比例：positive的labels被重复采样，从而确保有$r$个positive样本在label set中。</li>
</ul>
<p>特别的$F=N\times r$是重复标签后的预期正样本数。 我们首先将每个正标签重复 $F//M$次，然后随机抽取 $F \% M $个正标签而不重复。 默认情况下，我们使用固定重复次数策略，因为它更容易实现并且生成的标签集是确定性的。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>本文重点关注DETR的数据效率。因此，我们的大多数实验都是在 Cityscapes 和下采样 COCO 2017在内的小型数据集上进行的。具体来说，Cityscapes 数据集包含2,975 张用于训练的图像和500 张用于评估的图像。对于下采样的 COCO 2017 数据集，训练图像随机下采样0.1、0.05、0.02 和0.01，而评估集保持不变。此外，我们还验证了我们的方法在具有118K 训练图像的全尺寸 COCO 2017 数据集上的有效性。</p>
<h4 id="实施细节"><a href="#实施细节" class="headerlink" title="实施细节"></a>实施细节</h4><p>默认情况下，我们的特征采样实现为 RoIAlign，特征分辨率为4。包括三个不同的特征级别用于多尺度特征融合。我们的标签增强采用固定重复次数，并且使用阈值为0.7 的非极大值抑制(NMS)来去除重复。所有模型都训练了50 个 epoch，并且除非另有说明，否则学习率会在40 个 epoch 后衰减。在 ImageNet-1K 上预训练的 ResNet-50用作主干。为了保证足够的训练迭代次数，所有关于 Cityscapes 和下采样 COCO2017 数据集的实验都以8 的batch size进行训练。结果是使用不同的随机种子重复运行五次的平均值。我们的数据高效检测转换器仅对现有方法进行轻微修改。除非另有说明，否则我们遵循相应基线方法的原始实现细节。运行时间在 NVIDIA A100 GPU 上进行评估。</p>
<h4 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h4><h5 id="基于Cityscapes"><a href="#基于Cityscapes" class="headerlink" title="基于Cityscapes"></a>基于Cityscapes</h5><p>在本节中，我们将我们的方法与现有的DETR进行比较。 如下表所示，大多数检测变压器都存在数据效率问题。 尽管如此，通过对 CondDETR 模型进行微小更改，我们的 DE-CondDETR 能够实现与 DeformDETR 相当的数据效率。 此外，通过标签增强提供的更丰富的监督，我们的 DELA-CondDETR 超过了 DeformDETR 2.2 AP。 此外，我们的方法可以与其他检测转换器相结合，以显着提高它们的数据效率，例如，我们训练了 50 个 epoch 的 DE-DETR 和 DELA-DETR 的性能明显优于训练了 500 个 epoch 的 DETR。另外，我们的方法依旧提高了DeformDETR的数据效率。见下</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812145944038.png" alt="image-20220812145944038"></p>
<p>上表为DETR在Cityscapes上的比较，DE前缀表明使用了本文的data-efficient，LA表明使用了label增强。</p>
<hr>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812151829137.png" alt="image-20220812151829137"></p>
<p>上表为DeformDETR使用了LA之后的效果对比。</p>
<hr>
<h5 id="基于下采样的COCO2017数据集"><a href="#基于下采样的COCO2017数据集" class="headerlink" title="基于下采样的COCO2017数据集"></a>基于下采样的COCO2017数据集</h5><p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812151508333.png" alt="image-20220812151508333"></p>
<p>下采样的 COCO 2017 数据集分别包含 11,828 (10%)、5,914 (5%)、2,365 (2%) 和 1,182 (1%) 训练图像。 如上图 所示，我们的方法在很大程度上始终优于基线方法。 特别是，仅用 ∼1K 图像训练的 DELA-DETR 显着优于 DETR 基线，训练数据是训练数据的五倍。 同样，DELA-CondDETR 始终优于使用两倍数据量训练的 CondDETR 基线。</p>
<hr>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><p>在本节中，我们进行消融实验以更好地理解我们方法的每个组成部分。 所有消融研究都是在 DELACondDETR 和 Cityscapes 数据集上实施的，而更多基于 DELADETR 的消融研究可以在我们的附录中找到。</p>
<hr>
<h5 id="每个模块的有效性"><a href="#每个模块的有效性" class="headerlink" title="每个模块的有效性"></a>每个模块的有效性</h5><p>我们首先消融了我们方法中每个模块的作用，如下表所示。使用局部特征采样和多尺度特征融合将模型的性能分别显着提高了 8.3 和 6.4 AP。 此外，标签增强进一步将性能提高了 2.7 AP。 此外，单独使用标签增强也带来了 2.6 AP 的性能增益。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812152636147.png" alt="image-20220812152636147"></p>
<hr>
<h5 id="RoIAlign-的特征分辨率"><a href="#RoIAlign-的特征分辨率" class="headerlink" title="RoIAlign 的特征分辨率"></a>RoIAlign 的特征分辨率</h5><p>通常，RoIAlign 中较大的样本分辨率可提供更丰富的信息，从而提高检测性能。 然而，采样更大的特征分辨率也更耗时，并且增加了解码过程的计算成本。 如下表所示，当分辨率从 1 增加到 4 时，模型性能显着提高了 5.6 AP。但是，当分辨率进一步增加到 7 时，改进很小，并且增加了 FLOPs 和延迟。 为此，我们将 RoIAlign 的特征分辨率默认设置为 4。</p>
<h5 id="多尺度特征的数量"><a href="#多尺度特征的数量" class="headerlink" title="多尺度特征的数量"></a>多尺度特征的数量</h5><p>为了结合多尺度特征，我们还从主干中采样了 8 倍和 16 倍的下采样特征来构建3个不同级别的多尺度特征。 从上表可以看出，它显着提高了模型性能 6.4 AP。 然而，当我们进一步为多尺度融合添加 64 倍下采样特征时，性能下降了 0.5 AP。 默认情况下，我们使用 3 个特征级别进行多尺度特征融合。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812152843982.png" alt="image-20220812152843982"></p>
<hr>
<h5 id="标签增强的策略"><a href="#标签增强的策略" class="headerlink" title="标签增强的策略"></a>标签增强的策略</h5><p>在本节中，我们消融了提出的两种标签增强策略，即固定重复时间和固定正样本比率。 如下左表 所示，使用不同的固定重复次数可以持续提高 DE-DETR 基线的性能，但性能增益会随着重复次数的增加而降低。 因此，默认采用固定重复时间 2。 此外，如下右表 所示，虽然使用不同的比率可以提高 AP，但在正负样本比率为 1:3 时性能最佳，有趣的是，这也是Faster RCNN中最常用的正负采样比率。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812154348539.png" alt="image-20220812154348539"></p>
<h4 id="泛化到特征丰富的数据集"><a href="#泛化到特征丰富的数据集" class="headerlink" title="泛化到特征丰富的数据集"></a>泛化到特征丰富的数据集</h4><p>虽然上述实验表明，我们的方法可以在只有有限的训练数据可用时提高模型性能，但不能保证我们的方法在训练数据充足的情况下仍然有效。 为此，我们用足够多的数据在 COCO 2017 上评估了我们的方法。 从下表 中可以看出，我们的方法不会降低 COCO 2017 上的模型性能。相反，它提供了改进效果。 具体来说，DELA-DETR 和 DELA-CondDETR 分别将其相应的基线提高了 8.3 和 2.8 AP。</p>
<p><img src="/2022/08/22/Towards%20Data-Efficient%20Detection%20Transformer/image-20220812153538482.png" alt="image-20220812153538482"></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>在本文中，我们确定了DETR的数据效率问题。 通过从 Sparse RCNN 到 DETR 的逐步模型转换，我们发现局部区域的稀疏特征采样是数据效率的关键。基于这些，我们通过在预测的bounding box的指导下通过简单地采样多尺度特征在对原始模型的修改最少的前提下来改进现有的检测转换器。 此外，我们提出了一种简单而有效的标签增强策略，以提供更丰富的监督，从而进一步缓解数据效率问题。 大量实验验证了我们方法的有效性。 随着Transformer在视觉任务中越来越流行，我们希望我们的工作能够激发大家探索Transformer在不同任务中的数据效率。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/Deformable%20DETR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/Deformable%20DETR/" class="post-title-link" itemprop="url">Deformable DETR</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 20:56:45 / 修改时间：21:31:44" itemprop="dateCreated datePublished" datetime="2022-08-22T20:56:45+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Deformable-DETR-：-deformable-transformers-for-End-to-End-object-detection"><a href="#Deformable-DETR-：-deformable-transformers-for-End-to-End-object-detection" class="headerlink" title="Deformable DETR ： deformable transformers for End-to-End object detection"></a>Deformable DETR ： deformable transformers for End-to-End object detection</h2><h3 id="DETR的问题"><a href="#DETR的问题" class="headerlink" title="DETR的问题"></a>DETR的问题</h3><p>是针对DETR进行改进的一篇文章，其指出DETR主要存在以下两点问题：</p>
<ol>
<li>DETR需要相较于现有的目标检测器更长的训练epoch来收敛。</li>
<li>DETR在检测小物体时准确率较低。</li>
</ol>
<p>这是由于transformer结构所引入的问题，即是transformer组件处理特征图方面的不足：transformer结构其在初始化时分配给所有特征像素的注意力权重几乎是均等的，这就造成了模型需要长时间去学习关注真正有意义的位置。其次Transformer在计算注意力权重时，伴随着高计算量与空间复杂度。特别是在编码器部分，与特征像素点的数量成平方级关系，因此难以处理高分辨率的特征。</p>
<p>deformable DETR结合deformable conv的空间稀疏采样的优势和transformer元素间建模的能力。通过添加稀疏的空间位置，避免了上述的问题，因此DETR不采用全局的注意力计算，而是只计算reference point周围一小部分点的注意力。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Deformable DETR是一种End-to-End的目标检测器，其在DETR和transformer的基础上做了改进，能够更快收敛，同时减少计算量提高精度。其核心部件为Multi-scale Deformable Attention Module（多尺度可变形注意力模块），其为一种处理图像特征图的有效的注意力机制。</p>
<h3 id="Deformable-Attention-Module"><a href="#Deformable-Attention-Module" class="headerlink" title="Deformable Attention Module"></a>Deformable Attention Module</h3><p>针对于DETR存在的问题，提出Deformable Attention Module ，其不用遍历所有的空间位置，而是与可变形卷积相似，其只注意参考点周围的一小部分关键采样点，而不是特征图的整体。其通过为每个query分配少量固定的键，可以缓解难以收敛和特征空间分辨率所带来的问题。</p>
<p>示意图如下所示：</p>
<p><img src="/2022/08/22/Deformable%20DETR/image-20220810091027439.png" alt="image-20220810091027439"></p>
<p>下面给出MultiheadAttention和DeformableAttention的计算公式</p>
<script type="math/tex; mode=display">
 MultiHeadAttn(z_q , x) =\sum ^M_{m=1}W_m[\sum_{k∈Ω_k}A_{mqk}\cdot W_m^`x_k]
\\
 DeformAttn(z_q , p_q , x) =\sum ^M_{m=1}W_m[\sum^K_{k=1}A_{mqk}\cdot W_m^`x(p_q+∆p_{mqk})]
\\
 MSDeformAttn(z_q , p_q , \{x^l\}^L_{l=1}) =\sum ^M_{m=1}W_m[\sum^L_{l=1}\sum^K_{k=1}A_{mlqk}\cdot W_m^`x^l( \phi_l ( p̂ _q )+∆p_{mlqk})]</script><p>对DeformAttn，输入特征图尺寸为$C<em>H</em>W$，$z_q$为带有内容特征的第q个查询元素，为$p_q$为一个二维参考点，公式中参数如下解释：</p>
<ul>
<li>m 表示注意力头（head）。</li>
<li>k 表示 sampled key（被采样的键）。K 表示 total sampled key number( $K \lt\lt HW$ )。</li>
<li>$ \Delta p_{mqk} $表示第 m 个注意力头中第 k 个采样点的采样偏移量。</li>
<li>$ A_{mqk} $表示第 m 个注意力头中第 k 个采样点的注意力权重 V。</li>
<li><img src="/2022/08/22/Deformable%20DETR/4b4befc3e13742eea81d3b220c04133e.png" alt="img">，Xk表示第k个采样点，Um和Vm是可学习的参数。</li>
<li>标量注意力权重 $ A_{mqk} $的取值范围为[ 0 , 1]，通过$ \sum^K_{k=1}A_{mqk}=1 $进行归一化。</li>
<li><img src="/2022/08/22/Deformable%20DETR/fc909ee8a2414a778ba1f82d7120c36a.png" alt="img">是范围不受限制的2-d实数。</li>
<li>由于$ p_q+\Delta p_{mqk} $是分数阶的，所以在计算$x(p_q+\Delta p_{mqk})$时，采用了《Deformableconvolutional networks (ICCV)》中的双线性插值。</li>
<li>$\Delta p_{mqk}$和$A_{mqk}$都是通过在查询特征$z_q$上的线性投影获得的。</li>
<li>在实现中，查询特征$z_q$被送入3MK通道的线性投影算子，其中前2MK通道编码采样偏移量为 ，其余MK通道被送入softmax算子以获得注意力权重 。</li>
</ul>
<p>可变形注意力模块是为了将卷积特征图作为 key 要素进行处理而设计的。令 $N_q$ 为 query 元素的个数，当 MK 比较小时，可变形注意力模块的复杂度为$O(2N_q C^2 + min(HW C^2 , N_q KC^2 ))$。当它应用于DETR编码器时，其中 $N_q = HW$，复杂度变为$O(HWC^2)$，与空间大小成线性复杂度。当它被用作DETR解码器中的交叉注意力模块时，其中 $N_q = N$ ( N为对象查询次数)，复杂度变为$O (NKC^2)$，这与空间大小HW无关。</p>
<h3 id="Multi-scale-Deformable-Attention-Module"><a href="#Multi-scale-Deformable-Attention-Module" class="headerlink" title="Multi-scale Deformable Attention Module"></a>Multi-scale Deformable Attention Module</h3><p>仿照其余目标检测框架中的多尺度特征，提出Multi-scale Deformable Attention Module，将可变形注意力模块扩展为多尺度，其公式依旧如下所示：</p>
<script type="math/tex; mode=display">
MultiHeadAttn(z_q , x) =\sum ^M_{m=1}W_m[\sum_{k∈Ω_k}A_{mqk}\cdot W_m^`x_k]
\\
DeformAttn(z_q , p_q , x) =\sum ^M_{m=1}W_m[\sum^K_{k=1}A_{mqk}\cdot W_m^`x(p_q+∆p_{mqk})]
\\
MSDeformAttn(z_q , p̂ _q , \{x^l\}^L_{l=1}) =\sum ^M_{m=1}W_m[\sum^L_{l=1}\sum^K_{k=1}A_{mlqk}\cdot W_m^`x^l( \phi_l ( p̂ _q )+∆p_{mlqk})]</script><p>对MSDeformAttn，$\{x^l\}^L_{l=1}$为输入的多尺度特征图，其每层的输入特征图尺寸为$C<em>H_l</em>W_l$，$z_q$为带有内容特征的第q个查询元素，$p̂ _q ∈ [0, 1]^2$二维参考点，公式中参数如下解释：</p>
<ul>
<li>m 表示注意力头（head）。</li>
<li>k 表示 sampled key（被采样的键）。K 表示 total sampled key number( K &lt;&lt; HW )。</li>
<li>$\Delta p_{mlqk}$表示第 L 个特征层和第 m 个注意力头中第 k 个采样点的采样偏移量。</li>
<li>$A_{mlqk}$表示第 L 个特征层和第 m 个注意力头中第 k 个采样点的注意力权重 V。</li>
<li><img src="/2022/08/22/Deformable%20DETR/4b4befc3e13742eea81d3b220c04133e.png" alt="img">，Xk表示第k个采样点，Um和Vm是可学习的参数。</li>
<li>标量注意力权重 $A_{mlqk}$的取值范围为[ 0 , 1]，通过$\sum^L_{l=1}\sum^K_{k=1}A_{mlqk}=1$进行归一化。</li>
<li>$p̂ _q ∈ [0, 1]^2$是归一化坐标，我们用其清晰的表示尺度公式，其中$(0,0)$表示左上角的点,$(1,1)$表示右下角的点</li>
<li>$\phi_l ( p̂ _q )$将归一化的坐标$p̂ _q $重新缩放至输入特征图的第$l$层上。</li>
<li>多尺度可变形注意力与以前的单尺度版本非常相似，只是它从多尺度特征图中采样LK 点，而不是从单尺度特征图中采样 K 点。</li>
</ul>
<p>当选$L=1,K=1，且W_m^`∈R^{C_v\times C}$固定为单位矩阵的时候，上述公式退化为deformable convolution。</p>
<h3 id="Deformable-Transformer-Encoder"><a href="#Deformable-Transformer-Encoder" class="headerlink" title="Deformable Transformer Encoder"></a>Deformable Transformer Encoder</h3><p>我们将DETR中处理特征图的Transformer注意力模块替换为提出的多尺度可变形注意力模块。编码器的输入和输出都是具有相同分辨率的多尺度特征图。</p>
<p>在编码器中，通过ResNet (transformed by a 1 × 1 convolution) 中的从 $C_3$阶段到$C_5$阶段的输出特征图中提取多尺度特征图$\{x^l\}^{L-1}_{l=1}(L = 4)$，其中$C_l$分辨率是输入图的$\frac{1}{2^l}$ 。 在最后的$C_5$级上通过 3 × 3 步长为 2 的卷积得到的最低分辨率特征图$x^L$，记为$C_6$。所有多尺度特征图的通道数为 C = 256 。注意：FPN 中自顶向下的结构没有被使用，因为我们提出的多尺度可变形注意力机制本身可以在多尺度特征图之间交换信息。多尺度特征图的构造如下图所示。注：添加FPN不会提高性能，因为本文所设计的结构能在不同层级之间交换信息，和FPN的功能相同</p>
<p><img src="/2022/08/22/Deformable%20DETR/image-20220810101839490.png" alt="image-20220810101839490"></p>
<p>在Deformable Transformer Encoder的应用中，输出是与输入具有相同分辨率的多尺度特征图。key和query均为多尺度特征图中的像素。对于每个查询像素，参考点为其本身。为了识别每个查询像素位于哪个特征级别（即属于目标物体的概率），除了位置嵌入外，我们在特征表示中添加了一个尺度级别的嵌入，记为$e_l$。不同于固定编码的位置嵌入，尺度级嵌入$\{e_l\}^L_{l=1}$随机初始化并与网络联合训练。</p>
<h3 id="Deformable-Transformer-Decoder"><a href="#Deformable-Transformer-Decoder" class="headerlink" title="Deformable Transformer Decoder"></a>Deformable Transformer Decoder</h3><p>解码器中存在交叉注意力和自注意力模块，两种类型的注意力模块的query elements都是object query。</p>
<p>在交叉注意力模块中，object query从特征图中提取特征，其中的key元素是编码器输出的特征图。</p>
<p>在自注意力模块中，object query是相互作用的，其中的key元素是object query。由于我们提出的可变形注意力模块是为了处理卷积特征图作为key元素而设计的，因此我们只将每个交叉注意力模块替换为多尺度可变形注意力模块，而自注意力模块保持不变。对于每个object query，参考点的二维归一化坐标$p̂ _q$ 通过可学习的线性投影和sigmoid函数从其对象查询嵌入中预测。</p>
<p>由于multi-scale deformable attention module提取参考点周围的图像特征，我们让检测头预测边界框作为参考点的相对偏移量，以进一步降低优化难度。将参考点作为箱体中心的初始猜测。检测头预测参考点的相对偏移量。这样，学习到的解码器注意力将与预测的边界框具有较强的相关性，这也加速了训练收敛。</p>
<p>通过将DETR中的Transformer注意力模块替换为可变形注意力模块，我们建立了一个高效、快速收敛的检测系统，称为可变形DETR 。</p>
<h3 id="多种计算方式之间关系"><a href="#多种计算方式之间关系" class="headerlink" title="多种计算方式之间关系"></a>多种计算方式之间关系</h3><p><img src="/2022/08/22/Deformable%20DETR/1858467-20220401094649223-612897571.png" alt="img"></p>
<h3 id="deformable-DETR结构示意图"><a href="#deformable-DETR结构示意图" class="headerlink" title="deformable DETR结构示意图"></a>deformable DETR结构示意图</h3><p><img src="/2022/08/22/Deformable%20DETR/1858467-20220401094708641-447017032.png" alt="img"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/VIT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/VIT/" class="post-title-link" itemprop="url">VIT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 20:56:31 / 修改时间：21:35:42" itemprop="dateCreated datePublished" datetime="2022-08-22T20:56:31+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="VIT-An-image-is-worth-16-x-16-words-Transformer-for-image-recognition-at-scale"><a href="#VIT-An-image-is-worth-16-x-16-words-Transformer-for-image-recognition-at-scale" class="headerlink" title="(VIT) An image is worth 16 x 16 words: Transformer for image recognition at scale"></a>(VIT) An image is worth 16 x 16 words: Transformer for image recognition at scale</h2><h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p>ViT是2020年Google团队提出的将Transformer应用在图像分类的模型，因其模型“简单”且效果好，可扩展性强，在数据量越大的前提下效果越好，从而成为了transformer在CV领域应用的里程碑著作。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置（即先验知识，如：卷及神经网络所默认的平移不变性等）的限制，可以在下游任务中获得较好的迁移效果。但是当训练数据集不够大的时候，ViT的表现通常比同等大小的ResNets要差一些。这是因为CNN具有两种归纳偏置，一种是局部性（locality/two-dimensional neighborhood structure），即图片上相邻的区域具有相似的特征；一种是平移不变形（translation equivariance）（即$f(g(x))=g(f(x))$),其中g代表卷积操作，f代表平移操作。当CNN具有以上两种归纳偏置，就有了很多先验信息，需要相对少的数据就可以学习一个比较好的模型。</p>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>ViT的整体结构如下图所示：</p>
<p><img src="/2022/08/22/VIT/v2-5afd38bd10b279f3a572b13cda399233_720w.jpg" alt="img"></p>
<p>整个ViT的运行流程如下所示：</p>
<ul>
<li><p>假设输入图片大小为224x224,每个patch对应的像素为16x16，则对于每张图片而言，其生成的patch数量为$（224/16）×（224/16）=196$，即生成的patch序列长度为196，每个patch的大小为$16<em>16</em>3$，每个patch的元素总量为$768$。</p>
</li>
<li><p>对于ViT而言，其整体的结构和流程都是模仿transformer和bert的所以其分为以下几个部分：</p>
<ul>
<li>Patch Embeding：上述所生成的每个patch通过投影层，投影成固定长度的向量，作为encoder的第一部分输入。其固定长度的向量的长度定义为768，所以输入的patch序列的维度为$196<em>768$，Patch Embedding的维度为$768</em>768$，最终得到的Patch Embedding的向量长度为$196*768$。</li>
<li>Position Embedding：由于将图片分为多个patch之后，每个patch经过投影的过程中不引入位置编码信息，所以仿照bert引入position enbedding部分。其位置编码可以理解为是一个有N行（输入patch序列的长度），每行有768（embedding的维度）个元素的矩阵，其第i行就代表了第i个位置所对应的Position Embedding的值。将Patch Embedding与Position Embedding的值相加，由于维度都是$196*768$，所以加之后的维度相同</li>
<li>$[cls]$:仿照bert中的$[cls]$，在196x768的基础上加一维，变成197x768，由于其具体的计算过程中是元素和元素之间两两计算，所以作者认为这样可以在计算过程中学到如何从其他元素上学到我们所需要的信息。并最终在经过Encoder部分的计算之后，取对应位置的输出进行分类。</li>
</ul>
</li>
<li>Encoder：encoder部分由$Add/Norm+多头自注意力机制+Add/Norm+MLP$组成，其过程与transformer中的一致。其输入维度为$(196+1)*768$，经过Encoder之后输入维度与输入相同。所以支持多个Encoder块进行叠加。</li>
<li>MLP Head：在MLP时，输入为197x768，并经过与bert相似的操作，将维度放大四倍再收缩回去，即变为$197<em>（768</em>4）$再缩小变为$197*768$。</li>
<li>最终选取MLP的第0个位置处的元素（即为[cls]对应的位置处的元素）进行图片分类</li>
</ul>
<p>最终的计算步骤如下图所示：</p>
<p><img src="/2022/08/22/VIT/v2-ebf697b1994598019a6a59855dc0dbed_720w.png" alt="img"></p>
<h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><h4 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h4><p>作者研究对比了多种位置编码方式：</p>
<ul>
<li>1-D的位置编码</li>
<li>2-D的位置编码</li>
<li>相对位置编码</li>
</ul>
<p>作者实验结论为：不管使用哪种位置编码方式，模型的精度都很接近，甚至不适用位置编码，模型的性能损失也没有特别大。原因可能是ViT是作用在image patch上的，而不是image pixel，对网络来说这些patch之间的相对位置信息很容易理解，所以使用什么方式的位置编码影像都不大。</p>
<p><img src="/2022/08/22/VIT/v2-99f02198921e7aed8162cd7af8a29805_720w.jpg" alt="img"></p>
<h4 id="image-presentation"><a href="#image-presentation" class="headerlink" title="image presentation"></a>image presentation</h4><p>关于使用[cls]进行学习和直接对输出的结果通过average pooling进行学习的方法，通过实验表明两者区别不大。</p>
<p><img src="/2022/08/22/VIT/v2-4a8b39b1d2dd43d1e9b16edbc38b1660_720w.jpg" alt="img"></p>
<p>文章主要为了和bert类似，所以引入[cls]进行学习</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/DETR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/DETR/" class="post-title-link" itemprop="url">DETR</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 20:56:17 / 修改时间：21:34:04" itemprop="dateCreated datePublished" datetime="2022-08-22T20:56:17+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="DETR-End-to-End-Object-Detection-with-Transformers"><a href="#DETR-End-to-End-Object-Detection-with-Transformers" class="headerlink" title="DETR: End-to-End Object Detection with Transformers"></a>DETR: End-to-End Object Detection with Transformers</h2><p>目标检测的目标是预测bounding boxes的集合和每个感兴趣物体的类别，之前的方法大都是采用间接的方法进行解决的，例如利用anchor，提出大量的region proposals或者window centers等，将问题视为回归或分类的问题。这些间接的方法都采用了很多的先验知识，并且这些先验的选取会严重的影响检测的效果。DETR将目标检测的问题视为集合预测的问题，真正建立了一个end-to-end的检测网络，并移除了许多需要手动设计的组件，例如NMS。</p>
<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p><img src="/2022/08/22/DETR/image-20220808085358261.png" alt="image-20220808085358261"></p>
<p>DETR的整体结构如上所示，其由backbone、transformer encoder、transformer decoder、预测前馈网络(FFNs)、辅助解码损失构成。</p>
<p>其中基于CNN的backbone负责对输入图片进行特征的提取，encoder-decoder负责对backbone传入的图片特征和位置编码进行全局范围的特征的学习和注意力学习，然后最后使用FFNs进行最终目标的预测。</p>
<h4 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a>backbone</h4><p>假定输入图像尺寸为$3 <em>H</em>W $，通过backbone进行特征的提取，从而生成一个尺寸为$C<em>H_0</em>W_0$的feature map。这个feature map的维度为C，在原文中使用的值是2048。每个feature map的尺寸大小为$H<em>W$，在原文中使用的尺寸为$\frac{H_0 }{32}</em>\frac{W_0 }{32}$。</p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><h4 id="Encoder的输入"><a href="#Encoder的输入" class="headerlink" title="Encoder的输入"></a>Encoder的输入</h4><p>​    另外，由于transformer的过程中需要对输入元素进行注意力机制的计算，其计算复杂度为$O(n^2)$，所以backbone最后得出的特征需要首先通过$1<em>1$的卷积层进行降维。如原始的feature map的尺寸为$C</em>H<em>W$，经过$1</em>1$的卷积之后得到的大小为$D<em>H</em>W$。又由于transformer需要的是序列信息的输入，所以将三维的$D<em>H</em>W$进行压缩，压缩为$D*(HW)$。这样传入的序列，每个序列的长度为$HW$，共有$D$个序列。在原文中$D=256$</p>
<p>​    由于在transformer的注意力机制的计算过程中，是对位置不敏感的，所以需要加入positional encoding代表其的空间信息。在原文的positional encoding的过程中，源码为：</p>
<p><img src="/2022/08/22/DETR/image-20220808103125644.png" alt="image-20220808103125644"></p>
<p><img src="/2022/08/22/DETR/image-20220808103146058.png" alt="image-20220808103146058"></p>
<p>可见其为随机初始化的，然后将其进行复制从而扩展。最终的尺度也为$D*(HW)$</p>
<h4 id="Encoder本身"><a href="#Encoder本身" class="headerlink" title="Encoder本身"></a>Encoder本身</h4><p>​    Encoder本身的结构的定义与经典transformer的相似，如下所示：</p>
<p><img src="/2022/08/22/DETR/7.png" alt="DETR Transformer"></p>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>Decoder本身的架构也与transformer中的经典架构相似。其输入一部分为encoder部分的输出，尺度为$D<em>(HW)$，另一部分的输入为可学习的object query，其尺度为$N</em>D$，其中N为decoder最终输出的，一个远大于图像中可能存在的物体的数量，在原文中采用的是$N=100$。object query是一个随机初始化的向量，对训练之后得到的object query进行可视化，得到的结果如下所示：</p>
<p><img src="/2022/08/22/DETR/10.png" alt="在这里插入图片描述"></p>
<p>可得其是学到了从图片的哪个位置进行目标的搜寻的。</p>
<p>target被初始化为0，其尺度为$D*N$。</p>
<ul>
<li>在self-attention部分，QKV均与target相关，$Q,K=target+query$，$V=target$</li>
<li>在cross-attention部分，Q由self-attention的输出结合位置编码query得到，K由encoder部分的输出结合位置编码P E得到，即$K=memory+PE$，V不使用位置编码，即$V=memory$</li>
</ul>
<p>Decoder的输出尺度为$D*N$。</p>
<h4 id="预测前馈网络-FFNs"><a href="#预测前馈网络-FFNs" class="headerlink" title="预测前馈网络(FFNs)"></a>预测前馈网络(FFNs)</h4><p>其本质上就是一个三层的前馈网络，用于进行类别的判断和bounding box的回归。</p>
<ul>
<li>用于为目标分类的逻辑回归层(线性映射+softmax)，定义为class_head=nn.Linear(d,num_classes+1)，这个加的1指的是<code>no object</code></li>
<li>用于检测框回归的MLP，定义为<code>box_head=MLP(input_dim=d, hidden_dim=d,output_dim=4, num_layers=3)</code></li>
</ul>
<h4 id="辅助解码损失"><a href="#辅助解码损失" class="headerlink" title="辅助解码损失"></a>辅助解码损失</h4><p>在训练过程中，我们发现在解码器中使用辅助损耗[1]是很有帮助的，特别是有助于模型输出每个类的对象正确数量。每个解码器层的输出用共享层范数进行归一化，然后送到共享预测头(分类和盒预测)（The output of each decoder layer is normalized with a shared layer-norm then fed to the shared prediction heads (classification and box prediction).）。然后，我们像往常一样将匈牙利损失用于监督。</p>
<h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><p>本文的创新点主要有如下亮点</p>
<ul>
<li>DETR将目标检测问题视为集合预测的问题，是NMS-free的，并完成了one-to-one label assignment</li>
<li>DETR利用transformer将图像表示为固定大小的预测集合</li>
<li>DETR使用基于集合的全局损失来强制进行独一无二的预测</li>
</ul>
<h3 id="损失计算"><a href="#损失计算" class="headerlink" title="损失计算"></a>损失计算</h3><p>在一次计算的过程中，DETR解码器会生成一个固定大小为N的预测集合（原文中设置N=100），N被设置为明确大于图像中物体数量的数值。</p>
<ul>
<li>我们假设$y$表示真实目标的集合，用$\hat{y}=\{\hat{y}\}^{N}_{i=1}$表示N个预测集合的结果。</li>
<li>对真实目标的标签集合进行padding，加入no object，使得预测集合的数量和真实目标经过padding后的数量一致，为N。</li>
<li><p>为了在两个集合之间找到一个二部图匹配，我们寻找一个使得N个元素的开销$σ̂ $最小的排列。</p>
<ul>
<li>$σ̂ = arg min_{σ∈S_N}\sum^N_i L_{match} (y_i , ŷ_{σ(i)} )$</li>
<li>其中 $L_{match} (y_i , ŷ_{σ(i)} )$是在真实目标集合和预测集合在索引为$σ(i)$的情况下的成对匹配成本，前人的工作证明匈牙利算法对这个问题能有效计算</li>
<li>匹配的损失同时考虑了类别以及预测框和真实框的相似程度，每个真实目标元素$i$可以被看作为$y_i=(c_i,b_i)$，其中$c_i$是目标的类别标签（可能为$no\ object$），$b_i∈ [0, 1]^4$是一个定义了真实边框中心点的坐标和其到边缘框的宽和高的图像尺寸的参数。我们定义预测集合的编号为$σ(i)$的样本的类别$c_i$的可能性为$p̂ _{σ(i)} (c_i )$，定义预测框为$b̂ _{σ(i)}$。并由此我们定义$L_{match} (y_i , ŷ_{σ(i)} )$为:</li>
<li><script type="math/tex; mode=display">
L_{match} (y_i , ŷ_{σ(i)} )=−1_{c_i \neq ∅} p̂ _{σ(i)} (c_i ) + 1_{c_i  \neq ∅} L_{box}(b_i , b̂ _{σ(i)} )</script></li>
<li>这种匹配方式与基于anchor和基于region proposal的作用相同，主要区别在于本文的方式需要找到一对一的匹配来直接进行集合预测而没有重复</li>
</ul>
</li>
<li><p>对于上面步骤所表述的对所有匹配对的匈牙利损失如下所示，其与常规物体检测的损失定义类似：</p>
<ul>
<li><script type="math/tex; mode=display">
L_{Hungarian}(y, ŷ) =\sum^N_{i=1}[ − log p̂ _{σ̂(i)} (c_i ) + 1_{c_i \neq ∅} L_{box} (b_i , b̂ _{σ̂ }(i))]</script><ul>
<li>其中$σ̂(i)$是在损失计算的第一个公式中计算得出的最优项，并且在损失的计算中，物体与$no\ object$的损失与预测无关</li>
</ul>
</li>
</ul>
</li>
<li><p>对于边缘框损失而言，其与边缘框有关且表示为$L_{box}$。本文直接提出了对目标的预测，为了解决所带来的物体大小所带来的对损失计算的影响，本文使用了$l_1\ loss$和广义的IOU loss的线性组合。即最终的$L_{IOU}(b_i,b̂ _{σ(i)})$公式表达如下：</p>
<ul>
<li><script type="math/tex; mode=display">
L_{IOU}(b_i,b̂ _{σ(i)})=λ_{iou} L_{iou}(b_i , b̂ _{σ(i)} ) + λ_{L1} ||b i − b̂ _{σ(i)} ||_1 \\其中λ_{iou}、 λ_{L1} 为超参数</script></li>
</ul>
</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><p>DETR做到了真正没有非最大抑制（NMS）后处理，而且不需要anchor（锚点生成）等人工的先验知识。</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>训练时间较长，对小目标的检测性能不是很高。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/Bert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/Bert/" class="post-title-link" itemprop="url">Bert</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 20:56:02 / 修改时间：21:33:54" itemprop="dateCreated datePublished" datetime="2022-08-22T20:56:02+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Bert-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#Bert-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="(Bert)Pre-training of Deep Bidirectional Transformers for Language Understanding"></a>(Bert)Pre-training of Deep Bidirectional Transformers for Language Understanding</h2><h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p>bert是一种预训练模型，在训练的过程中主要训练如下所示的两个子任务</p>
<ul>
<li>预测句子中被掩盖的词</li>
<li>判断输入的两个句子是不是上下句</li>
</ul>
<p>然后在该预训练模型后面依据特定任务加上相应的微调即可完成NLP的下游任务，例如翻译、问答等。</p>
<p>bert的架构是基于transformer的，其通过叠加transformer的encoder部分组成bert的整体框架，其encoder部分由一层多头自注意力机制、一层全链接网络、两层残差及标准化层组成，其中一个encoder部分的示意图如下所示：</p>
<p><img src="/2022/08/22/Bert/image-20220802090401760.png" alt="image-20220802090401760"></p>
<p>在论文中提出了两种大小的模型，大的模型有24层，每层16个attention。小的模型12层，每层12个attention。</p>
<p>整个模型由模型输入、网络训练以及对每个具体任务的微调组成。</p>
<h3 id="模型任务"><a href="#模型任务" class="headerlink" title="模型任务"></a>模型任务</h3><p>要了解模型的训练过程，首先需要对模型的训练任务有一定了解。模型的训练任务分为以下两个部分<strong>masked language model</strong>和<strong>next sentence prediction</strong>。</p>
<ul>
<li>masked language model：即随机掩盖掉输入中的部分单词，然后让模型通过上下文来预测该单词。<ul>
<li>具体在bert中，选用了15%的token会被随机掩盖，这15%的token中80%用[MASK]这个token来代替，10%用随机的一个词来替换，10%保持这个词不变。这种设计使得模型具有捕捉上下文关系的能力，同时能够有利于token-level tasks例如序列标注。</li>
<li>不将选中的token全都用[mask]替换的原因是，如果全部替换，会让模型学习到“如果当前词是 [MASK]，就根据其他词的信息推断这个词；如果当前词是一个正常的单词，就直接抄输入”，但是在模型微调的时候，并不会存在[mask]，所以会让模型在微调时失效。</li>
</ul>
</li>
</ul>
<p>示意图如下所示：</p>
<p><img src="/2022/08/22/Bert/1620.jpeg" alt="img"></p>
<ul>
<li>next sentence prediction：给定两句话，判断第二句话是否紧跟在第一句话之后。<ul>
<li>其任务具体为在所有样本中的50%的句子，选择其相应的下一句一起形成上下句，作为正样本；其余50%的句子随机选择一句非下一句一起形成上下句，作为负样本。且作者强调，应使用document-level的而不是sentence-level的样本，从而具备长序列特征的抽象能力。</li>
</ul>
</li>
</ul>
<p>示意图如下所示：</p>
<p><img src="/2022/08/22/Bert/1620-16594041231163.jpeg" alt="img"></p>
<h3 id="模型输入"><a href="#模型输入" class="headerlink" title="模型输入"></a>模型输入</h3><p>对于bert而言，其输入分为三部分：即Token Enbeddings，position embeddings，segment embeddings。其中由bert的设计，我们引入[CLS]作为输入的开始的起始符，引入[SEP]作为两个句子的分割符（next sentence prediction中输入为两句话，故引入[SEP]）。</p>
<ul>
<li>Token Enbeddings：为单词本身的向量表示。在bert中使用的是WordPiece方法，其是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。最终用30000左右的token表达了所有的词。</li>
<li>position embedding：将单词的位置信息编码成特征向量。因为我们的网络结构没有RNN 或者LSTM，因此我们无法得到序列的位置信息，所以需要构建一个position embedding来表示位置信息。<ul>
<li>构建position embedding有两种方法：BERT是初始化一个position embedding，然后通过训练将其学出来；而Transformer是通过制定规则来构建一个position embedding（sin、cos表示）</li>
</ul>
</li>
<li>segment embedding：用于区分两个句子的向量表示。</li>
</ul>
<p>下图为模型输入的示意图：</p>
<p><img src="/2022/08/22/Bert/embedding.png" alt="img"></p>
<p>并最终将上述三种Embedding相加，作为最终的输入。</p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><h4 id="不同结构的bert"><a href="#不同结构的bert" class="headerlink" title="不同结构的bert"></a>不同结构的bert</h4><p>对于bert而言，其主要的结构在于其encoder块的叠加，大的模型有24层，每层16个attention，隐藏层大小为1024，参数总量为340M。小的模型12层，每层12个attention，隐藏层大小为768，参数总量为110M。</p>
<p>以小的模型为例，输入的size由WordPiece可得为30000。</p>
<ul>
<li>嵌入层：嵌入层可学习的参数量为：$30000*12$</li>
<li>对于一个encoder块：<ul>
<li>首先经过多头自注意力的计算，参数量为$(64<em>12)</em>(64<em>12)</em>4$</li>
<li>然后经过全链接层的计算，参数量为$(64<em>12)</em>(64<em>12)</em>8$</li>
<li>总共有12层，所以总共encoder部份的计算总量为$(64<em>12)</em>(64<em>12)</em>(4+8)*12$</li>
</ul>
</li>
</ul>
<p>总共的计算量为:</p>
<p>$30k<em>(12</em>64)+(64<em>12)</em>(64<em>12)</em>(4+8)*12\approx110M$</p>
<p>大模型同理。</p>
<p>其所对应的Encoder块的计算原理如之前写的transformer所示。</p>
<h4 id="BN-LN的选择"><a href="#BN-LN的选择" class="headerlink" title="BN/LN的选择"></a>BN/LN的选择</h4><p>对于BN、LN的选择，可直观的如下图所示：</p>
<p><img src="/2022/08/22/Bert/BNLN.png" alt="img"></p>
<p>可以简单的对其理解为，BN是对一个batch里面所有样本的同一个位置的特征做归一化，然后LN是对一个batch里面的一个样本的所有特征做归一化。</p>
<p>所以LN更加符合对文本处理的直觉。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="模型特点"><a href="#模型特点" class="headerlink" title="模型特点"></a>模型特点</h4><ul>
<li>使用transformer作为算法的主要框架，transformer能<strong>更彻底的捕捉语句中的双向关系</strong></li>
<li>使用了mask language model 和next sentence prediction的多任务训练目标，<strong>是一个自监督的过程，不需要数据的标注</strong></li>
<li>使用tpu这种强大的机器训练了大规模的预料，是NLP的很多任务达到了全新的高度。</li>
</ul>
<p>​    BERT本质上是在海量语料的基础上，通过自监督学习的方法为单词学习一个好的特征表示。该模型的优点是可以根据具体的人物进行微调，或者直接使用预训练的模型作为特征提取器。</p>
<h4 id="可优化空间"><a href="#可优化空间" class="headerlink" title="可优化空间"></a>可优化空间</h4><p>（1）如何让模型有<strong>捕捉Token序列关系</strong>的能力，而不是简单依靠位置嵌入。</p>
<p>（2）模型太大，太耗机器</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/22/transformer%E7%9B%B8%E5%85%B3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/" class="post-title-link" itemprop="url">transformer相关</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-22 20:55:36 / 修改时间：21:34:52" itemprop="dateCreated datePublished" datetime="2022-08-22T20:55:36+08:00">2022-08-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Transformer相关"><a href="#Transformer相关" class="headerlink" title="Transformer相关"></a>Transformer相关</h2><p>要了解transformer，首先需要对其发展有一定的了解，即从RNN开始。</p>
<h3 id="经典RNN（N-vs-N）"><a href="#经典RNN（N-vs-N）" class="headerlink" title="经典RNN（N vs N）"></a>经典RNN（N vs N）</h3><p>个人在搜寻资料过程中，认为以下两个链接讲得很好，就不再赘述，直接贴链接了。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52119092">NLP中的RNN、Seq2Seq与attention注意力机制</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28054589">完全图解RNN、RNN变体、Seq2Seq、Attention机制</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Tink1995/article/details/105012972">Attention详解</a></li>
<li><a href="[https://blog.csdn.net/Tink1995/article/details/105080033?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-1-105080033-blog-104374257.pc_relevant_vip_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-1-105080033-blog-104374257.pc_relevant_vip_default&amp;utm_relevant_index=2](https://blog.csdn.net/Tink1995/article/details/105080033?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-1-105080033-blog-104374257.pc_relevant_vip_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~default-1-105080033-blog-104374257.pc_relevant_vip_default&amp;utm_relevant_index=2">Transformer详解</a>)</li>
</ul>
<p>总而言之，经典RNN是存储并利用了历史信息的网络，其输入和输出必须相同。</p>
<h3 id="seq2seq模型（N-vs-M）"><a href="#seq2seq模型（N-vs-M）" class="headerlink" title="seq2seq模型（N vs M）"></a>seq2seq模型（N vs M）</h3><p>seq2seq模型为RNN的一种变种，其输入输出不定，也叫做Encoder-Decoder模型，但其是不存在注意力机制的。</p>
<p>Encoder和Decoder均可以看做一个独立的有记忆系统的网络（RNN、LSTM等）</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/v2-77e8a977fc3d43bec8b05633dc52ff9f_720w.jpg" alt="img"></p>
<p>如上图所示，输入x1～x4，通过Encoder生成h1～h4。则最终生成的语义编码c依据其具体定义可得为h1～h4的组合，即可表示为$C=q(h1,h2,h3,h4)$，C最终为一个固定长度的语义向量。在Decoder阶段，将C作为输入，Decoder将其解码成所需的序列数据。解码过程如下所示：</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/1.png" alt="在这里插入图片描述"></p>
<p>生成的语义编码C,在计算每一时刻的输出$y_t$的时候均作为独立的输入，即其对应的公式可表示为如下所示：</p>
<script type="math/tex; mode=display">
y_t=f(C,y_1,y_2.....y_{t-1})</script><p>有另一种解码方式是C只在$y_1$的时候作为输入，并不对其余的$y_t$输入。</p>
<p>这两种解码方式均有以下缺点：</p>
<ul>
<li><p>在生成对应的$y_t$的时候，其使用的C是相同的，即无论生成哪个单词，其输入序列中的任意组成部分对目标的影响力是相同的，没有区别</p>
</li>
<li><p>将整个序列的信息压缩在了一个语义编码C中，导致序列长度极长，容易引起梯度消失，信息损失等问题。</p>
</li>
</ul>
<h3 id="Attention-注意力机制"><a href="#Attention-注意力机制" class="headerlink" title="Attention 注意力机制"></a>Attention 注意力机制</h3><p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/v2-9407244671e4bc4fa32da7e66fba25bf_720w.jpg" alt="img"></p>
<p>故引入Attention 注意力机制：’’机器学习’’翻译而得’machine learning’ ，我们显然希望在翻译得到machine的时候，机器的权重较大，得到learning的时候学习的权重较大。对应到上图及为红色的权重大。这样的权重机制便可理解为注意力机制</p>
<p>对应的模型框图如下所示：</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/2.png" alt="在这里插入图片描述"></p>
<p>即不再使用一个单一的语义编码C，而是使用多个$C_1,C_2..C_N$的编码，预测Y的时候，Y的注意力集中在语义编码$C_i$上，则使用对应的$C_i$，从而模拟人的注意力机制。那如何计算对应的$C_1,C_2..C_N$，假设$\alpha_{ij}$表示权值分布，$h_j$表示第j个输入对应的隐藏层输出，则$C_i$公式可如下所示：</p>
<script type="math/tex; mode=display">
C_i=\sum_{j=1}^n\alpha_{ij}h_j</script><p>那问题就转变为了$\alpha_{ij}$的计算</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/3.png" alt="在这里插入图片描述"></p>
<p>$\alpha_{ij}$的计算：decoder上一时刻的输出值$Y_{i-1}$与上一时刻传入的隐藏层的值$S_{i-1}$进行计算生成$H_i$，然后计算$H_i$与$h_1，h_2，h_3…h_m$的相关性，得到相关性评分$[f_1,f_2,f_3…f_m]$，然后对$F_i$进行softmax就得到注意力分配$α_{ij}$。然后将encoder的输出值h与对应的概率分布αij进行点乘求和，就能得到注意力attention值了。</p>
<h4 id="Attention机制的本质思想"><a href="#Attention机制的本质思想" class="headerlink" title="Attention机制的本质思想"></a>Attention机制的本质思想</h4><p>为更深刻的了解上述过程，Attention机制的本质思想可如下所示：</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/4.png" alt="在这里插入图片描述"></p>
<p>即对于source而言，其由Key和Value构成的数据对构成，给定Target中的某个元素query，通过计算query与key的相似度从而得到query和key之间的相似性或者相关性，从而得到对应的权重系数。然后按照权重系数对value进行加权求和。</p>
<p>上述所提到的相似度计算一般有如下三种方式：点积、cosine相似性和MLP网络，对应的计算公式如下所示：</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/20200322200849586.png" alt="在这里插入图片描述"></p>
<h4 id="Attension框图"><a href="#Attension框图" class="headerlink" title="Attension框图"></a>Attension框图</h4><p>Attention过程总体上均可如下图所示：</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/5.png" alt="在这里插入图片描述"></p>
<p>阶段1：Query与每一个Key计算相似性得到相似性评分s<br>阶段2：将s评分进行softmax转换成[0,1]之间的概率分布$\alpha$<br>阶段3：将[$\alpha_1,\alpha_2,\alpha_3….\alpha_n$]作为权值矩阵对Value进行加权求和得到最后的Attention值</p>
<h4 id="Attension的优缺点"><a href="#Attension的优缺点" class="headerlink" title="Attension的优缺点"></a>Attension的优缺点</h4><ul>
<li>优点：<ul>
<li>速度快。Attention机制不再依赖于RNN，解决了RNN不能并行计算的问题。这里需要说明一下，基于Attention机制的seq2seq模型，因为是有监督的训练，所以咱们在训练的时候，在decoder阶段并不是说预测出了一个词，然后再把这个词作为下一个输入，因为有监督训练，咱们已经有了target的数据，所以是可以并行输入的，可以并行计算decoder的每一个输出，但是再做预测的时候，是没有target数据地，这个时候就需要基于上一个时间节点的预测值来当做下一个时间节点decoder的输入。所以节省的是训练的时间。</li>
<li>效果好。效果好主要就是因为注意力机制，能够获取到局部的重要信息，能够抓住重点。</li>
</ul>
</li>
<li>缺点：<ul>
<li>1.只能在Decoder阶段实现并行运算，Encoder部分依旧采用的是RNN，LSTM这些按照顺序编码的模型，Encoder部分还是无法实现并行运算，不够完美。</li>
<li>2.就是因为Encoder部分目前仍旧依赖于RNN，所以对于中长距离之间，两个词相互之间的关系没有办法很好的获取。</li>
</ul>
</li>
</ul>
<h3 id="Self-Attension"><a href="#Self-Attension" class="headerlink" title="Self-Attension"></a>Self-Attension</h3><p>针对于Attension的缺点，提出Self-Attension，其输入sourve与输出Target的内容是相同的，其具体的计算过程与基本原理与Attension是完全相同的，其的Key=Value=Query。其优点为：可以捕获句子中长距离的相互关联的特征，可以通过一个计算步骤直接将其联系起来。且其可以增加计算的并行性，一次性解决了Attension的两个缺点。</p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/6.png" alt="在这里插入图片描述"></p>
<p>Transformer的结构如上所示，主要由四个部分组成：Input、Encoder、Decoder、Output。其中最为重要的为encoder和decoder部份。对于Transformer而言，其的超参数只有两个，一个为N，即Encoder block重复几次，另一个为每一层对应的长度，在transformer中，其将每一层的长度限制为512不变。所以整个Transformer中只有两个超参数。</p>
<h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p>对于Input部分，一般而言其输入都是将文字序列转化为vector，即经过word2vec、one-hot等形式的编码之后得到的向量。由于transformer的方法在整个计算过程中完全是基于self-Attension的，其整个计算过程中是没办法获取词语位置信息的。而词语的位置信息对句子的意思有巨大的影响。为了强调位置在输入之中的重要性，我们需要给每一个词向量添加一个位置编码，即上图中所示的Positional Encoding。</p>
<p>Positional Encoding的常用方式有以下两种：</p>
<ul>
<li>通过数据学习的到positional Encoding ，如google所提出的bert</li>
<li>通过正余弦位置编码等编码方式进行编码，如Attension is all you need中。位置编码通过使用不同频率的正弦、余弦函数生成，然后和对应的位置的词向量相加，位置向量维度必须和词向量的维度一致。过程如上图，PE（positional encoding）计算公式如下：</li>
</ul>
<script type="math/tex; mode=display">
P E (pos,2i) = sin(pos/10000^{2i/d_{model}} )\\
P E (pos,2i+1) = cos(pos/10000^{2i/d_{model}} )</script><p>在上述公式中，pos为绝对位置，$d_{model}$为词向量的维度。</p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><h5 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p>假设经过Input之后输出的Embedding Vector为$X_1,X_2….$，在Attention计算的的时候，需要$X_i$所对应的Query、Keys、Values向量，这些向量由Input$X_i$与三个权值矩阵$W^Q,W^K,W^V$相乘求得，其对应图示如下所示：其中权值矩阵是可以通过学习优化的</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/7.png" alt="在这里插入图片描述"></p>
<p>其中X的每一行代表一个输入，一行的长度代表了Embedding的长度。</p>
<p>依据之前对attention的描述，以及上图的对$X,W,Q,K,V$的描述，我们可以类似的将其的计算过程表示为如下图所示：</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/attention.png" alt="在这里插入图片描述"></p>
<p>其具体计算过程可概括如下：</p>
<ol>
<li>输入序列中每个单词之间的相关性得分，在Transformer中使用的是点积法，就是用Q中每一个向量与K中每一个向量计算点积，具体到矩阵的形式为：$s c o r e = Q ⋅ K^T$ socre是一个(2,2)的矩阵</li>
<li>对于输入序列中每个单词之间的相关性得分进行归一化，归一化的目的主要是为了训练时梯度能够稳定。$score = score/\sqrt{d_k}$ ，dk就是K的维度</li>
<li>通过softmax函数，将每个单词之间的得分向量转换成[0,1]之间的概率分布，同时更加凸显单词之间的关系。经过softmax后，score转换成一个值分布在[0,1]之间的(2,2)α概率分布矩阵</li>
<li>根据每个单词之间的概率分布，然后乘上对应的Values值，α与V进行点积， $Z = softmax(score)\cdot V$，V的为维度是(2,64)，(2,2)x(2,64)最后得到的Z是(2,64)维的矩阵</li>
</ol>
<p>从self-attention到transformer中的multi-head attention，可以对其简单的理解为从：通过Embedding之后生成的vector X通过与多组的不同的权值矩阵$W^Q,W^K,W^V$相乘，求得多组的Query、keys、values。然后依据上述计算过程计算得出多个Z，然后将上述得到的多个Z矩阵进行拼接求得最终的输出矩阵。</p>
<h5 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add  &amp; Norm"></a>Add  &amp; Norm</h5><p>在multihead attention之后的是Add &amp; Norm层，其中Add层采用的是resnet的想法，残差链接。Norm层采用的是Layer Normalization（LN）。一般常采用的还有另一种Normalization方法是Batch Normalization，其的对比如下图：</p>
<p><img src="/2022/08/22/transformer%E7%9B%B8%E5%85%B3/BNLN.png" alt="在这里插入图片描述"></p>
<h5 id="Feed-Forward-Networks"><a href="#Feed-Forward-Networks" class="headerlink" title="Feed-Forward Networks"></a>Feed-Forward Networks</h5><p>在Add&amp;Norm之后的是Feed-Forward Networks，即一个前馈神经网络，在Transformer中直接使用了一个两层的神经网络，激活函数使用的Relu引入非线性因素，并在最终计算之后的结果输入encoder中。其公式大致如下所示</p>
<script type="math/tex; mode=display">
FFN(x)=max(0,x W_1+b_1 )W_2     +b_2</script><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><h5 id="Decoder在不同模式下的输入"><a href="#Decoder在不同模式下的输入" class="headerlink" title="Decoder在不同模式下的输入"></a>Decoder在不同模式下的输入</h5><p>Decoder在训练和预测的情况下，其对应的输入是有所不同的，如之前的transformer模型结构所示，其中的Outputs(shifted right)的输入只有在训练的时候输入。在训练的时候，假设任务为中译英，Inputs为我爱你，在训练的时候，Outputs则应输入I love you，而在预测的时候 ，Outputs初始输入为起始符，然后每次的输入是上一时刻的Transformer的输出。</p>
<h5 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h5><p>与Encoder的Multi-Head Attention计算原理一样，只是多加了一个mask码。mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。</p>
<ol>
<li><p>padding mask</p>
<p>padding mask 实际上在encoder和decoder两个模块中都存在，padding mask主要处理的问题是输入序列长度不一致的问题。所以我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。<br>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！</p>
</li>
<li><p>sequence mask</p>
<p>sequence mask 只存在于decoder的第一个Masked Multi-Head Attention 中。这样做是为了使得 decoder 不能看见未来的信息。也就是对于一个序列中的第i个token解码的时候只能够依靠i时刻之前(包括i)的的输出，而不能依赖于i时刻之后的输出。因此我们要采取一个遮盖的方法(Mask)使得其在计算self-attention的时候只用i个时刻之前的token进行计算。<br>那么具体的做法为：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p>
</li>
</ol>
<h5 id="Add＆Normalize"><a href="#Add＆Normalize" class="headerlink" title="Add＆Normalize"></a>Add＆Normalize</h5><p>Add＆Normalize与Encoder中一样</p>
<h5 id="Multi-Head-Attention-1"><a href="#Multi-Head-Attention-1" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h5><p>这是decoder中的第二个Multi-Head Attention。这个Multi-Head Attention相对于encoder中的Multi-Head Attention而言，其不是Self Attention的。在decoder中，它的输入Query来自于Masked Multi-Head Attention的输出，Keys和Values来自于Encoder中最后一层的输出。</p>
<p>对于decoder中的两个Multi-Head Attention而言：</p>
<ul>
<li>第一个Masked Multi-Head Attention是为了得到之前已经预测输出的信息，相当于记录当前时刻的输入之间的信息的意思。</li>
<li>第二个Multi-Head Attention是为了通过当前输入的信息得到下一时刻的信息，也就是输出的信息，是为了表示当前的输入与经过encoder提取过的特征向量之间的关系来预测输出。</li>
</ul>
<p>经过了第二个Multi-Head Attention之后的Feed Forward Network与Encoder中一样，然后就是输出进入下一个decoder，如此经过6层decoder之后到达最后的输出层。</p>
<h4 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h4><p>最终我们所得到的Decoder的输出为vector，我们将其通过Linear进行线性变换，然后经过SoftMax得到对应的概率分布，然后将其通过词典对应从而输出概率最大的对象作为我们的预测输出。</p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>效果好且可以并行训练，速度快</li>
<li>其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩</li>
<li>Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。</li>
<li>Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。</li>
</ol>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ol>
<li>完全基于self-attention，对于词语位置之间的信息有一定的丢失，虽然加入了positional encoding来解决这个问题，但也还存在着可以优化的地方。</li>
<li>粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/" class="post-title-link" itemprop="url">目标检测经典论文阅读</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-26 22:26:51" itemprop="dateCreated datePublished" datetime="2022-07-26T22:26:51+08:00">2022-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-22 21:34:23" itemprop="dateModified" datetime="2022-08-22T21:34:23+08:00">2022-08-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="SSD-Single-Shot-MultiBox-Detector"><a href="#SSD-Single-Shot-MultiBox-Detector" class="headerlink" title="(SSD) Single Shot MultiBox Detector"></a>(SSD) Single Shot MultiBox Detector</h2><h3 id="SSD主要有以下几个主要特点"><a href="#SSD主要有以下几个主要特点" class="headerlink" title="SSD主要有以下几个主要特点"></a>SSD主要有以下几个主要特点</h3><ul>
<li>特征提取主干网络：VGG16，去除全连接层fc8，fc6 和 fc7层转换为卷积层，pool5不进行分辨率减小，在fc6上使用dilated convolution弥补损失的感受野；并且增加了一些分辨率递减的卷积层；</li>
<li>SSD摈弃了proposal的生成阶段，使用anchor机制，这里的anchor就是位置和大小固定的box，可以理解成事先设置好的固定的proposal</li>
<li>SSD使用不同深度的卷积层预测不同大小的目标，对于小目标使用分辨率较大的较低层，即在低层特征图上设置较小的anchor，高层的特征图上设置较大anchor</li>
<li>预测模块：使用3x3的卷积对每个anchor的类别和位置直接进行回归</li>
<li>SSD使用的data augmentation对效果影响很大</li>
</ul>
<p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/v2-6c94c262502872e9d667cd03959d1f2e_720w.jpg" alt="img"></p>
<h3 id="SSD训练时的流程："><a href="#SSD训练时的流程：" class="headerlink" title="SSD训练时的流程："></a>SSD训练时的流程：</h3><ul>
<li>生成初始框<ul>
<li>SSD的初始框不是自适应的，是根据预先设置好的anchor生成规则进行生成的，其无需利用输入图像的信息进行生成，这也导致有一部分的先验信息没有被利用到。</li>
</ul>
</li>
<li>先验框匹配<ul>
<li>即label assignment ，即为判断训练图片中的目标真实框与哪个先验框来进行匹配，从而使匹配的框进行位置的预测。SSD中使用的先验框匹配的方法是IOU。</li>
</ul>
</li>
<li>损失计算<ul>
<li>$L(x,c,l,g) = \frac{1}{N}(L_{conf}(x,c)+\alpha{L_{loc}(x,l,g)}$ 其中$N$为正样本的个数</li>
<li>位置损失计算：$L_{loc}(x,l,g) = \sum^{N}_{i\in{Pos}}\sum_{m\in{cx,cy,w,h}}{x^k_{ij}smooth_{L_1}(l^m_i - g^m_j)}$</li>
<li>类别损失计算：$L_{conf}(x,c) = - \sum^N_{i\in{Pos}}{x^p_{ij}log(C^p_i)}- \sum_{i\in{Neg}}{log(C^o_{i})} \quad where \quad C^p_{i}=\frac{exp(c^p_{i})}{\sum_{p}{exp(c^p_i)}}$</li>
<li>$\alpha$设置为1</li>
</ul>
</li>
</ul>
<h2 id="（FPN）Feature-Pyramid-Networks-for-Object-Detection"><a href="#（FPN）Feature-Pyramid-Networks-for-Object-Detection" class="headerlink" title="（FPN）Feature Pyramid Networks for Object Detection"></a>（FPN）Feature Pyramid Networks for Object Detection</h2><h3 id="FPN解决的问题："><a href="#FPN解决的问题：" class="headerlink" title="FPN解决的问题："></a>FPN解决的问题：</h3><p>FPN是目前较为先进的一种目标检测中的neck结构（即图片经过特征提取网络backbone进行图片特征提取之后以及检测头head进行物体检测之间的承上启下的部分），其主要的作用是对提取到的特征进行再加工和合理利用。</p>
<h3 id="FPN的结构以及相对于以前结构的改进："><a href="#FPN的结构以及相对于以前结构的改进：" class="headerlink" title="FPN的结构以及相对于以前结构的改进："></a>FPN的结构以及相对于以前结构的改进：</h3><ul>
<li>(a)图像金字塔，即将图像做成不同的scale，然后不同scale的图像生成对应的不同scale的特征。<ul>
<li>优点：每一种尺度的图像进行特征提取，能够产生多尺度的特征表示，并且所有等级的特征图都具有较强的语义信息，甚至包括一些高分辨率的特征图。</li>
<li>缺点：<ul>
<li>增加了时间成本</li>
<li>内存需求巨大，从而导致用图像金字塔的形式训练一个端到端的深度神经网络变得不可行</li>
<li>如果只在测试阶段使用图像金字塔，那么会由于训练时网络只是针对于某一个特点的分辨率进行训练，推理时产生“矛盾”。</li>
</ul>
</li>
</ul>
</li>
<li>(b)像SPP net，Fast RCNN，Faster RCNN是采用这种方式，即仅采用网络最后一层的特征，==特征利用不充分，没有利用到多尺度的特征==。</li>
<li>(c)像<strong>SSD（Single Shot Detector）</strong>采用这种多尺度特征融合的方式，没有上采样过程，即从网络不同层抽取不同尺度的特征做预测，这种方式不会增加额外的计算量。作者认为==SSD算法中没有用到足够低层的特征，而在作者看来足够低层的特征对于检测小物体是很有帮助的==。</li>
<li>(d)FPN这种网络结构，能够在增加较少计算量的前提下融合低分辨率语义信息较强的特征图和高分辨率语义信息较弱但空间信息丰富的特征图。本文作者是采用这种方式，顶层特征通过上采样和低层特征做融合，而且每层都是独立预测的。后续例如YOLOv3 4都采用了类似的结构。</li>
</ul>
<p>即如下图所示：</p>
<p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/89b55dd2738f4f4c99fd315cd59304b3.png" alt="在这里插入图片描述"></p>
<h3 id="FPN的主要组成部分及解析："><a href="#FPN的主要组成部分及解析：" class="headerlink" title="FPN的主要组成部分及解析："></a>FPN的主要组成部分及解析：</h3><p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/d12943d3e5da404b9fd07dad4617fb09.png" alt="在这里插入图片描述"></p>
<p>FPN的主要由三部分组成：<strong>自底向上</strong>，<strong>自顶向下</strong>，<strong>横向连接</strong></p>
<h4 id="自底向上"><a href="#自底向上" class="headerlink" title="自底向上"></a>自底向上</h4><p>自底向上的网络是前馈网络，即网络的backbone生成了feature map之后，通过step=2进行每一级的降采样。</p>
<h4 id="自顶向下"><a href="#自顶向下" class="headerlink" title="自顶向下"></a>自顶向下</h4><p>自顶向下的过程是通过上采样的方式进行实现的，实现的方法为最近邻插值法，示意图如下：</p>
<p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/b3b55c106933490ea832bfcbe4f9b6ee.png" alt="在这里插入图片描述"></p>
<h4 id="横向连接"><a href="#横向连接" class="headerlink" title="横向连接"></a>横向连接</h4><p>横向连接的实现方式如figure 3所示，为：首先自底向上的过程中生成的feature map经过1x1的卷积层改变特征图的通道数然后与自顶向下过程生成的feature map进行直接元素与元素的相加。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>FPN（Feature Pyramid Network）算法同时利用低层特征高分辨率和高层特征的高语义信息，通过融合这些不同层的特征达到预测的效果。并且预测是在每个融合后的特征层上单独进行的，这和常规的特征融合方式不同。</p>
<h2 id="Retina-Net-Focal-Loss-for-Dense-Object-Detection"><a href="#Retina-Net-Focal-Loss-for-Dense-Object-Detection" class="headerlink" title="(Retina Net)Focal Loss for Dense Object Detection"></a>(Retina Net)Focal Loss for Dense Object Detection</h2><p>RetinaNet 本篇论文的主要贡献来自于其所提出的Focal Loss和参数初始化的设置，其所提出的Retina Net相对来说贡献并没有那么关键。</p>
<h3 id="稠密-dense-预测与稀疏-sparse-预测"><a href="#稠密-dense-预测与稀疏-sparse-预测" class="headerlink" title="稠密(dense)预测与稀疏(sparse)预测"></a>稠密(dense)预测与稀疏(sparse)预测</h3><p>在目前的目标检测的算法框架中，一般把所有的目标检测分为两个类别，two-stage和one-stage。</p>
<p>其中，RCNN类的经典目标识别算法是属于two-stage的。其主要的识别流程可以看做：首先传入一张图片，依据算法中所提出的目标框算法，如faster-RCNN的RPN算法，例如RCNN的selective search算法在目标图片中预测出大量的bbox（100k的数量级），然后依据其算法从其中筛选出评价较好的目标框（1～2k的数量级），将其送入检测头中进行检测。在这个过程中，RPN和SS提取出大量的bbox的过程就是稠密预测，然后从其中提取出得分比较高的bbox的过程就是稀疏预测。</p>
<p>在one-stage的经典算法中，以yolov3为例，其生成anchor的过程可以大致描述为在backbone生成feature map之后，以三个层级的feature map内的每个点为中心进行anchor的预测，最后生成的anchor的数量级大致为30～100k左右。</p>
<h3 id="one-stage和two-stage效果差异分析"><a href="#one-stage和two-stage效果差异分析" class="headerlink" title="one-stage和two-stage效果差异分析"></a>one-stage和two-stage效果差异分析</h3><p>作者想分析one-stage的密集预测精度没有two-stage的高的原因，发现极度不平衡的前背景数量是导致精度下降的原因。因此作者想通过修改标准交叉熵损失函数去改善这种不平衡关系。Focal Loss用来通过一组难训练的稀疏预测框进行训练，防止简单大量的负样本对训练造成影响。</p>
<p>训练效率低：大多数位置都是非常简单的负样本，对训练没有大的作用。<br>大量简单负样本甚至会损坏模型。(degenerate models)Focal Loss</p>
<p>作者研究中发现，正负样本极度不均衡的问题会导致以下两个问题：</p>
<ol>
<li>训练效率低：大多数位置都是非常简单的负样本，对训练没有大的作用。</li>
<li>大量简单负样本甚至会损坏模型。(degenerate models)</li>
</ol>
<h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p>Focal Loss 是用于解决在训练过程中前景和背景之间极度不平衡的问题。</p>
<p>Focal Loss是基于Cross Entropy，在Cross Entropy的基础上改进而来的。Cross Entropy的公式如下所示：</p>
<script type="math/tex; mode=display">
CE(p,y)=\left\{
                            \begin{aligned}
                                        -log(p)&&if\ y=1\\  
                                         -log(1-p) && otherwise
                             \end{aligned}
\right.</script><p>其中$y=1$ 表示是前景，p为模型预测为前景的概率，对其进行简化可得：</p>
<script type="math/tex; mode=display">
p_t=\left\{
                            \begin{aligned}
                                        p&&if\ y=1\\  
                                        1-p&& otherwise
                             \end{aligned}
\right.</script><p>所以我们可得：$CE(p, y) = CE(p t ) = − log(p t )$</p>
<p>一种普遍的解决分类不平衡的方法为引入权重因子$\alpha$,让目标为前景时加$\alpha$,当目标是背景时加$1-\alpha$，可得引入权重因子之后的CE Loss可以写作：$CE(p_t ) = −α_t log(p_t )$。</p>
<p>最终作者提出的Focal Loss的公式是在CE的基础上增加了一个调节因子$(1-p_t)^{\gamma}$,其中$\gamma$被称作可调节聚焦因子，Focal Loss公式如下所示：$FL(p_t ) = −(1 − p_t )^γ log(p_t )$，在不同的$\gamma$值的情况下，对应的Loss值与Ground Truth值如下图所示，其中当$\gamma=0$时，对应的是标准的CE曲线。</p>
<p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/v2-6ec9b40f6d0d936735c1c5b000a11e6f_1440w.jpg" alt="CV论文精读系列之目标检测模型（六）RetinaNet (Focal Loss)"></p>
<p>对上图进行分析可得，当正负样本极不平衡的时候，对于标准的CE而言，即使是预测为0.9的样本，其对应的loss也较高，数量积累之后对应的loss会占主导作用。为了减少这个情况，取$\gamma$值进行调节，当值越大的，容易样本就越不重要。实际使用过程中，引入权重因子$\alpha$，最终式为$FL(p_t ) = −α_t (1 − p_t )^γ log(p_t )$</p>
<h2 id="EfficientDet-Scalable-and-Efficient-Object-Detection"><a href="#EfficientDet-Scalable-and-Efficient-Object-Detection" class="headerlink" title="(EfficientDet) Scalable and Efficient Object Detection"></a>(EfficientDet) Scalable and Efficient Object Detection</h2><p>EfficientDet以EfficientNet作为网络的backbone，因此，首先对EfficientNet做一个基本介绍</p>
<h3 id="EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks"><a href="#EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks" class="headerlink" title="(EfficientNet)Rethinking Model Scaling for Convolutional Neural Networks"></a>(EfficientNet)Rethinking Model Scaling for Convolutional Neural Networks</h3><p>对于EfficientNet，该论文总共有两个较为重要的贡献点：</p>
<ol>
<li>提出了一种卷积神经网络的架构</li>
<li>研究并提出了一种卷积神经网络的不需手动设定指标的模型缩放方法</li>
</ol>
<p>对于贡献点1而言，作者使用了NAS(神经架构搜索)的技术，设计了一个新的backbone，称为EfficientNet。其相对于以前的ConvNets实现了更好的准确性和效率。</p>
<p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/image-20220726203542698.png" alt="image-20220726203542698"></p>
<p>对于贡献点2而言，作者认为，网络深度的增加、网络宽度的增加以及图像分辨率的增加均会提升卷积神经网络的精度。但若只是单尺度的提升，网络的识别精度提升是有限的，应三个尺度同时进行放缩才能在确定的计算资源的情况下实现最好的识别精度。</p>
<p>作者提出两点对于模型放缩的观察：</p>
<ul>
<li>放缩网络的任何一个维度（宽度、深度、图像分辨率）都可以提升准确性，但对于更大型的网络其准确性增益会降低</li>
<li>为了追求更好的精度和效率，在卷积网络的缩放过程中平衡网络宽度、深度和分辨率的各个维度至关重要</li>
</ul>
<p>基于上述的两点观察，以及卷积神经网络中计算资源占比最多的是卷积操作这一点先验，作者将模型的放缩定义如下：</p>
<p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/image-20220726091149041.png" alt="image-20220726091149041"></p>
<p>且由于是卷积操作占所有操作中计算资源的大头，所以模型的深度对于计算而言大致为线性关系，而图像的分辨率和宽度对于计算而言为平方的关系，所以其定义中的约束条件如上所示。当满足<script type="math/tex">\alpha*\beta^2*\gamma^2\approx2</script>的前提下，我们可以通过设定$\phi$来控制计算资源大致上扩充到之前的$2^\phi$倍。</p>
<p>EfficientDet的主要贡献点有如下两个：</p>
<ul>
<li>提出了BiFPN，一种新的特征融合的方法，是FPN的加强版</li>
<li>与EfficientNet相似的，提出了混合缩放。将EfficientNet中模型放缩的思维迁移到目标识别中。</li>
</ul>
<h4 id="BiFPN"><a href="#BiFPN" class="headerlink" title="BiFPN"></a>BiFPN</h4><p>常见的Neck阶段的特征融合方法如下所示：<img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/12321312.png" alt="在这里插入图片描述"></p>
<p>作者认为，多尺度特征融合的存在存在如下问题：</p>
<ul>
<li>各个尺度的特征信息不一致，但在实践中却占有相同权重</li>
<li>改进的特征融合算法的计算量过大且特征融合效果不算好</li>
</ul>
<p>所以基于以上两点，作者提出BiFPN，BiFPN相较于FPN而言有以下三点改进：</p>
<ul>
<li><strong>增加残差链接</strong>：意在通过简单的残差操作，增强特征的表示能力</li>
<li><strong>移除单输入边的结点</strong>：因为但输入边的结点没有进行特征融合，故具有的信息比较少，对于最后的融合没有什么贡献度，相反，移除还能减少计算量。</li>
<li><strong>权值融合：</strong>简单来说，就是针对融合的各个尺度特征增加一个权重，调节每个尺度的贡献度，其中，作者对比了Unbounded fusion、Softmax-based fusion和Fast normalized fusion，并最终选用了Fast-softmax.</li>
</ul>
<h4 id="混合缩放"><a href="#混合缩放" class="headerlink" title="混合缩放"></a>混合缩放</h4><p>本文还提出了一种混合缩放技术来全面地提升所有主干网络深度/宽度/分辨率、BiFPN和box/class检测网络，其思想与EfficientNet相同，不再赘述，其结果如下图所示：</p>
<p><img src="/2022/07/26/SSD%20Single%20Shot%20MultiBox%20Detector/v2-f59c80f65952ec08747236c0d6f509ce_720w.jpg" alt="img"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/07/25/Label%20Assignment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/25/Label%20Assignment/" class="post-title-link" itemprop="url">Label Assignment</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-25 22:26:51" itemprop="dateCreated datePublished" datetime="2022-07-25T22:26:51+08:00">2022-07-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-22 21:34:13" itemprop="dateModified" datetime="2022-08-22T21:34:13+08:00">2022-08-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Label-Assignment-是什么"><a href="#Label-Assignment-是什么" class="headerlink" title="Label Assignment 是什么"></a>Label Assignment 是什么</h2><p>Label Assignment 也称为Target Assignment，可理解为网络提供正负样本进行学习，让网络学习我们所要完成的目标的“正确”和“错误”的概念.</p>
<p>例如在目标检测的相关任务中，以下图为例任务为检出其中的蛙人、数字和字母，我们所期望的检测结果为：</p>
<p><img src="/2022/07/25/Label%20Assignment/objectDetect.png" alt="HT_2022_004_00543" style="zoom:50%;"></p>
<p>但是实际的检测效果可能为：</p>
<p><img src="/2022/07/25/Label%20Assignment/objectDetect_bad.png" alt="HT_2022_004_00543" style="zoom:50%;"></p>
<p>显然我们需要告诉网络，我们所需要的是第一张图而不是第二张，因为其包裹对象完整，而不是不完整或存在位置偏移的第二张图。而这个<strong>告诉</strong>的动作本质上就是Label Assignment的任务。</p>
<p>然而在实际的目标检测过程中，一张图中会有多个类别的目标，多个GT，Label Assignment的过程会十分复杂。只要是一个检测器，其只要需要划分正负样本就可以看作一个label assignment的过程。label assignment已经作为检测网络的最核心的问题之一，建立GT和预测之间的对应关系（类别，Box，置信度）的好坏会直接影响到网络的效果。</p>
<p>Label Assignment一般可以分为两个方面的内容，一为学习目标的表示，一为正负样本的匹配，即可以理解为我们输出的预测框该怎么框，以及我们输出的预测框与GT的对应关系。</p>
<h3 id="学习目标的表示"><a href="#学习目标的表示" class="headerlink" title="学习目标的表示"></a>学习目标的表示</h3><p>对于学习目标的表示，基于网络的不同，先验的不同，学习目标的表示也各不相同。</p>
<h4 id="基于anchor的目标检测"><a href="#基于anchor的目标检测" class="headerlink" title="基于anchor的目标检测"></a>基于anchor的目标检测</h4><p>基于anchor的目标检测大都采用bounding box的方法，即在此类方法中的学习目标是anchor的坐标，通过anchor作为分类和框回归的先验。</p>
<h4 id="基于set-prediction的目标检测"><a href="#基于set-prediction的目标检测" class="headerlink" title="基于set-prediction的目标检测"></a>基于set-prediction的目标检测</h4><p>基于set-prediction的目标检测的代表为DETR，其将transformer引入目标检测，将任务视为一个图像到集合的问题，避免了人工设计anchor，转而embedding，让网络自己去学习anchor，学习embedding</p>
<h4 id="基于key-point、anchor-point等方式目标检测"><a href="#基于key-point、anchor-point等方式目标检测" class="headerlink" title="基于key-point、anchor-point等方式目标检测"></a>基于key-point、anchor-point等方式目标检测</h4><h3 id="学习正负样本的匹配"><a href="#学习正负样本的匹配" class="headerlink" title="学习正负样本的匹配"></a>学习正负样本的匹配</h3><h4 id="正负样本匹配的定义"><a href="#正负样本匹配的定义" class="headerlink" title="正负样本匹配的定义"></a>正负样本匹配的定义</h4><p><img src="/2022/07/25/Label%20Assignment/v2-bcce6dc999beb25f778a50e094bbfe8a_720w.jpg" alt="img"></p>
<p>如上图所示，我们关注的目标为人和推车，假设黄框为Ground Truth，蓝绿色为算法自动生成的anchor，那么将自动生成的anchor与推车的ground truth之间做匹配的过程，判断哪个anchor应该标注为正样本，哪些anchor应该标注为负样本的过程就是正负样本匹配的过程</p>
<h4 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster-RCNN"></a>Faster-RCNN</h4><p>以Faster-RCNN类的目标检测器为例，其通过RPN生成anchor，若目标图像中有两个目标物体，我们需要将生成的成千上万个anchor与真实目标的ground truth做匹配，其分配策略为基于IoU的分配策略，首先计算anchor与ground truth之间的IoU，IoU&gt;fg_thres(0.7)作为正样本，IoU&lt;bg_thres(0.3)作为负样本，IoU在bg_thres~fg_thres(0.3~0.7)之间作为ignore样本(不参与训练)，并使用NMS进行过滤</p>
<p>但这样的存在问题：</p>
<ol>
<li>IoU不能代表anchor的定位能力，IoU在0.3以下的anchor也可能被回归到0.7以上；</li>
<li>IoU为0.95和0.75的anchor有优劣之分，但一刀砍策略无法体现它们的区别；</li>
<li>anchor是预定义的，所以无法保证每个ground truth都能匹配很好的anchor，导致不同ground truth分配到的anchor不均衡。</li>
</ol>
<p>后续针对上述问题，提出了算法如下：（更新后的IOU算法：CIOU DIOU等）</p>
<h4 id="TopK"><a href="#TopK" class="headerlink" title="TopK"></a>TopK</h4><p>用于解决问题3</p>
<p><strong>分配策略</strong>：对每个ground truth，找到与它IoU为TopK的anchor作为正样本；可以看作通过动态改变IoU阈值来划分正负样本，同时保证不同大小的目标都能得到一定数量的anchor进行训练。</p>
<h4 id="Learning-from-Noisy-Anchor"><a href="#Learning-from-Noisy-Anchor" class="headerlink" title="Learning from Noisy Anchor"></a>Learning from Noisy Anchor</h4><p>用于解决问题2</p>
<p><strong>核心思想</strong>：提出一个评价anchor质量的指标cleanliness，根据回归后IoU以及分类置信度得出，用于判断一个正anchor是否是noisy的。</p>
<p><strong>分配策略</strong>：cleanliness可以代替0/1作为分类标签加入focal loss，同时还作为权重加权回归，即质量好的anchor多回归，质量不好(noisy)的anchor少回归。</p>
<h4 id="HAMBox"><a href="#HAMBox" class="headerlink" title="HAMBox"></a>HAMBox</h4><p>用于解决问题1</p>
<p><strong>核心思想</strong>：提出一种anchor补偿策略，动态地把那些本身和ground truth重叠度不高但回归结果很好的anchor设为正样本。</p>
<p><strong>分配策略</strong>：与TopK类似，在训练中对每个ground truth动态地补偿k个anchor作为正样本，这些anchor根据回归结果好坏选出。</p>
<h4 id="ATSS（该论文证明了回归的方式，数据的表示方式不影响训练效果，影响训练效果的是正负样本的分配）"><a href="#ATSS（该论文证明了回归的方式，数据的表示方式不影响训练效果，影响训练效果的是正负样本的分配）" class="headerlink" title="ATSS（该论文证明了回归的方式，数据的表示方式不影响训练效果，影响训练效果的是正负样本的分配）"></a>ATSS（该论文证明了回归的方式，数据的表示方式不影响训练效果，影响训练效果的是正负样本的分配）</h4><p><strong>核心思想</strong>：从统计意义上思考正负样本的定义，把每个ground truth周围的anchor与它的IoU进行统计可以形成一个分布，通过取这个分布上的某个分位数来决定每个ground truth的IoU阈值</p>
<p><strong>分配策略</strong>：</p>
<ol>
<li>对于每个输出的检测层，选计算每个anchor的中心点和目标的中心点的L2距离，选取K个anchor中心点离目标中心点最近的anchor为候选正样本（candidate positive samples）</li>
<li>计算每个候选正样本和groundtruth之间的IOU，计算这组IOU的均值和方差根据方差和均值，设置选取正样本的阈值：t=m+g ；m为均值，g为方差</li>
<li>根据每一层的t从其候选正样本中选出真正需要加入训练的正样本然后进行训练</li>
</ol>
<p>普遍思路：1、如何度量一个anchor的好坏 2、如何将anchor（GT）分配给GT（anchor）使网络学习最大化</p>
<p>其余方法：OTA、DETR、OneNet、E2E with FCN</p>
<h2 id="A-Dual-Weighting-Label-Assignment-Scheme-for-Object-Detection"><a href="#A-Dual-Weighting-Label-Assignment-Scheme-for-Object-Detection" class="headerlink" title="A Dual Weighting Label Assignment Scheme for Object Detection"></a>A Dual Weighting Label Assignment Scheme for Object Detection</h2><p>2022 cvpr 一种用于目标检测的双加权标签分配方案</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>标签分配是要给每一个训练样本分配一个正损失权重和一个负损失权重，这两个权重会在目标检测的过程中发挥重要的作用。现存的标签分配方法大都专注于正权重的设计而负权重只是直接从正权重的基础上推导而来，这样的机制限制了检测器的性能。<strong>本文拓展研究了一种新型的权重范式$dual\ weighting(DW)$,$DW$分别指定了正权重和负权重。一个样本的正权重由其的分类和定位分数的一致性来决定，负权重被分解为两个部分：一个样本为负样本的可能性以及其为负样本的前提下其的重要性。</strong>这样的权重策略更灵活的区分重要以及不那么重要的样本，并最终导致物体检测的有效性的增加。</p>
<h3 id="简介及相关工作"><a href="#简介及相关工作" class="headerlink" title="简介及相关工作"></a>简介及相关工作</h3><p>目标检测作为一项基础的视觉任务，其已经吸引了很多研究者数十年的注意力。目前最为先进$(SOTA)$的目标检测大都通过预先定义的anchor来预测类别属性和回归偏移来执行密集检测。Anchor作为一个基础的检测单元，需要被分配合适的分类和回归的标签去监督整个训练过程。这样的标签分配的过程可以看作为为每个anchor分配损失权重的过程，一个anchor的分类损失（回归损失类似）可以简单的表示为：</p>
<p>  $ \mathcal {L}_{cls}= - w_{pos} \times \ln {(s)} - w_{neg} \times \ln {(1-s)}$</p>
<p>其中$w_{pos}$和$w_{neg}$分别为正权重和负权重，$s$是预测的分类分数。基于$w_{pos}$和$w_{neg}$的设计，标签分类的方法可以大致的分为两类，分别为hard LA和soft LA。</p>
<h4 id="hard-LA"><a href="#hard-LA" class="headerlink" title="hard LA"></a>hard LA</h4><p>hard LA假设每个anchor要么是正要么是负的，这意味着$w_{pos} , w_{neg} ∈ {0, 1}  $并且$w_{pos} + w_{neg} =1$，这个策略的核心策略是找到一个合适的边界去将anchor分为positive set和negative set。基于这样的分割策略，可以细分为静态的与动态的。</p>
<h5 id="静态-hard-LA"><a href="#静态-hard-LA" class="headerlink" title="静态 hard LA"></a>静态 hard LA</h5><p>​    静态 hard LA采取了预先定义好的指标来进行区分。</p>
<ul>
<li>IoU以及IoU类（RCNN类）</li>
<li>anchor中心到对应的GT中心点的距离（FCOS、Foveabox）</li>
</ul>
<p>​    问题/缺陷：<strong>静态匹配策略忽略了具有不同大小和形状的对象的划分边界可能会有所不同。</strong></p>
<h5 id="动态-hard-LA"><a href="#动态-hard-LA" class="headerlink" title="动态 hard LA"></a>动态 hard LA</h5><ul>
<li>ATSS</li>
<li>Prediction-aware assignment strategies</li>
<li>OTA</li>
<li>Transformer-based detectors</li>
</ul>
<p>问题/缺陷：<strong>动态和静态的分配策略都忽略了样本不是相同重要的事实</strong></p>
<p>分析目标检测的评价指标我们可以发现，<strong>最优预测不仅应该具有较高的分类分数，还应该具有准确的定位</strong>，这意味着在训练中，<strong>分类头和回归头之间具有较高一致性的Anchor应该更为重要</strong>。</p>
<h4 id="soft-LA"><a href="#soft-LA" class="headerlink" title="soft LA"></a>soft LA</h4><p>基于以上的问题、缺陷以及分析，我们可以发现soft LA的策略更加适合。</p>
<ul>
<li>GFL</li>
<li>VFL<ul>
<li>上述两种方法是经典的基于IoUs并通过乘以一个调制因子转化成为目标标签的soft LA方法。</li>
</ul>
</li>
<li>Focal Loss</li>
<li>Generalized focal loss</li>
<li>Varifocal loss</li>
<li>FreeAnchor、Autoassign</li>
</ul>
<p>现有的方法大都集中于正权重的设计，然而负权重仅仅由正权重得出。这样的方法会限制检测器的学习能力，这是因为负样本权重只提供了很少的新监督信息。这样的耦合权重设计机制会导致细腻度不够。</p>
<p>如下图所示：</p>
<p><img src="/2022/07/25/Label%20Assignment/image-20220713133757496.png" alt="image-20220713133757496"></p>
<p>对于左上图而言，假设其为目标物体，假设分别有四个anchor分别为A、B、C、D，其对应的与GT的IoU和Score如右上图所示，则常见的soft LA的算法得到的$w_{pos}  \  w_{neg} $如上图下部分所示。框A、B、C、D有不同的预测结果，，然而GFL和VFL算法分配了几乎相同的权重给（B、C、D）。由于在现存的soft LA算法中负权重与正权重高度相关耦合，所以具有不同特点的anchor有的时候会被赋予基本上相同的正负权重，从而影响检测器的有效性。</p>
<p>为了给检测器提供更具有分辨性能的监督信号，我们提出了$dual \  weighting (DW)$，一种新的LA算法，从不同的角度分别指定正权重和负权重，并使其互为补充。</p>
<p>正权重：正权重动态的被从类别检测头中包含的置信度分数以及回归检测头中包含的回归分数决定的</p>
<p>负权重：对于每个anchor而言，负样本被分为两部分，1、这个样本是负样本的可能性2、其是负样本的情况下，他的重要性 并由这两部份相乘得到。</p>
<p>通过这样的方式，在推理的时候，有更高分类分数和更精确的定位的bounding boxes会更容易在NMS之后剩下，而其余的会排序较后并会被筛除。根据上图所示，DW给四个不同的anchor分配了几乎不同的正、负权重，这样可以提供给检测器更加精细的监督特征。</p>
<p>并附加设计了一个边框修正模块去提供给我们的权重一个更加精确的回归分数。基于粗略回归图设计了一个回归优化算法。通过引入适当的计算负担，得到了更精确的回归.</p>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="动机和框架"><a href="#动机和框架" class="headerlink" title="动机和框架"></a>动机和框架</h4><p>要与NMS相容，一个好的稠密检测器应该可以预测同时具有好的分类分数和精确定位的边缘框，然而如果训练样本都一致的被对待，就容易出现以下问题：有最高分类分数的通常不是拥有最好位置回归的边缘框。特别是在IoU的评判标准下，这样的问题会严重影响检测器的效果。soft LA尝试着评价分类和回归的一致性，当使用soft LA时，一个anchor的loss，可表示如下：</p>
<p>$L_{cls}= -w_{pos} \times ln(s)-w_{neg}\times ln(1-s)$</p>
<p>$L_{reg}= w_{reg} \times l_{reg}(b,b^`)$</p>
<p>其中$s$是预测的类别分数,$b$和$b^`$是预测的边缘框和GT，$l_{reg}$是回归损失函数，例如$L_1 \ loss$ ，IoU Loss等等。在类别预测和回归中的不一致可以通过给一致性较好的anchor分配更大的$w_{pos}\ w_{neg}$来解决。因此这些经过较好训练的anchor就会在预测的时候预测更高的分类分数和更精确的定位，</p>
<p>现有的工作大都将$w_{pos}$与$w_{neg}$设定为相同的值，并主要注重于如何去定义其的一致性以及整合其到损失函数中，下表整理了在最近的代表性工作中，pos anchor的$w_{pos}\ w_{neg}$的公式。</p>
<p><img src="Label Assignment/url=http%3A%2F%2Fdingyue.ws.126.net%2F2022%2F0412%2F938a56efj00ra7am0002fd200u000b6g00id006u.jpeg" alt="img"></p>
<p>我们可以发现现有的方法中通常定义一个度量因素t作为分类和定位之间的一致性的程度的表示。然后将不一致性度量因素定义为（1-t），并最终通过增加比例因子$((s-t)^2,s^2,t)$整合到正损失和负损失之中去。</p>
<p>与之前的$w_{pos}\ w_{neg}$高度相关的方法不同，我们提出的方法将$w_{pos}\ w_{neg}$分别使用预测感知的方法进行测试</p>
<p>即：pos加权函数以预测的cls得分s和预测框与GT目标之间的IoU作为输入，并通过估计cls与reg head之间的一致性程度来设置pos权重。 neg加权函数采用与pos加权函数相同的输入，但将负样本权重表示为以下2项的乘法：Anchor是负样本的概率以及其为负样本时的重要性。这样，具有相似pos权值的模糊Anchor可以接收到具有不同负样本权值的更细粒度的监督信号，这是现有方法中是没有的。</p>
<p>该方法的流程示意图如下：</p>
<p><img src="/2022/07/25/Label%20Assignment/image-20220714104625755.png" alt="image-20220714104625755" style="zoom:200%;"></p>
<p>即首先通过选择GT中心附近的Anchor（中心先验），为每个GT目标构建一个候选正样本集合。候选集合外的Anchor被认为是负样本，不会参与加权函数的设计过程，因为它们的统计数据(如IoU，cls分数)在早期训练阶段非常混乱。候选集内的Anchor将被分配到$w_{pos}\ w_{neg} \ w_{reg}$三个权重上，以更有效地监督训练过程。</p>
<h4 id="pos加权函数"><a href="#pos加权函数" class="headerlink" title="pos加权函数"></a>pos加权函数</h4><p>一个样本的正加权函数应当反映其在分类和定位两方面上的准确检测物体的能力。本文通过分析物体检测的评价指标分析得出影响其的因素。在进行COCO数据集上进行测试期间，对一个类别的所有预测应该通过一个排序指标被合适的排序。现存的方法大都使用分类分数或分类和预测的IoU作为排序指标。每个边界框的正确性将从排名列表的开头开始检查。一个预测将被定义为一个正确的预测的条件如下：</p>
<ul>
<li>预测的边界框和最近的GT之间的IoU大于预先设定的阈值$\theta$</li>
<li>在本预测框之前没有排名更靠前的预测框满足上一个条件</li>
</ul>
<p>即只有第一个具有大于阈值$\theta$的IoU的边界框会被定义为pos detection。其他的框对于这个GT而言都会被认为是假阳性。</p>
<p>我们可以研究得到，高的排名分数和高IoU都是pos预测的充要条件，这表明同时满足这两个条件的anchor会在测试阶段更容易被定义为pos prediction，因此其在训练阶段就应该具有更高的重要性。从这个角度分析，$w_{pos}$应当与IoU和排名分数正相关，即</p>
<p>$w_{pos} ∝ IoU\ and\ w_{pos} ∝ s$</p>
<p>我们定义一致性度量t如下所示,t是为了衡量两个条件之间的对齐度：</p>
<p>$t=s\times IoU^{\beta}$</p>
<p>其中，$\beta$被用来平衡这两个条件。</p>
<p>为了使正权重在不同的anchor中有较大的变化，从而提供较为高细腻度的监督信息，添加一个指数项的调制因子：</p>
<p>$w_{pos}=e^{ut}  \times t$</p>
<p>其中u是一个超参数，控制不同pos权重的相对差距，最后，每个实例的每个Anchor的pos权值由候选集内的所有pos权值之和进行归一化。</p>
<h4 id="neg加权函数"><a href="#neg加权函数" class="headerlink" title="neg加权函数"></a>neg加权函数</h4><p>虽然pos权重可以表明具有高的cls分数和大的IoUs的一致Anchor，但不一致Anchor的重要性不能用pos权重来区分。如本文首图所示，Anchor D的位置更好(IoU大于θ)，但cls得分较低，而Anchor B的位置较差(IoU小于θ)，但cls得分较高。它们可能具有相同的一致性程度度量t，这不能反映它们的差异。为了为检测器提供更多的鉴别监督信息，本文建议通过为它们分配区别更明显的负权重来忠实地表明它们的重要性，这定义为以下2项的乘法。</p>
<h5 id="样本作为负样本的概率"><a href="#样本作为负样本的概率" class="headerlink" title="样本作为负样本的概率"></a>样本作为负样本的概率</h5><p>根据COCO的衡量指标，小于$\theta$的IoU是将一个预测判断为错误的充分条件。这意味着一个不能满足IoU衡量标准预测边缘框即使有高的类别分数也会被定义为neg detection。所以，IoU是决定一个样本是否为neg detection的唯一的因素，我们定义其为$P_{neg}$</p>
<p>依据COCO数据集的衡量标准采取IoU从0.5到0.95去衡量AP，本文定义$P_{neg}$应当满足如下规则：</p>
<script type="math/tex; mode=display">
P_{neg} = \begin {cases} 1, & \textit {if } \; \text {IoU $<$ 0.5}, \\ [0,1], & \textit {if } \; \text {IoU $\in $ [0.5,0.95]}, \\ 0, & \textit {if } \; \text {IoU $>$ 0.95}, \end {cases} \label {eq5}</script><p>在区间[0.5,0.95]内定义的任何单调递减函数都适用于上式。为简单起见，将$P_{neg}$实例化为以下函数：</p>
<p>$  P_{neg} = -k \times IoU ^ {\gamma _1} + b, \quad \textit {if } \text { IoU $\in $ [0.5,0.95]} $</p>
<p>其通过点（0.5,1）和（0.95,0）。一旦确定$\gamma _1$，参数k和b可以用未确定系数的方法得到。图3绘制了$  P_{neg} $和IoU在具有不同$\gamma _1$值的曲线。</p>
<p><img src="/2022/07/25/Label%20Assignment/v2-14dc210edad1c4ba4fec5ae5635e98bd_720w.jpg" alt="img"></p>
<h5 id="样本作为负样本的前提下的重要程度"><a href="#样本作为负样本的前提下的重要程度" class="headerlink" title="样本作为负样本的前提下的重要程度"></a>样本作为负样本的前提下的重要程度</h5><p>在推理的时候，Rank列表中的负样本预测不会影响召回率，但会降低精度。所以负样本边界框的Rank应该尽可能落后，也就是说，它们的Rank分数应该尽可能小。基于这一点，Rank得分较高的负样本预测比Rank得分较低的负样本预测更重要，因为它们是网络优化的困难样本。</p>
<p>因此我们定义$I_{neg}$为负样本的重要程度，其应该是排名分s的函数，特别的，我们定义其为：</p>
<p>$I_{neg}=s^{\gamma_2}$</p>
<p>其中$\gamma_2$是表明对重要的负样本应该给予多少优先考虑的一个因素</p>
<p>所以最终，我们定义neg weight为$w_{neg}=P_{neg}\times I_{neg}$,整合之后如下：</p>
<script type="math/tex; mode=display">
\small { w_{neg}= \begin {cases} s^{\gamma _2}, & \textit {if } \; \text {IoU $<$ 0.5}, \\ (-k \times IoU^{\gamma _{1}}+b) \times s^{\gamma _{2}}, & \textit {if } \; \text {IoU $\in $ [0.5,0.95]}, \\ 0, & \textit {if } \; \text {IoU $>$ 0.95}, \end {cases} }</script><p>我们分析可得：$w_{neg}$与IoU呈负相关，但与s呈正相关。可以看出，对于2个pos权重相同的Anchor，IoU较小的Anchor的neg权重较大。 $w_{neg}$的定义与推理过程很好地兼容，它可以进一步区分具有几乎相同pos权重的模糊Anchor。</p>
<h5 id="边框修正"><a href="#边框修正" class="headerlink" title="边框修正"></a>边框修正</h5><p>由于pos和neg都以IoU作为输入，所以更准确的IoU可以促使更高质量的样本，有利于学习更强的特征。本文提出一个Box Refinement操作，基于预测偏移图$O ∈ R^{H×W ×4 }$其中$ O(j, i) = {∆l, ∆t, ∆r, ∆b}$ 表示从当前Anchor中心到最左边的l、最上面的r、最右边的r和最下面的b边的预测距离。由于靠近物体边界的点更有可能预测准确的位置，所以作者设计了一个可学习的预测模块基于粗边界框为每边生成一个边界点。</p>
<p>如下图示意所示：</p>
<p><img src="/2022/07/25/Label%20Assignment/v2-262a6397bb3c6fcd0d2a0d712ad31b3c_720w.jpg" alt="img"></p>
<p>四个边界点的坐标定义如下：</p>
<p>$B_{l}=\left (j+\Delta _{l}^{y}, i-\Delta l+\Delta _{l}^{x}\right )$<br>$B_{t}=\left (j-\Delta t+\Delta _{t}^{y}, i+\Delta _{t}^{x}\right )$<br>$B_{r}=\left (j+\Delta _{r}^{y}, i+\Delta r +\Delta _{r}^{x}\right )$<br>$B_{b}=\left (j+\Delta b+\Delta _{b}^{y}, i+\Delta _{b}^{x}\right )$</p>
<p>其中的$\Delta$都是上述模块的输出，且偏移图更新为</p>
<script type="math/tex; mode=display">
O^{\prime }(j, i)=\left \{\hspace {-1.mm}\begin {array}{l} \Delta l+\Delta _{l}^{x}+O(B_{l},0), \; \Delta t+\Delta _{t}^{y}+O(B_{t},1) \\ \Delta r+\Delta _{r}^{x}+O(B_{r},2), \; \Delta b+\Delta _{b}^{y}+O(B_{b},3) \end {array}\hspace {-1.mm}\right \}</script><h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><p>本文所提出的DW可以被应用在大多数现存的稠密检测器上，下面以FCOS应用DW为例。 按照惯例将中心度分支和分类分支的输出乘以最终的分类分数，损失函数如下所示：</p>
<p>$  \mathcal {L}_{det}=\mathcal {L}_{cls}+\beta \mathcal {L}_{reg}$</p>
<p>其中$\beta$是一个平衡因子，与$t = s \times IoU^ \beta  $中的$\beta$相同。对上式进行进一步解释可得：</p>
<script type="math/tex; mode=display">
\small { \begin {aligned} \mathcal {L}_{c l s}&=\sum \nolimits _{n=1}^{N} -w_{p o s}^{n} \times \ln \left (s^{n}\right )-w_{n e g}^{n} \times \ln \left (1-s^{n}\right ) \\ &+\sum \nolimits _{m=1}^{M} F L\left (s^{m}, 0\right ), \\ \mathcal {L}_{reg}&=\sum \nolimits _{n=1}^{N} w_{pos}^{n} \times GIoU\left (b, b^{\prime }\right ), \end {aligned} }</script><p>其中N和M分别是Anchor的总数，FL是Focal Loss，GIoU回归损失，s时预测的cls得分，b和b’分别是预测框和GT的位置。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>使用的数据集为MS-COCO，其包含115k的train set，5k的val set和20k的tset set。对其进行了消融实验，并通过AP（平均精度）来对其性能进行衡量。</p>
<p>使用ImageNet上预训练的ResNet-50和FPN作为实验的backbone，绝大多是使用12个epoch的训练，初始学习率为0.01,并在第8和第11个epoch后衰减十倍，在消融实验中，都使用800像素大小的图片进行训练。所有的实验都使用SGDM在8个GPU，总batchsize为16上运行。推理的时候，threshold设定背景框为0.05，并移除阈值为0.6的冗余框，得到最终的预测结果，超参数设置为：$γ_1=2$ , $γ_2=2$ , $β=5$ , $μ=5$</p>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><h5 id="1、正样本加权的超参数"><a href="#1、正样本加权的超参数" class="headerlink" title="1、正样本加权的超参数"></a>1、正样本加权的超参数</h5><p><img src="/2022/07/25/Label%20Assignment/v2-e0b9d845e784f904b8f6bcec964a22cb_720w.jpg" alt="img"></p>
<p>pos权重有2个超参数：$\beta$和$u$</p>
<ul>
<li>$\beta$在一致性度量t中平衡了cls得分和IoU之间的贡献。随着$\beta$值的增加，IoU的贡献程度也在增加。</li>
<li>$u$控制着pos权重的相对尺度。与较不一致的样本相比，更大的$u$使最一致的样本具有相对较大的pos权重。</li>
</ul>
<p>在表中展示了通过改变 $\beta$从3到7和$u$从3到8来改变DW的性能。可以看到，当  $\beta$ 为5，$u$为5时，效果最好。 $\beta$和$u$的其他组合会使AP性能从0.1降至0.7。因此，在其余所有实验中，将  $\beta$和$u$设为5。</p>
<h5 id="2、负样本加权的超参数"><a href="#2、负样本加权的超参数" class="headerlink" title="2、负样本加权的超参数"></a>2、负样本加权的超参数</h5><p><img src="/2022/07/25/Label%20Assignment/v2-09fd13f1d0709079d09e145dc6880e28_720w.jpg" alt="img"></p>
<p>作者还进行了几个实验来研究DW对超参数 $\gamma _1$和$\gamma _2$的鲁棒性，这些参数用于调节负样本权重的相对尺度。使用不同的  $\gamma _1$和 $\gamma _2$组合得到的AP结果范围为41~41.5，如表所示。这意味着DW的性能对这2个超参数不敏感。因此，在所有的实验中都采用了 $\gamma _1=2, \gamma _2=2$</p>
<h5 id="3、候选集的构建"><a href="#3、候选集的构建" class="headerlink" title="3、候选集的构建"></a>3、候选集的构建</h5><p><img src="/2022/07/25/Label%20Assignment/v2-e609b081788ca0d076f841911e3fbd66_720w.jpg" alt="img"></p>
<p>作为目标检测的常见做法，Soft LA只应用于候选集的Anchor。作者测试了3种候选集的构建方法，它们都是基于从Anchor到相应的GT中心的距离r（由特征stride归一化）。</p>
<ul>
<li>第1种方法是选择距离小于阈值的Anchor。</li>
<li>第2种方法是从FPN的每个级别中选择最前k个最近的Anchor。</li>
<li>第3种方法是给每个Anchor一个Soft中心权重 $e^{-r^2}$，并将其与$w_{pos}$相乘。</li>
</ul>
<p>结果如表4所示。可以看出，AP性能在41.1~41.5之间略有波动，这表明我们的DW对候选袋的分离方法具有鲁棒性。</p>
<h5 id="4、负样本加权函数的设计"><a href="#4、负样本加权函数的设计" class="headerlink" title="4、负样本加权函数的设计"></a>4、负样本加权函数的设计</h5><p><img src="/2022/07/25/Label%20Assignment/v2-754d5b17ec7d646d36d44bb42657df8c_720w.jpg" alt="img"></p>
<p>本文通过用其他替代方法来研究负权重函数的影响，如表所示。可以看到，只使用pos权重会将性能降低到39.5，这表明对于一些低质量的Anchor，只分配它们小的$w_{pos}$不足以降低它们的Rank分数。它们可以被强制赋予更大的$w_{neg}$从而使排名下降，从而在测试期间带来更高的AP。</p>
<p>在不使用$I_{neg} , P_{neg}$的情况下，分别得到了40.5AP和40.0AP，这验证了这两项都是必要的。正如现有方法所做的，试图用 $1−w_{pos}$ 替换$w_{pos}$ 实现了40.7AP的性能，比标准DW的低0.8点。</p>
<h5 id="5、Box-Refinement"><a href="#5、Box-Refinement" class="headerlink" title="5、Box Refinement"></a>5、Box Refinement</h5><p>在没有Box Refinement的情况下，DW方法达到41.5AP，这是第1个在不增加FCOS-ResNet-50的情况下，在COCO上实现超过41AP性能的方法。通过Box Refinement，DW可达到42.2AP，如表6所示。表7还显示，Box Refinement可以持续地提高具有不同Backbone的DW的性能。</p>
<h5 id="6、加权策略"><a href="#6、加权策略" class="headerlink" title="6、加权策略"></a>6、加权策略</h5><p>为了证明DW策略的有效性，将其与其他使用不同加权策略的LA方法进行了比较。结果如表所示。前5行是Hard LA方法，而其他的则是Soft LA方法。</p>
<p><img src="/2022/07/25/Label%20Assignment/v2-4db83f6b986f5e87b8794488b8216ad5_720w.jpg" alt="img"></p>
<p>Hard LA的最佳性能是通过OTA，40.7AP。由于OTA将LA作为一个最优规划问题，它将增加训练时间的20%以上。GFLv2利用一个额外复杂的分支来估计定位质量，并在Soft LA方法中获得了41.1AP的第2名性能。</p>
<p>与将权重分配给损失的主流方法不同，将自动分配权重分配给cls分数，并在训练期间通过它们的梯度更新它们。作者尝试分离自动分配中的权重并分配给损失，但只得到39.8和36.6AP，分别比原始性能低0.6和3.8分。这意味着自动分配中的加权方案在适应主流实践时不能很好地工作。</p>
<h4 id="与SOTA方法对比"><a href="#与SOTA方法对比" class="headerlink" title="与SOTA方法对比"></a>与SOTA方法对比</h4><p><img src="/2022/07/25/Label%20Assignment/v2-1365a72879d6593ed1070cc695d66c55_720w.jpg" alt="img"></p>
<h3 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h3><h4 id="DW的可视化"><a href="#DW的可视化" class="headerlink" title="DW的可视化"></a>DW的可视化</h4><p>下图为DW和目前现有的方法的可视化图</p>
<p><img src="/2022/07/25/Label%20Assignment/v2-df32a561f9b4fd2187410fc2e63614dd_720w.jpg" alt="img"></p>
<p>对上图进行分析可得，DW中的正权重和副权重大都集中于GT的中心，而GFL和VFL分配权重大都在更宽的范围。这种差异意味着DW可以更多地关注重要的样本，并减少容易获得的样本的贡献，比如那些在物体边界附近的样本。这就是为什么DW对candidate bag的选择更稳健。</p>
<p>我们还可以看到，中心区域的锚在DW中有不同的(pos，neg)重量对。相比之下，GFL和VFL中的阴性权重与pos权重高度相关。而DW变化则相对较大</p>
<h4 id="DW目前存在的问题"><a href="#DW目前存在的问题" class="headerlink" title="DW目前存在的问题"></a>DW目前存在的问题</h4><p>虽然DW可以很好地区分不同Anchor对一个物体的重要性，但它会同时减少训练样本的数量，如图5所示。这可能会影响对小目标的训练效果。如表7所示，DW对小目标的改进不如对大目标的改进高。为了缓解这一问题，作者可以根据目标大小动态设置不同的$w_{pos}$超参数，以平衡大小目标之间的训练样本。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我们提出了一种名为双重加权（DW）的自适应标签分配方案，以训练准确的密集对象检测器。 DW 打破了以往密集检测器中耦合加权的惯例，它通过从不同方面估计一致性和不一致性指标，为每个锚点动态分配单独的 pos 和 neg 权重。还开发了一种新的框细化操作来直接细化回归图上的框。 DW 与评估指标高度兼容。在 MS COCO 基准上的实验验证了 DW 在各种主干下的有效性。无论有没有框细化，带有 ResNet-50 的 DW 分别达到了 41.5 AP 和 42.2 AP，记录了新的 state-of-the-art。作为一种新的标签分配策略，DW 还展示了对不同检测头的良好泛化性能。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">YOLOv4总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-04 21:14:44" itemprop="dateCreated datePublished" datetime="2022-07-04T21:14:44+08:00">2022-07-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-22 21:35:35" itemprop="dateModified" datetime="2022-08-22T21:35:35+08:00">2022-08-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="目标检测组成及常见技术"><a href="#目标检测组成及常见技术" class="headerlink" title="目标检测组成及常见技术"></a>目标检测组成及常见技术</h3><p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/v2-dd7959839adc00c2803eb69574650a5a_720w.jpg" alt="img"></p>
<p>yolov4原文中提及的目前常见的目标检测的方法:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/v2-229510bb08fbe321ce6c041f75b676b5_720w.jpg" alt="img"></p>
<p>可以理解为如下,目标检测网络一般由以下四个部分组成:</p>
<ul>
<li><p><strong>Input部分：</strong>Image，Patches，Images Pyramid(图像金字塔)</p>
</li>
<li><p><strong>Backbone部分</strong>(Backbone的作用是目标的特征提取,用来提取基础特征,一般是在不同图像细粒度上聚合并形成图像特征的卷积神经网络)： VGG16，ResNet-50，SpineNet，EfficientNet-B0 / B7，CSPResNeXt50，CSPDarknet53</p>
</li>
<li><p><strong>neck部分</strong>(neck的作用是对backbone提取到的重要特征进行加工及再利用,目标检测常在backbone和heads部分加入一些层,用来进行一系列混合和组合图像的特征,并将图像特征传递到heads层):</p>
</li>
<li><ul>
<li>Additional blocks：SPP，ASPP，RFB，SAM</li>
<li>Path-aggregation blocks：FPN，PAN，NAS-FPN，Fully-connected FPN，BiFPN，ASFF，SFAM</li>
</ul>
</li>
<li><p><strong>Heads部分</strong>(heads的作用是根据传入的图像特征进行边界框的生成和类别的预测):</p>
</li>
<li><ul>
<li><p><strong>Dense Predictions</strong>(one-stage)：</p>
</li>
<li><ul>
<li>RPN，SSD，YOLO，RetinaNet （基于anchor）</li>
<li>CornerNet，CenterNet，MatrixNet，FCOS（无anchor）</li>
</ul>
</li>
<li><p><strong>Sparse Predictions</strong>(two-stages)：</p>
</li>
<li><ul>
<li>Faster R-CNN，R-FCN，Mask R-CNN（基于anchor）</li>
<li>RepPoints（无anchor）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="BOF-bag-of-freebies"><a href="#BOF-bag-of-freebies" class="headerlink" title="BOF(bag of freebies)"></a>BOF(bag of freebies)</h3><p>BOF是指那些能够提高精度但不增加推断时间的技术(但有可能会增加训练时间)</p>
<p>常见的BOF方法如下:</p>
<ul>
<li>数据增强.数据增广<ul>
<li>模拟几何畸变:Random Scaling,Random Cropping,Random Flipping, Random Rotating</li>
<li>模拟光照变化:brightness,contrast,hue,saturation(饱和度),noise</li>
<li>模拟遮挡:Ramdom Rease,CutOut,Hide-and-Seek,Grid Mask</li>
<li>利用多张图像进行增强:Mixup,CutMix</li>
<li>风格迁移:Style Transfer GAN</li>
</ul>
</li>
<li>网络正则化<ul>
<li>Dropot,DropConect,DropBlock</li>
</ul>
</li>
<li>处理数据分布不平衡<ul>
<li>two-stage:Hard Negative Example Mining,Online Hard Example Mining</li>
<li>one-stage:Focal Loss</li>
</ul>
</li>
<li>one-hot类别之间没有关联<ul>
<li>Label Smoothing,知识蒸馏</li>
</ul>
</li>
<li>BBox回归的损失函数的设计:<ul>
<li>IOU Loss,DIOU Loss,GIOU Loss,CIOU Loss</li>
</ul>
</li>
</ul>
<h3 id="BOS-bag-of-specials"><a href="#BOS-bag-of-specials" class="headerlink" title="BOS(bag of specials)"></a>BOS(bag of specials)</h3><p>BOS指的是那些增加少许推断代价,但是可以提高模型精度的方法.</p>
<p>常见的BOS方法如下</p>
<ul>
<li>增大模型感受野<ul>
<li>SPP,ASPP,RFB</li>
</ul>
</li>
<li>引入注意力机制<ul>
<li>Squeeze-and-Excitation(SE),Spatial Attention Module(SAM),modified SAM</li>
</ul>
</li>
<li>特征融合,特征集成模块<ul>
<li>Skip Connection,Hyper Column,FPN(SFAM,ASFF.BiFPN)</li>
</ul>
</li>
<li>改变激活函数<ul>
<li>Mish.Swish.Hard Swish.ReLu类</li>
</ul>
</li>
<li>后处理方法<ul>
<li>soft NMS,greedy NMS,DIOU NMS</li>
</ul>
</li>
</ul>
<h3 id="BOF和BOS中部分关键技术解析"><a href="#BOF和BOS中部分关键技术解析" class="headerlink" title="BOF和BOS中部分关键技术解析"></a>BOF和BOS中部分关键技术解析</h3><h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><h5 id="传统数据增强"><a href="#传统数据增强" class="headerlink" title="传统数据增强"></a>传统数据增强</h5><p>模拟几何畸变,模拟光照变换,是通过旋转,镜像,平移,改变图像亮度,色域直方图等方式进行数据的增强操作.</p>
<h4 id="数据增强-模拟遮挡"><a href="#数据增强-模拟遮挡" class="headerlink" title="数据增强-模拟遮挡"></a>数据增强-模拟遮挡</h4><h5 id="Random-Erease"><a href="#Random-Erease" class="headerlink" title="Random Erease"></a>Random Erease</h5><p>方法Random Erease属于模拟遮挡,不需要额外的参数或者内存消耗,通过随机选择图像中的一个矩形区域,并用随机值覆盖图像,从而模拟目标物体部分被其它物体遮掩的情况；</p>
<p>但其由于擦除的随机性,容易导致随机的填充区域将目标覆盖(例如7变成1)；当使用随机的像素值时可能会改变数据的均值和方差,从而导致测试表现不好；与其他数据增强技术同时使用时,先后顺序会对结果产生影响.</p>
<h5 id="CutOut"><a href="#CutOut" class="headerlink" title="CutOut"></a>CutOut</h5><p>方法CutOut属于模拟遮挡,其为通过填充区域从而将区域的图像信息遮挡,从而提升模型的泛化能力.但相较于Random Reaerse的随机取区域,CutOut使用的是固定大小的正方形区域,并用全0代替随机值进行填充,并且允许正方形区域在图片外.</p>
<p>但其会受到正方形边长设定的影响,其边长设定容易导致图像主要信息被覆盖或对信息完全不构成影响等效果.其在尺度不一的实际环境中可能会导致测试效果较差.且在使用cutout之前,应当首先进行图像的归一化,从而减少像素填充的影响.</p>
<h5 id="Hide-and-Seek"><a href="#Hide-and-Seek" class="headerlink" title="Hide-and-Seek"></a>Hide-and-Seek</h5><p>方法Hide-and-Seek属于模拟遮挡,和上述两种方法的本质相同,可以看作是对CutOut,Random Erease方法的扩展,其核心原理就是把图像划分为若干小块的区域,然后随机删除.其理论依据为将一些区域进行填充迫使模型通过其它区域的特征进行物体的识别,从而增强特征的表现能力和学到的特征的的多样性,提高模型的泛化能力.</p>
<p>其存在将主要物体完全遮掩的可能性,存在背景信息取代目标信息的可能性,且存在数据分布被改变的可能性.</p>
<h5 id="Grid-Mask"><a href="#Grid-Mask" class="headerlink" title="Grid Mask"></a>Grid Mask</h5><p>Grid Mask通过生成一个和原图相同分辨率的Mask,然后将该Mask和原图相称得到一个图像来进行模拟遮挡的.其中Mask的设置是通过控制ratio来控制原图像的信息保留比例,d用来控制每个块的大小.Mask中的空值是固定间隔,固定大小的方块空值在空间内复制而得到的.用这种方法可以避免过度删除或保持连续区域.一方面,过度删除区域会导致完整目标被删除或上下文信息缺失,从而导致剩下的区域不足以表达目标信息.另一方面,区域保留过多会导致其泛用性较差.</p>
<h4 id="数据增强-利用多张图片进行增强"><a href="#数据增强-利用多张图片进行增强" class="headerlink" title="数据增强-利用多张图片进行增强"></a>数据增强-利用多张图片进行增强</h4><h5 id="Mixup"><a href="#Mixup" class="headerlink" title="Mixup"></a>Mixup</h5><p>方法Mixup是一种运用在计算机视觉中的对图像进行混类增强的算法,其从每个batch中随机选择两张图像,以一定的比例混合形成新的图像.其混合方式为其标签和样本按随机比例进行混合,并将混合生成的图像进行训练.其公式如下所示,其中  $mixed_batch_x$是经过mixup处理之后得到的图片,而$mixed_batch_y$是mixup操作之后得到的标签,其中$\lambda$是比例系数.</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220526151020848.png" alt="image-20220526151020848"></p>
<h5 id="CutMix"><a href="#CutMix" class="headerlink" title="CutMix"></a>CutMix</h5><p>方法cutmix是指切割出图片中的一小块,然后将这一小块贴到其他的图片之中,并且label依据同样的原理进行混合.其公式如下所示:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220526162414668.png" alt="image-20220526162414668"></p>
<p>其中$X_A,X_B$是两张图片$Y_A,Y_B$是对应的label,$\lambda$是随机生成的权重.对于label而言当前图片内容在融合后面积的占比决定了label的值,假设分别用两张图的0.3和0.7融合在一起,原始label为[1,0]和[0,1],则融合之后的label为[0.3,0.7]</p>
<p>cutmix最大程度的利用了统一张图像上的两种不同图像信息,具有较好的分类性能和目标定位功能</p>
<h4 id="风格迁移GAN"><a href="#风格迁移GAN" class="headerlink" title="风格迁移GAN"></a>风格迁移GAN</h4><h5 id="Style-Transfer-GAN"><a href="#Style-Transfer-GAN" class="headerlink" title="Style Transfer GAN"></a>Style Transfer GAN</h5><p>因为在网络训练的过程中,网络常常会学习到细致的纹理特征,而不是我们常常所需要的形状特征,与我们的需求不符.因而我们使用Style Transfer GAN使图片的分割发生改变,改变图像的纹理特征而不改变图像的大致形状,从而进行数据增强,从而让模型学到纹理特征减少.提高模型的泛化能力.</p>
<h4 id="网络正则化"><a href="#网络正则化" class="headerlink" title="网络正则化"></a>网络正则化</h4><p>机器学习中的一个核心问题是需要设计的神经网络不仅在训练数据上表现良好,并且能在新的输入上具有泛化性.网络正则化的目的是避免过拟合造成的高方差.其可以理解为通过给模型添加限制,使其在被限制的条件下进行特征的学习从而使模型具有较强的泛化能力.常见的正则化方法有:L0正则化,L1正则化,L2正则化,Dropout,DropConnect,DropBlock,早停法等.</p>
<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的,更瘦的网络,这样降低了下一个节点对上一个节点的依赖,不会给上一层的某一个节点过高的权重,起到了压缩权重的作用。一般只在全连接层进行使用.</p>
<h5 id="DropConnect"><a href="#DropConnect" class="headerlink" title="DropConnect"></a>DropConnect</h5><p>不同于Dropout的直接将节点的输出值置为1,DropConnect是将权值(即节点和节点之间的边)以(1-p)的概率乘以0.一般只在全连接层进行使用.</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/20160528171125066.png" alt="img"></p>
<h5 id="DropBlock"><a href="#DropBlock" class="headerlink" title="DropBlock"></a>DropBlock</h5><p>Dropout在全连接层效果较好,但在卷积层效果不好,其原因是因为卷积层的特征图中相邻位置元素在空间上共享语义信息,所以尽管某个单元被drop,但其余单元可以将该信息补上,所以针对卷积神经网络,提出一钟结构形式的dropout来正则化,即dropblock.DropBlock则将整个局部空间进行删减丢弃,并将其运用在网络的每一个特征图上,并且可以在训练的不同阶段进行不同的设置.其效果比Dropout DropConnect好.</p>
<h4 id="数据分布不平衡"><a href="#数据分布不平衡" class="headerlink" title="数据分布不平衡"></a>数据分布不平衡</h4><p>数据分布不均衡一般的处理办法可以分为两大类:1. 数据增强 2.损失函数权值均衡</p>
<h5 id="TWO-STAGE"><a href="#TWO-STAGE" class="headerlink" title="TWO-STAGE"></a>TWO-STAGE</h5><h6 id="Hard-Negative-Example-Mining-困难负例样本挖掘"><a href="#Hard-Negative-Example-Mining-困难负例样本挖掘" class="headerlink" title="Hard Negative Example Mining(困难负例样本挖掘)"></a>Hard Negative Example Mining(困难负例样本挖掘)</h6><p>在目标检测中,检测的时候常常会遇到的问题是我们无法预知一张图片里面会存在多少个目标,所以目标检测框架常常会提出远高于实际数量的区域提议,但由于提出的区域太多,常常会使训练时大部分都是负样本,导致大量无意义的负样本在训练时对正样本产生了影响.根据Focal Loss的统计,通常包含少量信息的”easy examples”(负例) 与包含有用信息的”hard examples”(正例+难负例)的比例为100000:100.这会导致简单例的损失函数数值是难例的40倍.所以为了让模型正常训练,我们必须要通过某种方法抑制大量的简单负例,挖掘所有难例的信息,这是Hard Negative Example Mining的初衷.</p>
<p>Hard Negative Example Mining的本质为在训练时,尽量多挖掘难负例加入负样本集,这样会比easy negative组成的负样本集效果更好.</p>
<p>在RCNN中,采用了自举法(boootstrap)的方法:</p>
<ul>
<li>先用初始的正负样本训练分类器（此时为了平衡数据，使用的负样本也只是所有负样本的子集）</li>
<li>用上一步训练好的分类器对样本进行分类,把其中错误分类的那些样本(hard negative)放入负样本子集</li>
<li>继续训练分类器</li>
<li>如此反复,直到达到停止条件(比如分类器性能不再提升).</li>
</ul>
<p>即可以理解为RCNN的Hard Negative Mining 可以理解为给模型定制一个错题集,在每轮训练中不断将错误的投入下一轮训练中,直到网络性能不能提升为止.</p>
<h6 id="Online-Hard-Example-Mining"><a href="#Online-Hard-Example-Mining" class="headerlink" title="Online Hard Example Mining"></a>Online Hard Example Mining</h6><p>主要思想可以理解为:一个batch的输入经过网络的前向传播后，有一些困难样本loss较大，我们可以对loss进行降序排序，取前K个认为是hard example，然后有两种方案：</p>
<ul>
<li><p>第一个为最终loss只取前k个,其余置0,然后进行BP,其缺点为虽然置0,但内存中依然会为其分配内存</p>
</li>
<li><p>第二个方案的步骤如下所示:</p>
<ul>
<li>将Fast RCNN分成两个components：ConvNet和RoINet. ConvNet为共享的底层卷积层，RoINet为RoI Pooling后的层，包括全连接层；</li>
<li>对于每张输入图像，经前向传播，用ConvNet获得feature maps（这里为RoI Pooling层的输入）；</li>
<li>将事先计算好的proposals，经RoI Pooling层投影到feature maps上，获取固定的特征输出作为全连接层的输入；</li>
</ul>
<p>​     需要注意的是，论文说，为了减少显存以及后向传播的时间，这里的RoINet是有两个的，它们共享权重，</p>
<p>​     RoINet1是只读（只进行forward），RoINet2进行forward和backward：</p>
</li>
</ul>
<h5 id="ONE-STAGE"><a href="#ONE-STAGE" class="headerlink" title="ONE-STAGE"></a>ONE-STAGE</h5><h6 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h6><p>Focal Loss可以理解为一种处理样本分类不均衡的损失函数,其车中的点为根据样本分辨的难易程度给样本对应的损失添加权重,即给容易区分的样本添加较小的权重$a_1$,给难以区分的样本添加较大的权重$a_2$,那么损失函数的表达式可以写作:$L_{sum}=a_1<em>L_{易区分}+a_2</em>L_{难区分}$.其中$a_1$较小而$a_2$较大,所以损失函数中的难区分对象就将主导损失函数,即将损失函数的重点集中在难分辨的样本上,这种处理方法可以理解为Focal Loss.其中对于易分辨和难分辨的个体,我们用他们的置信度进行区分,分类置信度接近0或者1的样本称作易分辨样本,其余的称作难分辨样本.</p>
<p>Focal Loss的公式如下:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220527153939965.png" alt="image-20220527153939965"></p>
<p>其中$-log(p_t)$为标准交叉熵,$(1-p_t)^{\gamma}$ 为权重因子,对于$\gamma$取不同的值时.</p>
<ul>
<li>当 $\gamma=0$ 时，focal loss等于标准交叉熵函数。</li>
<li>当 $\gamma&gt;0$时，因为$(1-p_t)&gt;=0$,所以focal loss的损失应该是小于等于标准交叉熵损失。所以，我们分析的重点应该放在难、易分辨样本损失在总损失中所占的比例。<br>即假设有两个$y=1$的样本，它们的分类置信度分别为0.9和0.6，取 $\gamma=2$ 。按照公式计算可得它们的损失分别为：$-(0.1)^2\log(0.9)$ 和 $ -(0.4)^2\log(0.6)$ .<br>将它们的权重相除：$\frac{0.16}{0.01}=16 $，可得到分类置信度为0.6的样本损失大大增强，分类置信度为0.9的样本损失大大抑制，从而使得损失函数专注于这些难分辨的样本上，这也是函数的中心思想。</li>
</ul>
<h4 id="one-hot类别之间没有关联"><a href="#one-hot类别之间没有关联" class="headerlink" title="one-hot类别之间没有关联"></a>one-hot类别之间没有关联</h4><p>One-hot是指将类别变量转换为机器学习易于利用的一种形式的过程.其只有一个值不为0,其余特征均为0.</p>
<h5 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h5><p>多分类问题中,一般一个物体会输出对应于各个类别的置信度,然后将该置信度通过softmax便得到了该数据属于各个类别的概率.并使用cross-entropy进行loss的计算迭代.但cross-entropy和one-hot的结合使用会导致以下结果:</p>
<ul>
<li>真实标签跟其他标签之间的关系被忽略了，很多有用的知识无法学到；比如：“鸟”和“飞机”本来也比较像，因此如果模型预测觉得二者更接近，那么应该给予更小的loss；</li>
<li>倾向于让模型更加“武断”，成为一个“非黑即白”的模型，导致泛化性能差；</li>
<li>面对易混淆的分类任务、有噪音（误打标）的数据集时，更容易受影响</li>
</ul>
<p>label smoothing可以通过soft one-hot的方法解决上述问题,其加入了噪声,减少了真实样本标签的类别在计算损失函数时的权重,最终起到抑制过拟合的效果,增加label smoothing前后的概率分布改变如下:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/v2-56899017cd0d5c113edc8002997381d8_720w.jpg" alt="img"></p>
<p>交叉熵损失函数的改变如下:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/v2-858823f138177de7f61b725b5075e491_720w.png" alt="img"></p>
<p>最优预测概率分布如下:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/v2-2eb621ebddc7bc3b2722cb6bf535de17_720w.jpg" alt="img"></p>
<h5 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h5><p>知识蒸馏也是处理one-hot编码的一个思路.在传统的ont-hot或者硬编码过程中,一张图只存在一个标签,但忽略了标签和标签之间的关系,例如一张图片上的物体A与B很接近,那合理的分类输出应该是A最高,B次高.但使用了硬编码便会导致只输出了概率最大的类别特征,而忽略了物体与类别B的相似性,而转而告诉大家物体A与类别B与类别C的相似概率相同,这是不合常理的.而soft label则包含了更多了信息,给出了硬编码未曾给出的,物体与谁更像,不像谁,像和不像的概率等信息.且知识蒸馏引入了蒸馏温度T,从而将softmax变得更软,让其的非正确类别概率的信息暴露得更多,即让知识暴露得就越多.</p>
<p>蒸馏温度T对softmax的影响如下图所示:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ2xpY2hvbmc=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center.png" alt="(Wُ̑�ceQ�VGr�c��"></p>
<p>当T越大的时候,类别之间的相似信息就保留得越多,当T=1的时候,即为softmax本身.</p>
<p>在知识蒸馏的过程中存在两个网络,一个是复杂但高精度的模型Teacher模型,一个是精简但复杂度低,易部署的模型student,我们的目的是让教师网络通过hard target训练输出的soft target,作为学生网络的输入,其训练过程如下:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/f1e99d932a90ac1d4f94fdf55157cdfd.png" alt="img"></p>
<p>在使用知识蒸馏的方法进行训练时,总的损失分为两个部分,分别为==student loss==和==distillation loss==,而最后的loss函数为student loss和distillation loss的加权求和.而在预测的时候,与Teacher模型无关,直接输入学生模型进行预测即可.</p>
<p>这样的训练方式解决了使用one-hot编码时忽略了类间关系的问题,且压缩了模型,可以实现少样本的学习.</p>
<p>soft targets与label smoothing相比,label smoothing将正确分类突出,而将其余错误类别拉成相同的,给予了其它类别一些分数从而避免模型过于自信,但忽略了类间关系,其对比可如下所见:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/adda1387a384d25ca220f4319a8d4613.png" alt="img"></p>
<h4 id="BBox回归的损失函数的设计"><a href="#BBox回归的损失函数的设计" class="headerlink" title="BBox回归的损失函数的设计"></a>BBox回归的损失函数的设计</h4><h5 id="IOU-Loss"><a href="#IOU-Loss" class="headerlink" title="IOU Loss"></a>IOU Loss</h5><p>$IOU=\dfrac{ \vert A \cap B \vert}{ \vert A \cup B \vert}$</p>
<p>IOU Loss的计算公式是直接将构成区域的四个点看为一个整体进行回归的,解决了传统的$L_1$,$L_2$损失未考虑坐标点之间相关性的问题,其计算公式可以简单的被看为</p>
<p>$IOU Loss=1-IOU$</p>
<p>但IOULoss存在以下缺点:</p>
<ul>
<li>但预测框与目标框不相交时,即$IOU(A,B)=0$时,不能反映$A,B$距离的远近,此时IOU Loss无法优化两个框不相交的情况</li>
<li>假设预测框和目标框的大小都确定,其值只和其相交面积有关,但无法反映相交的方式.</li>
</ul>
<h5 id="GIOU-Loss"><a href="#GIOU-Loss" class="headerlink" title="GIOU Loss"></a>GIOU Loss</h5><p>对Ground Truth A和所得到的区域B求其的最小外接矩形C,并求A和B的IOU,$GIOU=IOU-\left(\dfrac{\vert C / A \cup B\vert}{\vert C\vert}\right)$</p>
<p>其具有如下性质:</p>
<ul>
<li>当IoU值为1时，GIoU 为 1，即|A U B| = |A ∩ B|；</li>
<li>Iou为0时，GIoU&lt;=0;</li>
<li>-1&lt;= GIoU &lt;=1;</li>
<li>GIou &lt;= IoU;</li>
</ul>
<p>由GIOU的计算过程可以得到,其的更新迭代过程中,若A与B相互包裹,则会导致其的外接矩形与$max(A,B)$相同,则GIOU退化为IOU,无法评估好坏.其次,其的训练过程首先需要其与目标框相交,所以其所需的训练轮次较多</p>
<p>$GIOULoss=1-GIOU$</p>
<h5 id="DIOU-Loss"><a href="#DIOU-Loss" class="headerlink" title="DIOU Loss"></a>DIOU Loss</h5><p>DIOU针对于GIOU的问题,提出了新的惩罚项,其表达式为$DIOU=IOU - \dfrac{\rho^2(A,B)}{c^2}$,其中的$\rho(A,B)$是指的是A框和B框中心点坐标的欧式距离,c是其外接矩形的对角线距离.$DIOULoss=1-DIOU$</p>
<p>DIOU的惩罚项$\dfrac{\rho^2(A,B)}{c^2}$,其优化的直接目的是缩小惩罚项,即为减小两个矩形框中心点之间的欧式距离.比GIOU要更为直接,损失收敛速度更快.</p>
<h5 id="CIOU-Loss"><a href="#CIOU-Loss" class="headerlink" title="CIOU Loss"></a>CIOU Loss</h5><p>边界框的回归应考虑三个比较重要的几何因素,即重叠面积,中心点距离和纵横比,在以前的各种IOULoss中,IOULoss,GIOULoss考虑重叠面积,DIOULoss考虑重叠面积和中心点距离,CIOULoss则同时考虑上述三点.</p>
<p>CLOULoss的惩罚项如下所示:$R_{CIOU}=\dfrac{\rho^2(A,B)}{c^2}+\alpha v$,其中$\alpha$是一个正的权衡参数,v则衡量长宽比的一致性,其定义如下:</p>
<p>$v=\dfrac{4}{\pi^2} \left(arctan\dfrac{w^{gt}}{h^{gt}}-arctan\dfrac{w}{h}\right)^2$</p>
<p>$\alpha=\dfrac{v}{\left( 1-IOU\right)+v}$</p>
<p>$CIOULoss=1-CLOU$</p>
<p>对于IOULoss相关:</p>
<ul>
<li>IOU_Loss：主要考虑检测框和目标框重叠面积。</li>
<li>GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。</li>
<li>DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。</li>
<li>CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。</li>
</ul>
<h4 id="增大模型感受野"><a href="#增大模型感受野" class="headerlink" title="增大模型感受野"></a>增大模型感受野</h4><h5 id="SPP"><a href="#SPP" class="headerlink" title="SPP"></a>SPP</h5><p>传统的CNN网络对图像的输入尺寸有要求,这是因为传统的CNN网络存在全连接层,全连接层的参数是上一层传入的特征个数,在传入的图像尺寸大小存在改变的情况下,该层学得的权重个数是不确定的,为了解决这个问题,SPP在全连接成之前加入了一个网络层,使其对任意的输入产生固定的输出.在SPP中,所添加的是一个pooling层,其的各种参数都是相对的,使最终pooling的结果是确定的,SPPNet思路是对于任意大小的feature map首先分成16、4、1个块，然后在每个块上最大池化，池化后的特征拼接得到一个固定维度的输出。以满足全连接层的需要。SPPNet理论上可以改进任何CNN网络，通过空间金字塔池化，使得CNN的特征不再是单一尺度的。<img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220606162333329.png" alt="image-20220606162333329"></p>
<h5 id="ASPP"><a href="#ASPP" class="headerlink" title="ASPP"></a>ASPP</h5><p>SPP可以理解为在普通的SPP的基础上,添加了膨胀因子并将输入通过ASPPPooling层,从而实现自由的多尺度特征提取.</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/7548f8d2dfdc4c34884860e5c6e4cdb9.png" alt="img"></p>
<h5 id="RFB"><a href="#RFB" class="headerlink" title="RFB"></a>RFB</h5><p>RFB可以理解为在其每个分支上使用不同尺度的常规卷积+空洞卷积,通过各个分支上各自的不同参数来模拟人类视觉感知模式,其网络结构如下图所示:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5YWl5Z2RJuWhq-WdkQ==,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="(Wُ̑�ceQ�VGr�c��"></p>
<h4 id="引入注意力机制"><a href="#引入注意力机制" class="headerlink" title="引入注意力机制"></a>引入注意力机制</h4><p>注意力机制可以被认为是一种权重分配的机制和策略.</p>
<h5 id="Squeeze-and-Excitation-SE"><a href="#Squeeze-and-Excitation-SE" class="headerlink" title="Squeeze-and-Excitation(SE)"></a>Squeeze-and-Excitation(SE)</h5><p>Squeeze-and-Excitation提出了一种新的网络模型的设计角度- 通过通道间的关系进行模型设计,这样提出的新的网络结构单元被叫作”Squeeze-and-Excitation”网络块,作者的定位是通过精确的建模卷积特征各个通道之间的作用关系来改善网络模型的表达能力。</p>
<p>SE的示意图如下所示:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/20170928205849736.png" alt="img"></p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/20170928210056332.png" alt="img"></p>
<h5 id="Spatial-Attention-Module-SAM"><a href="#Spatial-Attention-Module-SAM" class="headerlink" title="Spatial Attention Module(SAM)"></a>Spatial Attention Module(SAM)</h5><p>SAM就是用来对特征图内部的空间位置添加注意力机制的模块，假定输入的特征图还是C×H×W（也就是C张大小为H×W的特征图），这次我们对特征图的每个点（H×W内）进行通道数为C的最大值池化，这样最大值池化输出的特征图大小就是1×H×W，同时也进行通道数为C的平均值池化，输出的特征图大小也是1×H×W，将最大值池化输出的特征图和平均值池化输出的特征图进行拼接形成2×H×W的拼接特征图，然后通过1×1卷积进行通道降维成1×H×W的输出特征图，再经过Sigmoid激活形成空间注意力权重，然后和原来的C×H×W的特征图进行相乘。这样相当于给每张H×W的特征图乘于一个H×W的空间权重，从而形成空间注意力模块。<br><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5YWl5Z2RJuWhq-WdkQ==,size_20,color_FFFFFF,t_70,g_se,x_16-16544820991039.png" alt="在这里插入图片描述"></p>
<h5 id="modified-SAM"><a href="#modified-SAM" class="headerlink" title="modified SAM"></a>modified SAM</h5><p>Modified SAM是YOLOv4的一个创新点，称为像素注意力机制，它的思路也非常简单，就是把SAM模块的池化层全部去除，对C×H×W的特征图进行1×1卷积（既没有降通道也没有升通道），得到C×H×W的输出特征图，然后使用Sigmoid激活，再与原来的C×H×W进行像素点相乘。</p>
<p>其示意图如下所示:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5YWl5Z2RJuWhq-WdkQ==,size_20,color_FFFFFF,t_70,g_se,x_16-165448298270712.png" alt="在这里插入图片描述"></p>
<p>YOLOV4里没有这样修改的好处的解释，这里只是个人见解：点卷积的一个特点是对信息进行跨通道的组合，原来的SAM里点卷积的对象是平均池化与最大值池化后的concat结果，在这里，点卷积能选择的只有2个通道，能选择的少。modified SAM利用这一点给卷积更多的通道去选择来组合更优的结果，并且是每个通道下都组合出一组更优的结果来和输入进行点乘，而SAM只组合出一组作为所有通道下的更优结果(SAM输入只有2通道，而且是均值池化和最大值池化，所以只能组合出一组，多组的结果就有问题了)，以一不好代表全部。</p>
<h4 id="特征融合-特征集成模块"><a href="#特征融合-特征集成模块" class="headerlink" title="特征融合,特征集成模块"></a>特征融合,特征集成模块</h4><ul>
<li>Skip Connection,Hyper Column,FPN(SFAM,ASFF.BiFPN)</li>
</ul>
<h4 id="改变激活函数"><a href="#改变激活函数" class="headerlink" title="改变激活函数"></a>改变激活函数</h4><ul>
<li>Mish.Swish.Hard Swish.ReLu类</li>
</ul>
<h4 id="后处理方法"><a href="#后处理方法" class="headerlink" title="后处理方法"></a>后处理方法</h4><ul>
<li>soft NMS,greedy NMS,DIOU NMS</li>
</ul>
<h3 id="YOLOv4最终采用方案"><a href="#YOLOv4最终采用方案" class="headerlink" title="YOLOv4最终采用方案"></a>YOLOv4最终采用方案</h3><p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220606140600019.png" alt="image-20220606140600019"></p>
<p>yolov4最后采用的结构为:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5peg5bC955qE5rKJ6buY,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="img"></p>
<p>上图中部分组件:</p>
<p>==CBM==：Yolo v4网络结构中的最小组件，其由Conv（卷积）+ BN + Mish激活函数组成。<br>==CBL==：Yolo v4网络结构中的最小组件，其由Conv（卷积）+ BN + Leaky relu激活函数组成。<br>==Res unit==：残差组件，借鉴ResNet网络中的残差结构，让网络可以构建的更深。<br>==CSPX==：借鉴CSPNet网络结构，由三个CBM卷积层和X个Res unint模块Concat组成。<br>==SPP==：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。</p>
<p>张量拼接与张量相加<br>==Concat==：张量拼接，会扩充两个张量的维度，例如26×26×256和26×26×512两个张量拼接，结果是26×26×768。<br>==Add==：张量相加，张量直接相加，不会扩充维度，例如104×104×128和104×104×128相加，结果还是104×104×128。</p>
<ul>
<li><p>输入时采用了Mosaic数据增强,cmBN,SAT的方法</p>
<ul>
<li>Mosaic数据增强的使用主要是为了解决小目标的AP一般比中目标和大目标低很多的问题,但小目标分布并不均匀,且在训练集和测试集中分布不同.使用Mosaic数据增强的方法就是随机使用四张图片并进行随机的缩放和拼接,这样操作增加了很多小目标,让网络鲁棒性更好.</li>
</ul>
</li>
<li><p>backbone 采用CSPDarknet53加一系列的trickrespond_bgd</p>
<ul>
<li><p>CSPNet全称是Cross Stage Partial Networks，也就是跨阶段局部网络。</p>
</li>
<li><p>CSPNet解决了其他大型卷积神经网络框架Backbone中网络优化的梯度信息重复问题，将梯度的变化从头到尾地集成到特征图中，因此减少了模型的参数量和FLOPS数值，既保证了推理速度和准确率，又减小了模型尺寸。</p>
</li>
<li><p>CSPNet实际上是基于Densnet的思想，复制基础层的特征映射图，通过dense block 发送副本到下一个阶段，从而将基础层的特征映射图分离出来。</p>
</li>
<li><p>这样可以有效缓解梯度消失问题(通过非常深的网络很难去反推丢失信号) ，支持特征传播，鼓励网络重用特征，从而减少网络参数数量。</p>
<p>CSP结构示意图如下:</p>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ4OTg0MTc0,size_16,color_FFFFFF,t_70.png" alt="在这里插入图片描述"></p>
<ul>
<li>CSPDarknet53的激活函数使用Mish激活函数,其与leaky ReLu相比计算量较大,效果有所提升.但需要注意的是,只有在backbone之中的激活函数使用的是Mish,其余后续步骤使用的激活函数还是使用的leaky ReLu</li>
<li>在backbone之中使用了dropblock,是一种缓解过拟合的正则化方法,其作用在任何卷积层之上.</li>
<li>CBM是yolov4中的最小组件,由$Conv+Bn+Mish$组成,<ul>
<li><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220606151708955.png" alt="image-20220606151708955"></li>
</ul>
</li>
<li>Res unit模块借鉴了ResNet的结构,直接将输入传到Res unit的输出端,其由经过两个CBM模块处理之后的结果和输入相加所得.<ul>
<li><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220606151722575.png" alt="image-20220606151722575"></li>
</ul>
</li>
<li>CSP模块借鉴了上面所提到过的CSP的思想,由如下所示的部分所组成:<ul>
<li><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220606151847805.png" alt="image-20220606151847805"></li>
<li>其由CBM和Res unit组件得到的结果concat而成.</li>
</ul>
</li>
<li>最终的backbone分别输出$76<em>76,38</em>38,19*19$的feature map</li>
</ul>
</li>
</ul>
</li>
<li><p>neck 主要采用了SPP+PAN的思想</p>
<ul>
<li>CBL模块和CBM模块类似,不过其的激活函数由Mish换成了Leaky ReLu,其余组件没有改变<ul>
<li><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/image-20220606152324192.png" alt="image-20220606152324192"></li>
</ul>
</li>
<li>SPP模块的组成如下所示:<ul>
<li><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA54yq5LiN54ix5Yqo6ISR,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="img"></li>
<li>其分别采用了$1<em>1,5</em>5,9<em>9,13</em>13$的最大池化的方式进行多尺度融合,并最终concat成为最终的feature map</li>
</ul>
</li>
<li>PAN模块的组成如下所示:<ul>
<li><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/v2-b427ad60e3080fd4784df23e05ff675c_720w.jpg" alt="img"></li>
<li>原本的PAN中,两个特征图相结合采用的是shortcut,但在yolov4中对其进行改进,采用的是concat操作,融合后的特征图尺寸有所改变:<ul>
<li><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5peg5bC955qE5rKJ6buY,size_20,color_FFFFFF,t_70,g_se,x_16-16545031928827.png" alt="img"></li>
</ul>
</li>
<li>SPP+PAN<ul>
<li>SPP层自适应的进行池化提取特征,自顶向下传达强语义特征，而PAN则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合，加速了不同尺度特征的融合，进一步提高特征提取的能力。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Head 采用YOLOv3的Head</p>
<ul>
<li>YOLOv4中$Loss=边框位置损失+类别损失+置信度损失$,其使用了CIOULoss替代了YOLOv3中的边框位置损失,其余部分没有分别,其损失函数如下:</li>
</ul>
</li>
</ul>
<p><img src="/2022/07/04/YOLOv4%E6%80%BB%E7%BB%93/123213123123.png" alt="(sd"></p>
<p>其中置信度损失使用了focal loss</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">凯</p>
  <div class="site-description" itemprop="description">选择大于努力</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">凯</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
