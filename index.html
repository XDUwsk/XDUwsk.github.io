<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="选择大于努力">
<meta property="og:type" content="website">
<meta property="og:title" content="凯_kaiii">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="凯_kaiii">
<meta property="og:description" content="选择大于努力">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="凯">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>凯_kaiii</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">凯_kaiii</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">暂无</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/ELAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/ELAN/" class="post-title-link" itemprop="url">ELAN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-30 16:11:09 / 修改时间：16:12:28" itemprop="dateCreated datePublished" datetime="2023-06-30T16:11:09+08:00">2023-06-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="（ELAN）Designing-Network-Design-Strategies-Through-Gradient-Path-Analysis"><a href="#（ELAN）Designing-Network-Design-Strategies-Through-Gradient-Path-Analysis" class="headerlink" title="（ELAN）Designing Network Design Strategies Through Gradient Path Analysis"></a>（ELAN）Designing Network Design Strategies Through Gradient Path Analysis</h1><p>文章作者的想法为，发现当今主流的网络设计策略大多是基于前馈路径，即基于数据路径设计网络架构。在本文中，我们希望通过提高网络学习能力来增强训练模型的表达能力。由于驱动网络参数学习的机制是反向传播算法，我们设计了基于反向传播路径的网络设计策略。提出了层级、阶段级和网络级的梯度路径设计策略。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>很多研究都是从相同的角度出发的，也就是从浅层抽取 low-level 特征，从深层抽取 high-level 特征，然后将这些特征结合起来，即是 data path（前向传播）的角度</p>
<p><strong>本文作者的思考：</strong></p>
<p><img src="/2023/06/30/ELAN/3d05a481006d4031bc834323bbad4180.png" alt="在这里插入图片描述">如图所示，作者在 objective 和 loss 的角度分析了浅层和深层模型，作者发现通过调整 objective 和 loss layer 的配置，就可以控制每层学习到的特征（无论浅层还是深层)。<br>也就是说，网络学习到什么类型的特征取决于训练人员用什么信息投喂，而不是如何组合这些层，基于此，作者重新设计的网络结构</p>
<p><strong>本文作者的出发点：</strong></p>
<p>由于目前的参数更新方法都是反向传播规则，即目标函数会根据梯度来更新权重参数，所以本文是基于梯度传播路径来设计网络结构<br>本文的做法：为 layer-level、stage-level、network-level 设计了梯度路径：</p>
<p><code>Layer-level design：</code>设计了梯度分流策略，并通过调整 layers 的数量和计算残差连接的 channel ratio，设计了 Partial Residual Network（PRN）（PRN 和本文是相同的作者团队）<br><code>Stage-level design：</code>将硬件的特性引入网络结构中来加速网络的推理过程。作者通过最大化梯度结合和最小化硬件消耗的两个方式，设计了 Cross Stage Network（CSPNet）[33] （CSPNet 和本文是相同的作者团队）<br><code>Network-level design：</code>作者考虑了梯度传播的效率来平衡网络的学习能力，以网络的梯度反传路径长度作为总基础，设计了 Efficient Layer Aggregation Network（ELAN）</p>
<h2 id="ELAN"><a href="#ELAN" class="headerlink" title="ELAN"></a>ELAN</h2><p>ELAN 的主要目标是为了解决 deep model scaling 时难以收敛的问题</p>
<p>ELAN 是由 VoVNet 和 CSPNet 结合而来的，且其整个网络的梯度长度的优化是基于 Stack in computational block 结构的</p>
<p><code>Stack in computational block：</code></p>
<p>在做模型缩放时，如果网络达到了一定的深度，再叠加深度时，网络的效果可能会不升反降</p>
<p>举个例子：</p>
<ul>
<li>scaled-YOLOv4，P7 model 使用很多操作和参数，但只获得了很小的性能提升</li>
<li>ResNet-152 约是 ResNet-50 参数量的 3 倍，但在 ImageNet 只带了了 1% 的 acc 提升，当 ResNet 堆叠到大约 200 层时，性能比 ResNet-152 更差</li>
<li>VoVNet 堆叠到 99 层时，其 acc 比 VoVNet-39 还低</li>
</ul>
<p>分析：</p>
<ul>
<li>从梯度路径的设计来看，作者认为随着堆叠层数的增加， VoVNet 比 ResNet 的性能下降更多的原因在于，VoVNet 是基于 OSA module 堆叠而来，而每个 OSA module 都包括一个 transition layer，所以每堆叠一个 OSA module，每个层的梯度路径都会增加 1</li>
<li>而 ResNet 是基于 residual layers 堆叠而来的，每堆叠一个 residual layer，只会增加梯度最长路径</li>
</ul>
<p>为了进一步分析，作者基于 YOLOR-CSP 进行了一些实验，并且发现：</p>
<ul>
<li>当堆叠层数达到 80+ 时， CSP 早融合的方式比 normal CSP 效果更好，每个 stage 的最短梯度路径会减 1</li>
<li>当网络继续变深和变宽，CSP 晚融合的方式得到了更好的效果，每个 layer 的最短梯度路径会减 1</li>
</ul>
<p>Stack in computational block 如图 6 所示：</p>
<ul>
<li>出发点 1：为了避免使用更多 transition layer</li>
<li>出发点 2：让整个网络的最短梯度路径变得更长一些</li>
</ul>
<p>E-LAN 结构如图 6c 所示：主要为了避免过多的使用 transition layer（会提升梯度最短路径，影响网络加深）</p>
<p><img src="/2023/06/30/ELAN/628ff466598746f4ac251d9abbe94326.png" alt="在这里插入图片描述"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/CSPNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/CSPNet/" class="post-title-link" itemprop="url">CSPNet</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-30 16:10:57 / 修改时间：16:12:49" itemprop="dateCreated datePublished" datetime="2023-06-30T16:10:57+08:00">2023-06-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CSPNET-A-NEW-BACKBONE-THAT-CAN-ENHANCE-LEARNING-CAPABILITY-OF-CNN"><a href="#CSPNET-A-NEW-BACKBONE-THAT-CAN-ENHANCE-LEARNING-CAPABILITY-OF-CNN" class="headerlink" title="CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN"></a>CSPNET: A NEW BACKBONE THAT CAN ENHANCE LEARNING CAPABILITY OF CNN</h1><h2 id="CSPNet-简介"><a href="#CSPNet-简介" class="headerlink" title="CSPNet 简介"></a>CSPNet 简介</h2><p>在本文中，作者提出了跨阶段局部网络(CSPNet)，用来缓解以往工作需要从网络架构角度进行大量推理计算的问题，作者把这个问题归结为网络优化中的<strong>重复梯度信息</strong>。</p>
<p>作者的主要想法是通过分割梯度流，使梯度流通过不同的网络路径传播。通过切换拼接和转换，传播的梯度信息可以具有较大的相关性差异。此外，CSPNet可以大大减少计算量，并提高推理速度和准确性。除此之外，CSPNet 易于实现，并且足够通用，可以与 ResNet、ResNeXt 和 DenseNet 的体系结构相融合。</p>
<p>本文主要解决了以下的三个问题：</p>
<ul>
<li><strong>加强CNN的学习能力：</strong>现有的CNN网络存在经过轻量化之后的准确率大大下降的问题，现有的网络使用CSPNet的思想之后，计算量将减少10％至20％，准确率更高。</li>
<li><strong>消除计算瓶颈：</strong>认为过高的bottleneck会导致花费更多的时间进行推理，或部份算术单元会被闲置。所以将CNN的计算量均匀的分布在每一层，从而有效的提升每个计算单元的利用率。</li>
<li><strong>降低内存成本：</strong>在特征金字塔生成过程中采用了跨通道池化的方式进行特征映射。</li>
</ul>
<h2 id="CSPNet思想"><a href="#CSPNet思想" class="headerlink" title="CSPNet思想"></a>CSPNet思想</h2><pre><code>    在原本DenseNet中，前面层的feature map全部传入后面层作为输入，在CSPNet中，将前面层的feature map在通道上一分为二，一部分输入到后面层，一部分直接通过short-cut的方式连接到transition层，这样可以缓解一部分的梯度信息重复计算问题，从而减少模型的计算量和显存占用。
</code></pre><h2 id="CSPNet网络创新点"><a href="#CSPNet网络创新点" class="headerlink" title="CSPNet网络创新点"></a>CSPNet网络创新点</h2><h3 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h3><p><img src="/2023/06/30/CSPNet/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56iL5aSn5rW3,size_20,color_FFFFFF,t_70,g_se,x_16.png" alt="img"></p>
<p>传统的DenseNet中，第i层的输入与第i层的输出做concat，作为第i+1层的输入，这就要求输入和输出的分辨率保持不变，就是不做下采样操作，下采样操作在transition层进行。</p>
<p>在CSPDenseNet中，将输入特征数据在通道维度上划分为<img src="/2023/06/30/CSPNet/281b211c0a544314af53007639dc64e8.png" alt="img">， <img src="/2023/06/30/CSPNet/f3ba8572acaf4dab9e857b6403f518ef.png" alt="img">输入到DenseNet中，<img src="/2023/06/30/CSPNet/103a8eac372346009eeffe0dcad93118.png" alt="img">直接在transition层与DenseBlock的输出在通道维度上做concat。在CSPDenseNet的transition层，先将Dense Block的输出结果<img src="/2023/06/30/CSPNet/2d7ec7a7f418470698bc5a1f820be714.png" alt="img">经过一个conv卷积操作，然后和<img src="/2023/06/30/CSPNet/6fcccbe8978840aea95bca7d05e6306b.png" alt="img">进行concat得到<img src="/2023/06/30/CSPNet/419c23c79b0945b294cf22b5c14741b8.png" alt="img">，输入到另一个conv卷积操作得到<img src="/2023/06/30/CSPNet/150b965a21f743e8a22a9068a2ca9c15.png" alt="img">。</p>
<p>上述图（b）中CSPDenseNet的前向推理过程如下：</p>
<p><img src="/2023/06/30/CSPNet/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56iL5aSn5rW3,size_13,color_FFFFFF,t_70,g_se,x_16.png" alt="img"></p>
<p>参数更新过程如下：</p>
<p><img src="/2023/06/30/CSPNet/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56iL5aSn5rW3,size_14,color_FFFFFF,t_70,g_se,x_16.png" alt="img"></p>
<p>经过上述改进之后，CSPDenseNet将原来DenseNet中对于全部feature map的重复梯度计算降低了一半，因为另一半x0’ 的feature map不在经过Dense Block，直接送入了transition层。所以这种网络结构叫做Cross Stage Partial DenseNet，就是跨Stage的部分的DenseNet。</p>
<h3 id="Partial-Dense-Block"><a href="#Partial-Dense-Block" class="headerlink" title="Partial Dense Block"></a>Partial Dense Block</h3><p>Partial Dense Block的设计目的为</p>
<ul>
<li><strong>增加梯度路径:</strong>通过拆分合并策略，可以使梯度路径的数量增加一倍。由于跨阶段策略，可以减轻使用显式特征映射复制进行连接所带来的缺点</li>
<li><strong>每层的平衡计算:</strong>通常，DenseNet的基础层通道数远大于增长率。由于局部密集块中涉及密集层操作的基础层通道仅占原始数量的一半，因此可以有效解决近一半的计算瓶颈</li>
<li><strong>减少内存流量:</strong>假设DenseNet中一个密集块的基本特征图大小为w × h × c，增长率为d，总共有m个密集层。则该密集块的CIO为$(c × m) + ((m^2 + m) × d)=2$，部分密集块的CIO为$(c × m) + (m^2 + m) × d)=2$。虽然m和d通常远小于c，但部分密集块最多可以节省网络内存流量的一半。</li>
</ul>
<h3 id="Partial-Transition-Layer"><a href="#Partial-Transition-Layer" class="headerlink" title="Partial Transition Layer"></a>Partial Transition Layer</h3><p>Partial Transition Layer的设计目的为使梯度组合的差异最大化。Partial Transition Layer是一种层次化的特征融合机制，它利用梯度流的聚合策略来防止不同的层学习重复的梯度信息。在这里，作者设计了两个CSPDenseNet变体来展示这种梯度流截断是如何影响网络的学习能力的。</p>
<p><img src="/2023/06/30/CSPNet/20201210234304222.png" alt="在这里插入图片描述"></p>
<p>上图中的 (c) 和 (d) 展示了两种不同的融合策略：</p>
<p><strong>Fusion First：</strong>是将两部分生成的feature map进行拼接，然后进入过渡层。如果采用这种策略，将会损失大量的梯度信息。<br><strong>Fusion Last：</strong>对于fusion last策略，来自稠密块的输出将经过过渡层，然后与来自Part1的feature map进行连接。如果采用这种策略，由于梯度流被截断，梯度信息将不会被重用。<br>如果我们使用上图所示的四种架构来进行图像分类，其结果如下图所示：</p>
<p><img src="/2023/06/30/CSPNet/20201210235050303.png" alt="在这里插入图片描述"></p>
<p>从上图可以看出，如果采用Fusion Last策略进行图像分类，计算成本明显下降，但Top-1的准确率仅下降0.1%。另一方面，CSP (fusion first)策略确实有助于显著降低计算成本，但Top-1的准确率显著下降1.5%。</p>
<p><strong>通过使用跨阶段的分割和合并策略，我们能够有效地减少信息集成过程中重复的可能性。如果能够有效地减少重复的梯度信息，那么网络的学习能力将会得到很大的提高。</strong></p>
<p>我们可以得到如下结论：</p>
<ul>
<li>使用Fusion First有助于降低计算代价，但是准确率有显著下降。</li>
<li>使用Fusion Last也是极大降低了计算代价，top-1 accuracy仅仅下降了0.1个百分点。</li>
<li>同时使用Fusion First和Fusion Last相结合的CSP所采用的融合方式可以在降低计算代价的同时，提升准确率。</li>
</ul>
<h2 id="应用CSPNet的思想至其他网络："><a href="#应用CSPNet的思想至其他网络：" class="headerlink" title="应用CSPNet的思想至其他网络："></a>应用CSPNet的思想至其他网络：</h2><p><img src="/2023/06/30/CSPNet/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA56iL5aSn5rW3,size_20,color_FFFFFF,t_70,g_se,x_16-168726774348025.png" alt="img"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/RegVGG/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/RegVGG/" class="post-title-link" itemprop="url">RegVGG</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-30 16:10:43 / 修改时间：16:13:03" itemprop="dateCreated datePublished" datetime="2023-06-30T16:10:43+08:00">2023-06-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="RepVGG-Making-VGG-style-ConvNets-Great-Again"><a href="#RepVGG-Making-VGG-style-ConvNets-Great-Again" class="headerlink" title="RepVGG: Making VGG-style ConvNets Great Again"></a>RepVGG: Making VGG-style ConvNets Great Again</h2><p>主要贡献：提出了一种简单但功能强大的卷积神经网络结构，其网络结构，在推理时只具有3x3卷积和ReLU，在训练时具有多分支拓扑结构，通过结构重参数化技术实现训练时间和推理时间的解耦，并命名为RepVGG。</p>
<h3 id="对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下："><a href="#对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下：" class="headerlink" title="对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下："></a>对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下：</h3><ul>
<li>会降低模型的推理速度并且减少内存利用率</li>
<li>有些节点及算子会增加内存消耗并且对别的设备不友好。</li>
</ul>
<p>论文中提到，大部分学者提到FLOPs（浮点运算的数量）会影响推理速度，但是论文中作者做了实验发现FLOPs对模型的速度并不是强相关。</p>
<p>作者提出的RepVGG，其具有以下优点：</p>
<ul>
<li>该模型具有类似VGG的拓扑结构，没有任何分支，这意味着每一层都将其唯一前一层的输出作为输入，并将输出馈送到其唯一的后一层。</li>
<li>该模型的主体部分仅使用3 × 3的conv和ReLU。</li>
<li>模型的具体架构(包括具体的深度和层宽度)的实例化没有模型结构的自动搜索，手工细化，复合缩放，也没有其他代价较大的设计。</li>
</ul>
<h3 id="作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。"><a href="#作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。" class="headerlink" title="作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。"></a>作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。</h3><p>针对多分支架构的优点集中于训练上，而不希望用于推理上，故提出重参数化的方法来解耦训练时的多分支结构和推理时的简单架构，即意味着通过转换其参数将架构从一个转换到另一个。</p>
<p><img src="/2023/06/30/RegVGG/4H7{5]]_TNU%XI%5PPH9KA9.png" alt="img"></p>
<p>如上图中(b)和(c)所示，即为转换之后的RepVGG和转换之前的RepVGG。其将分支看作退化的1x1卷积，进一步看作退化的3x3卷积。从而可以从(b)中的模型架构转变为(c)中的模型架构，可以用3x3卷积、BN、1x1卷积等模块进行原模型的等效替换。从而提升计算速度。</p>
<h3 id="本文的核心贡献点如下："><a href="#本文的核心贡献点如下：" class="headerlink" title="本文的核心贡献点如下："></a>本文的核心贡献点如下：</h3><ul>
<li>我们提出了RepVGG，这是一种简单的架构，与最先进的技术相比，具有良好的速度-精度权衡。</li>
<li>我们建议使用结构重参数化将训练时间的多分支拓扑与推理时间的平面结构解耦。</li>
<li>我们展示了RepVGG在图像分类和语义分割方面的有效性，以及实现的效率和易用性。</li>
</ul>
<h3 id="如何实现结构重参数化："><a href="#如何实现结构重参数化：" class="headerlink" title="如何实现结构重参数化："></a>如何实现结构重参数化：</h3><p>在上述提到，RepVGG在训练时每一层都有三个分支，分别是identify，1x1，3x3，模型训练时，输出$ y=x+g(x)+f(x) $，每一层就需要3个参数块，对于n层网络，就需要$3*n$个参数块。所以我们需要重参数化，会使得推理时模型参数量小。</p>
<p><img src="/2023/06/30/RegVGG/aa1ad31949b54e76b0a282fab915478f.png" alt="img"></p>
<p>上图中的过程即为将训练好的多分支模型转换为单分支模型，从而达到推理时的高性能</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对于重参数化的实现主要存在两个问题：</span><br><span class="line">第一个问题，在每个卷积后都接上一个BN，怎么将卷积和BN融合。</span><br><span class="line">第二个问题，存在不同大小的卷积，怎么将几个不同大小的卷积融合在一起。</span><br></pre></td></tr></table></figure>
<p>对于第一个问题，在每个卷积后都接上一个BN，怎么将卷积和BN融合。</p>
<p><img src="/2023/06/30/RegVGG/v2-84cdab58644fcbcafb3c690c1669b879_1440w.webp" alt="v2-84cdab58644fcbcafb3c690c1669b879_1440w"></p>
<p>这其实就是一个卷积层，只不过权重考虑了BN的参数 我们令：</p>
<p><img src="/2023/06/30/RegVGG/v2-b438e3a2ee316a6054a4e4c45443fef3_1440w.webp" alt="img"></p>
<p>最终的融合结果即为：</p>
<p><img src="/2023/06/30/RegVGG/v2-cd0d2de067e4850fe4fafce70f58acf1_1440w.webp" alt="img"></p>
<h3 id="2-2-2-conv-3x3和conv-1x1合并"><a href="#2-2-2-conv-3x3和conv-1x1合并" class="headerlink" title="2.2.2. conv_3x3和conv_1x1合并"></a>2.2.2. conv_3x3和conv_1x1合并</h3><p> 这里为了详细说明下，假设输入特征图特征图尺寸为(1, 2, 3, 3)，输出特征图尺寸与输入特征图尺寸相同，且stride=1，下面展示是conv_3x3的卷积过程：</p>
<p><img src="/2023/06/30/RegVGG/v2-89854f076457c9c03b733a389db96993_1440w.webp" alt="img"></p>
<p> conv_3x3卷积过程大家都很熟悉，看上图一目了然，首先将特征图进行pad=kernel_size//2，然后从左上角开始(上图中红色位置)做卷积运算，最终得到右边output输出。下面是conv_1x1卷积过程：</p>
<p><img src="/2023/06/30/RegVGG/v2-88962d2f0fc8f1371d0d521c04c2a57d_1440w.webp" alt="img"></p>
<p> 同理，conv_1x1跟conv_3x3卷积过程一样，从上图中左边input中红色位置开始进行卷积，得到右边的输出，观察conv_1x1和conv_3x3的卷积过程，可以发现他们都是从input中红色起点位置开始，走过相同的路径，因此，将conv_3x3和conv_1x1进行融合，只需要将conv_1x1卷积核padding成conv_3x3的形式，然后于conv_3x3相加，再与特征图做卷积(这里依据卷积的可加性原理)即可，也就是conv_1x1的卷积过程变成如下形式：</p>
<p><img src="/2023/06/30/RegVGG/v2-b7409c315f10a158331bf90fcf32efd6_1440w.webp" alt="img"></p>
<h3 id="2-2-3-identity-等效为特殊权重的卷积层"><a href="#2-2-3-identity-等效为特殊权重的卷积层" class="headerlink" title="2.2.3. identity 等效为特殊权重的卷积层"></a>2.2.3. identity 等效为特殊权重的卷积层</h3><p> identity层就是输入直接等于输出，也即input中每个通道每个元素直接输出到output中对应的通道，用一个什么样的卷积层来等效这个操作呢，我们知道，卷积操作必须涉及要将每个通道加起来然后输出的，然后又要保证input中的每个通道每个元素等于output中，从这一点，我们可以从PWconv想到，只要令当前通道的卷积核参数为1，其余的卷积核参数为0，就可以做到；从DWconv中可以想到，用conv_1x1卷积且卷积核权重为1，就能保证每次卷积不改变输入，因此，identity可以等效成如下的conv_1x1的卷积形式：</p>
<p><img src="/2023/06/30/RegVGG/v2-b05e6fa96bd642c1da2d36d39a543d7a_1440w.webp" alt="img"></p>
<p>从上面的分析，我们进一步可以将indentity -&gt; conv_1x1 -&gt; conv_3x3的形式，如下所示：</p>
<p><img src="/2023/06/30/RegVGG/v2-bc97e575d5007645901830109828a36f_1440w.webp" alt="img"></p>
<p> 上述过程就是对应论文中所属的下述从step1到step2的变换过程，涉及conv于BN层融合，conv_1x1与identity转化为等价的conv_3x3的形式：</p>
<p><img src="/2023/06/30/RegVGG/v2-f5ce0b89a10aa36223275dccd6327cbe_1440w.webp" alt="img"></p>
<p> 结构重参数化的最后一步也就是上图中step2 -&gt; step3， 这一步就是利用卷积可加性原理，将三个分支的卷积层和bias对应相加组成最终一个conv<em>3x3的形式即可。</em><br>这里，大家可能既然把BN，identity，conv_1x1和conv_3x3都融合在一起了，为什么不干脆把ReLU也融合进去呢？其实也是可以将ReLU层进行融合的，<strong>但是需要进行量化</strong>，<strong>conv输出tensor的值域直接使用relu输出的值阈（同时对应计算Ｓ和Z），就可以完成conv和relu合并。无量化动作的优化是无法完成conv+relu的合并*</strong>。这里的知识请大家参考论文：<em><br><em>*<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.05877">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a>。</em></em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/VovNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/VovNet/" class="post-title-link" itemprop="url">VovNet</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-30 16:10:29 / 修改时间：16:13:21" itemprop="dateCreated datePublished" datetime="2023-06-30T16:10:29+08:00">2023-06-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="VoVNet-An-Energy-and-GPU-Computation-Efficient-Backbone-Network-for-Real-Time-Object-Detection"><a href="#VoVNet-An-Energy-and-GPU-Computation-Efficient-Backbone-Network-for-Real-Time-Object-Detection" class="headerlink" title="VoVNet:An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection"></a>VoVNet:An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection</h1><h2 id="大体介绍及缘由"><a href="#大体介绍及缘由" class="headerlink" title="大体介绍及缘由"></a>大体介绍及缘由</h2><p>因为 <code>DenseNet</code> 通过用密集连接，来聚合具有不同感受野大小的中间特征，因此它在对象检测任务上表现出良好的性能。虽然特征重用（<code>feature reuse</code>）的使用，让 <code>DenseNet</code> 以少量模型参数和 <code>FLOPs</code>，也能输出有力的特征，但是使用 <code>DenseNet</code> 作为 <code>backbone</code> 的目标检测器却表现出了运行速度慢和效率低下的弊端。作者认为是密集连接(<code>dense connection</code>)带来的输入通道线性增长，从而导高内存访问成本和能耗。</p>
<p>为了提高 <code>DenseNet</code> 的效率，作者提出一个新的更高效的网络 <code>VoVet</code>，由 <code>OSA</code>（<code>One-Shot Aggregation</code>，一次聚合）组成。<code>OSA</code> <strong>仅在模块的最后一层聚合前面所有层的特征</strong>，这种结构不仅继承了 <code>DenseNet</code> 的多感受野表示多种特征的优点，也解决了密集连接效率低下的问题。基于 <code>VoVNet</code> 的检测器不仅速度比 <code>DenseNet</code> 快 2 倍，能耗也降低了 1.5-4.1 倍。另外，<code>VoVNet</code> 网络的速度和效率还优于 <code>ResNet</code>，并且其对于小目标检测的性能有了显著提高。</p>
<p>DenseNet和VoVNet之间的区别，大体上可以如下图所示：</p>
<p><img src="/2023/06/30/VovNet/v2-06f9e7e6761c98f4f554cb5aabe9cab2_1440w.webp" alt="img"></p>
<h3 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h3><ul>
<li><p>讨论了 MAC 和 GPU 计算的效率，并研究了如何设计更高效的结构</p>
</li>
<li><p>抛出了 DenseNet 网络结构中的问题，包括低效的、冗余的操作等</p>
</li>
<li><p>提出了 One-shot Aggregation（OSA），将中间的特征一次性聚合（在最后一层聚合一次），如图 1b 所示，能够在</p>
<p>保留 concat 优势的同时优化 MAC（中间层输入输出通道相同） 和 GPU 计算效率（无需 1x1 卷积）</p>
</li>
<li><p>基于 OSA 模块，构建了 VoVNet，一个 backbone 网络结构，并且将该 backbone 用于 DSOD、RefineDet、Mask R-CNN 等方法中，取得了比 DenseNet、ResNet 等方法更好的效率和准确率的平衡</p>
</li>
</ul>
<h2 id="proposed-method"><a href="#proposed-method" class="headerlink" title="proposed method"></a>proposed method</h2><h3 id="重新思考密集连接"><a href="#重新思考密集连接" class="headerlink" title="重新思考密集连接"></a>重新思考密集连接</h3><p><strong>DenseNet 的优点</strong>：</p>
<p>在计算第 $l$ 层的输出时，要用到之前所有层的输出的 concat 的结果。这种<strong>密集的连接使得各个层的各个尺度的特征都能被提取</strong>，供后面的网络使用。这也是它能得到比较高的精度的原因，而且<strong>密集的连接更有利于梯度的回传</strong>（ResNet shorcut 操作的加强版）。</p>
<p><strong>DenseNet 缺点</strong>（导致了能耗和推理效率低的）：</p>
<ul>
<li>密集连接会增加输入通道大小，但输出通道大小保持不变，导致的输入和输出通道数都不相等。因此，DenseNet 具有具有较高的 MAC。</li>
<li>DenseNet 采用了 <code>bottleneck</code> 结构，这种结构将一个 3×3 卷积分成了两个计算（1x1+3x3 卷积），这带来了更多的序列计算（sequential computations），导致会降低推理速度。</li>
</ul>
<blockquote>
<p>密集连接会导致计算量增加，所以不得不采用 1×1 卷积的 <code>bottleneck</code> 结构。</p>
</blockquote>
<h3 id="One-shot-Aggregation"><a href="#One-shot-Aggregation" class="headerlink" title="One-shot Aggregation"></a>One-shot Aggregation</h3><p>OSA 模块就是只聚合每个 block 的最后一层特征，也就是在每个 block 的最后一层，对该 block 的前面所有层的特征进行 concat，只进行这一次的聚合。</p>
<p>该模块将中间层的特征聚合到最后一层。如图所示。每个卷积层包含双向连接，一个连接到下一层以产生具有更大感受野的特征，而另一个仅聚合到最终输出特征映射。</p>
<p><img src="/2023/06/30/VovNet/4c2143990db24da7be3fa4c43c96dd82.png" alt="在这里插入图片描述"></p>
<ul>
<li>首先，在和 DenseNet-40 的 dense block 参数和计算量相似的基础上，设计 OSA module</li>
<li>先使用层数相同的方式，随着每个卷积层输入尺度的减小，OSA 的输出比 dense block 的输出更大，OSA 模块的网络得到 93.6% acc，比同量级的 ResNet 效果好，由此可见，只在最后一层进行特征聚合，比使用全部中间层聚合更好</li>
<li>OSA 的 transition layer 和 DenseNet 有较大不同，OSA 中，从浅层来的特征对 transition layer 更有效，因为深层特征对 transition layer 没有很大的影响</li>
<li>所以，将 OSA module 降为使用 5 层（共 43 通道），如图 2 最下边一行，得到了 5.44% err，和 DenseNet-40 的 5.24% 很接近，这说明使用大量的中间层的密集连接是低效且没有很大的作用</li>
<li>在检测任务上，使用 5 层 43 通道的 OSA module 可以将 MAC 从 3.7M 降低到 2.5M，这是因为 OSA 的中间层输入输出通道是相同的，使得MAC 最低，此外，因为检测任务比分类任务使用更大分辨率的特征图，MAC 会更严重的影响耗时和效率</li>
</ul>
<p>总之，OSA 能够提升 GPU 是计算效率，OSA 中间层的输入输出通道数相同，也不大需要使用 1x1 瓶颈层来降维，所以，OSA 层数更少、更高效</p>
<h3 id="OSA-与-DenseNet-的不同之处总结如下："><a href="#OSA-与-DenseNet-的不同之处总结如下：" class="headerlink" title="OSA 与 DenseNet 的不同之处总结如下："></a>OSA 与 DenseNet 的不同之处总结如下：</h3><ul>
<li>每一层的输出并没有按路线（route）到所有后续的中间层，这使得中间层的输入大小是恒定的。这样就提高了 GPU 的计算效率。</li>
<li>另外一个不同之处在于没有了密集连接，因此 MAC 比 DenseNet 小得多</li>
<li>此外，由于 OSA 模块聚集了浅层特征，它包含的层更少。因此，OSA 模块被设计成只有几层，可以在 GPU 中高效计算。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/MobileNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/MobileNet/" class="post-title-link" itemprop="url">MobileNet</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-30 16:10:03 / 修改时间：16:13:52" itemprop="dateCreated datePublished" datetime="2023-06-30T16:10:03+08:00">2023-06-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="MobileNet系列"><a href="#MobileNet系列" class="headerlink" title="MobileNet系列"></a>MobileNet系列</h1><h2 id="MobileNetv1"><a href="#MobileNetv1" class="headerlink" title="MobileNetv1"></a>MobileNetv1</h2><p>贡献：</p>
<ul>
<li>提出了深度可分离卷积，将标准卷积用逐通道卷积+逐点卷积来代替</li>
<li>能够在边端设备使用，在保证效果的同时提升速度</li>
</ul>
<h3 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h3><p>深度级可分离卷积其实是一种可分解卷积操作（factorized convolutions）。其可以分解为两个更小的操作：深度卷积（depthwise  convolution） 和点卷积（ pointwise convolution）。</p>
<p>对于一个标准卷积，输入一个$12<em>12</em>3$的一个输入特征图，经过$ 5<em>5</em>3$的卷积核得到一个$8<em>8</em>1$的输出特征图。如果我们此时有$256$个特征图，我们将会得到一个$8<em>8</em>256$的输出特征图，如下图所示:</p>
<p><img src="/2023/06/30/MobileNet/ed74994c96c043b086a6ef061bf0d4af.png" alt="img"></p>
<p> 对于深度卷积(其实就是组为1 的分组卷积)来说，将特征图通道全部进行分解，每个特征图都是单通道模式，并对每一个单独的通道特征图进行卷积操作。这样就会得到和原特征图一样通道数的生成特征图。假设输入$12<em>12</em>3$ 的特征图，经过$5<em>5</em>1<em>3$的深度卷积之后，得到了$8</em>8*3$的输出特征图。输入和输出的维度是不变的3，这样就会有一个问题，通道数太少，特征图的维度太少，不能够有效的获得信息。</p>
<p><img src="/2023/06/30/MobileNet/7ee519c66af94a6b9d3eb69cea3ce7bf.png" alt="img"></p>
<p>逐点卷积就是$1<em>1$卷积，主要作用就是对特征图进行升维和降维。在深度卷积的过程中，我们得到了$8</em>8<em>3$的输出特征图，我们用256个$1</em>1<em>3$的卷积核对输入特征图进行卷积操作，输出的特征图和标准的卷积操作一样都是$8</em>8*256$了。如下图：</p>
<p><img src="/2023/06/30/MobileNet/f750cdd5d58440e795006c3bee29c78c.png" alt="img"></p>
<p>标准卷积与深度可分离卷积的过程对比如下：</p>
<p><img src="/2023/06/30/MobileNet/721c0945ad57422da5c344f802e29d48.png" alt="img"></p>
<h3 id="深度可分离卷积的优势"><a href="#深度可分离卷积的优势" class="headerlink" title="深度可分离卷积的优势"></a>深度可分离卷积的优势</h3><p>对于标准卷积来说，卷积核的尺寸是$D_k<em>D_k</em>M$，一共有$N$个，所以标准卷积的参数量是：</p>
<p><img src="/2023/06/30/MobileNet/cd3017db342c44c5ae247f81d74e8413.png" alt="img"></p>
<p>其计算量计算如下</p>
<p><img src="/2023/06/30/MobileNet/6faa54a4ef7a4a4bb13c16196de618c8.png" alt="img"></p>
<p>,深度可分离卷积的参数量由深度卷积和逐点卷积两部分组成。深度卷积的卷积核尺寸$D_k<em>D_k</em>M$；逐点卷积的卷积核尺寸为$1<em>1</em>M$，一共有$N$个，所以深度可分离卷积的参数量是：</p>
<p><img src="/2023/06/30/MobileNet/3ad597ec791743ca9fbea1f78cf0bd8d.png" alt="img"></p>
<p>其计算量计算如下</p>
<p><img src="/2023/06/30/MobileNet/3e28ca45a556474ab109a013a6efb3c2.png" alt="img"></p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2023/06/30/MobileNet/099f6b5094204e1faf457ff6677f730e.png" alt="在这里插入图片描述"></p>
<p>MobileNetV1 的结构如表 1 所示，下采样是使用步长为 2 的卷积实现的，共 28 层</p>
<p><img src="/2023/06/30/MobileNet/f7310662dff144a79bce1bdccf5f90b2.png" alt="在这里插入图片描述"></p>
<h2 id="MobileNetv2"><a href="#MobileNetv2" class="headerlink" title="MobileNetv2"></a>MobileNetv2</h2><p>贡献：</p>
<ul>
<li>提出了倒残差结构：先 1x1 升维，使用 3x3 提取特征，最后再 1x1 降维，和残差结构的先降维后升维的结构是反的</li>
<li>提出了线性瓶颈</li>
</ul>
<p>MobileNetV2中的核心思想是，瓶颈对模型的中间输入和输出进行编码，而内层则用于封装模型从较低级别概念（如：像素等）转换到较高级别描述符（如：图像类别等）的能力。最后，与传统的剩余连接一样，快捷方式能够实现更快地训练速度和更高的准确率。</p>
<h3 id="倒残差结构"><a href="#倒残差结构" class="headerlink" title="倒残差结构"></a>倒残差结构</h3><p>实验发现在 MobileNetv1 中，深度卷积核的参数较多为 0，也就是其卷积核没有发挥提取特征作用。那么作者先通过 1x1 卷积将维度上升，再使用深度卷积，深度卷积的输入输出通道数更高，就能够提取更多的信息。</p>
<p><img src="/2023/06/30/MobileNet/dfc1b9f5f1c7443e85b6190eb6a8422b.png" alt="在这里插入图片描述"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">残差模块：输入首先经过1*1的卷积进行压缩，然后使用3*3的卷积进行特征提取，最后在用1*1的卷积把通道数变换回去。整个过程是“压缩-卷积-扩张”。这样做的目的是减少3*3模块的计算量，提高残差模块的计算效率。</span><br><span class="line">倒残差模块：输入首先经过1*1的卷积进行通道扩张，然后使用3*3的depthwise卷积，最后使用1*1的pointwise卷积将通道数压缩回去。整个过程是“扩张-卷积-压缩”。为什么这么做呢？因为depthwise卷积不能改变通道数，因此特征提取受限于输入的通道数，所以将通道数先提升上去。文中的扩展因子为6。</span><br></pre></td></tr></table></figure>
<h3 id="线性瓶颈"><a href="#线性瓶颈" class="headerlink" title="线性瓶颈"></a>线性瓶颈</h3><p>线性瓶颈结构，就是末层卷积使用线性激活的瓶颈结构（将 ReLU 函数替换为线性函数），因为 ReLU 激活函数对低维信息会造成很大损失。</p>
<p>具体来说当低维信息映射到高维，然后经过Relu映射回低维时，若映射到的维度相对较高，则信息变换回去的损失较小；若映射到的维度相对较低，则信息变换回去后损失很大，如下图所示：</p>
<p><img src="/2023/06/30/MobileNet/7abdcc43b7bb47c8937195e9a97f8ab3.png" alt="在这里插入图片描述"></p>
<h2 id="MobileNetv3"><a href="#MobileNetv3" class="headerlink" title="MobileNetv3"></a>MobileNetv3</h2><p>贡献：</p>
<ul>
<li>使用 NAS 的方法搜寻更适合移动 CPU 的结构</li>
<li>提出了 MobileNetV3-Large 和 MobileNetV3-Small，并引入了 h-swish 和 SE 等模块进行效果优化</li>
</ul>
<p>MobileNetV3 提出的目标就是为了实现移动设备上的模型的准确率和耗时的平衡。</p>
<ul>
<li>MobileNetV1 引入了深度可分离卷积，来代替传统卷积</li>
<li>MobileNetV2 引入了线性瓶颈和反残差结构，来提升速度</li>
<li>MobileNetV3 为了 NAS 来搜寻更合适的网络，并且引入了 Swish 非线性方法的优化版本 h-swish 和 SE 模块，建立更高效的网络</li>
</ul>
<h3 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h3><ul>
<li><strong>修改初始卷积核的个数</strong><ul>
<li>对于v2的输入层，通过3*3卷积将输入扩张成32维。作者发现，其实可以32再降低一点，所以这里改成了16，在保证了精度的前提下，降低了3ms的速度。关于这一点改变可以在最后给出的网络结构中看到</li>
</ul>
</li>
<li><strong>更改网络末端计算量大的层</strong></li>
<li><strong>引入了SE模块</strong></li>
<li><strong>H-Swish激活函数</strong></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/30/ShuffleNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/30/ShuffleNet/" class="post-title-link" itemprop="url">ShuffleNet</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-30 16:09:52 / 修改时间：16:14:07" itemprop="dateCreated datePublished" datetime="2023-06-30T16:09:52+08:00">2023-06-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="ShuffleNet系列"><a href="#ShuffleNet系列" class="headerlink" title="ShuffleNet系列"></a>ShuffleNet系列</h1><h2 id="ShuffleNetv1"><a href="#ShuffleNetv1" class="headerlink" title="ShuffleNetv1"></a>ShuffleNetv1</h2><p><strong>现有网络的问题：</strong></p>
<p>现有的高效结构如 Xception 和 ResNeXt，其实在极小的网络上的计算效率依然不太高，主要在于很耗费计算量的 1x1 卷积。</p>
<p><strong>ShuffleNet 如何解决：</strong>使用 point-wise 分组卷积和 channel shuffle 两个操作，很好的降低计算量并保持准确率。这种结构能够允许网络使用更多的通道，帮助 encode 阶段提取更多的信息，这点对极小的网络非常关键。</p>
<ul>
<li>使用 point-wise 卷积来降低 1x1 卷积的计算量</li>
<li>使用 channel shuffle 能够让不同通道的信息进行交互</li>
</ul>
<p>这里再介绍几个基本概念：</p>
<p><strong>分组卷积：</strong>AlexNet 中提出的概念，在 ResNeXt 中有使用，也就是将特征图分为 N 个组，每组分别进行卷积，然后将卷积结果 concat 起来<br><strong>深度可分离卷积：</strong>和 MobileNet 中都有使用，也就是每个特征图使用一个卷积核来提取特征，之后使用 1x1 的卷积进行通道间的特征融合<br><strong>channel shuffle：</strong>shuffle 可以翻译为重新洗牌，也就是把不同组的 channel 再细分一下，打乱重新分组<br><strong>模型加速：</strong>加速推理时候的速度，如剪枝、量化</p>
<p><img src="/2023/06/30/ShuffleNet/0ddfad3d997b42c1a675fa533f4645b2.png" alt="在这里插入图片描述"></p>
<h3 id="ShuffleNet的亮点"><a href="#ShuffleNet的亮点" class="headerlink" title="ShuffleNet的亮点"></a>ShuffleNet的亮点</h3><ul>
<li>结合<strong><em>\</em>Group convolutions**</strong>和<strong><em>\</em>Channel Shuffle**</strong></li>
</ul>
<p>​    <strong>group conv的问题：</strong>现在的精简CNN网络设计中使用Group convolutions已经成为一种趋势，它可有效地减少传统CNN所需的密集计算的运算量。但同时由于Groups之间彼此并<strong>不share feature map特征</strong>，这样就会导致每个filter只对限定的一部分输入特征可见，最终使得输出特征集合的表达能力大大降低。</p>
<p>​    <strong>本文改进点：</strong>为了有效地对冲Groups convolution使用导致的Groups间特征互不相通的负面影响，作者提出了<strong>对Group convolution计算后对输出的output feature maps进行\</strong>shuffle处理*<em>*</em>，以使得接下来的Group convolution filters可在每个group所输出的部分channels构成的集合上进行计算。</p>
<h2 id="ShuffleNetv2"><a href="#ShuffleNetv2" class="headerlink" title="ShuffleNetv2"></a>ShuffleNetv2</h2><p>贡献： </p>
<ul>
<li>提出了更应该使用直接的效率度量方法（如速度、耗时等）</li>
<li>在 V1 的 channel shuffle 的基础上，又提出了 channel split，增强特征的重用性的同时也减少了计算量</li>
<li>提出了设计高效网络的方法：<ul>
<li>使用输入输出通道相同的卷积</li>
<li>了解使用分组卷积的代价（分组越多，MAC 越大）</li>
<li>合理的设定分组个数</li>
<li>降低网络并行的分支（并行越多 MAC 越大）</li>
<li>减少逐点运算</li>
</ul>
</li>
</ul>
<p>ShuffleNetV2 首先提出了 4 条设计高效网络的方法：</p>
<ul>
<li>G1：Equal channel width minimizes memory access cost (MAC)：当卷积层的输入特征矩阵与输出特征矩阵 channel 相等时 MAC 最小 （保持FLOPs不变时）</li>
<li>G2： Excessive group convolution increases MAC：当 GConv 的 groups 增大时（保持FLOPs不变时），MAC 也会增大，所以建议针对不同的硬件和需求，更好的设计对应的分组数，而非盲目的增加</li>
<li>G3： Network fragmentation reduces degree of parallelism：网络设计的碎片化程度（或者说并行的分支数量）越高，速度越慢（Appendix Fig 1）</li>
<li>G4：Element-wise operations are non-negligible：Element-wise操作，即逐点运算，带来的影响是不可忽视的，轻量级模型中，元素操作占用了相当多的时间，特别是在GPU上。这里的元素操作符包括 ReLU、AddTensor、AddBias 等。将 depthwise convolution 作为一个 element-wise operator，因为它的 MAC/FLOPs 比率也很高</li>
</ul>
<p><strong>基于上面4条指导准则总结如下：</strong></p>
<ul>
<li>1x1卷积进行平衡输入和输出的通道大小；</li>
<li>组卷积要谨慎使用，注意分组数；</li>
<li>避免网络的碎片化；</li>
<li>减少元素级运算。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/" class="post-title-link" itemprop="url">YOLOv7 Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-18 17:04:34 / 修改时间：17:04:49" itemprop="dateCreated datePublished" datetime="2023-06-18T17:04:34+08:00">2023-06-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors"><a href="#YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors" class="headerlink" title="YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"></a>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</h1><h2 id="文章主要贡献"><a href="#文章主要贡献" class="headerlink" title="文章主要贡献"></a>文章主要贡献</h2><ul>
<li>设计了几种可训练的bag-of-freebies方法，使得实时目标检测在不增加推理成本的情况下大大提高了检测精度</li>
<li>对于目标检测方法的改进，我们发现了两个新问题<strong>，即重参数化模块如何替换原始模块，以及动态标签分配策略如何处理对不同输出层的分配</strong>。此外，我们还提出了解决这些问题所带来的困难的方法</li>
<li>提出了实时目标检测器的“扩展”和“复合缩放”方法，可以有效地利用参数和计算量</li>
<li>本文提出的方法可有效减少当前实时目标检测器约40%的参数和50%的计算量，具有更快的推理速度和更高的检测精度。</li>
</ul>
<p>技术上的点为：</p>
<p>1.模型重参数化<br>    YOLOV7将模型重参数化引入到网络架构中，重参数化这一思想最早出现于REPVGG中。<br>2.标签分配策略<br>    YOLOV7的标签分配策略采用的是YOLOV5的跨网格搜索，以及YOLOX的匹配策略。<br>3.ELAN高效网络架构<br>    YOLOV7中提出的一个新的网络架构，以高效为主。<br>4.带辅助头的训练<br>    YOLOV7提出了辅助头的一个训练方法，主要目的是通过增加训练成本，提升精度，同时不影响推理的时间，因为辅助头只会出现在训练过程中。</p>
<h2 id="作者认为SOTA的目标检测所需要的部件"><a href="#作者认为SOTA的目标检测所需要的部件" class="headerlink" title="作者认为SOTA的目标检测所需要的部件"></a>作者认为SOTA的目标检测所需要的部件</h2><ul>
<li>更快、更强的网络架构（backbone）</li>
<li>一种更有效的特征提取方法（neck）</li>
<li>更精确的检测方法（head）</li>
<li>更具鲁棒性的损失函数（loss）</li>
<li>更高效的标签分配方法（label assignment）</li>
<li>更高效的训练方法（train strategy）</li>
</ul>
<h2 id="模型重参数化"><a href="#模型重参数化" class="headerlink" title="模型重参数化"></a>模型重参数化</h2><p>模型重参数化分为两种主要的技术手段</p>
<ul>
<li>模块级集成<ul>
<li>在训练时将一个模块拆分为多个相同或不同的模块分支，在推理时将多个分支模块整合为一个完全等价的模块。</li>
</ul>
</li>
<li>模型级继承<ul>
<li>用不同的训练数据训练多个相同的模型，然后对多个训练模型的权值进行平均</li>
<li>对不同迭代次数下的模型权值进行加权平均。</li>
</ul>
</li>
</ul>
<h2 id="模型缩放"><a href="#模型缩放" class="headerlink" title="模型缩放"></a>模型缩放</h2><p>模型缩放常有不同的缩放因子，如分辨率(输入图像的大小)、深度(层数)、宽度(通道数)和阶段(特征金字塔的数量)，从而在网络参数的数量、计算量、推理速度和精度上达到很好的权衡。我们观察到，所有基于连接的模型，如DenseNet或VoVNet，当这些模型的深度被缩放时，都会改变某些层的输入宽度。由于所提出的体系结构是基于串联的，我们必须为该模型设计一种新的复合缩放方法</p>
<h2 id="模型结构图"><a href="#模型结构图" class="headerlink" title="模型结构图"></a>模型结构图</h2><p><img src="/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/v2-1e9750e05bc3e329c7095388ea3583a7_1440w.webp" alt="img"></p>
<p><img src="/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/d6fa41fd929243228535b61c93b6ea14.png" alt="请添加图片描述"></p>
<h3 id="扩展的高效层聚合网络E-ELAN"><a href="#扩展的高效层聚合网络E-ELAN" class="headerlink" title="扩展的高效层聚合网络E-ELAN"></a>扩展的高效层聚合网络E-ELAN</h3><p>要设计高效的网络结构，一般需要考虑参数量、计算量、计算密度、内存访问消耗memory access cost（MAC），还要输入输出通道比例、多分支结构和元素级的相加等等，此外在模型缩放时还要考虑激活函数。</p>
<p>下图a、b是VovNet和改进的CSPVoVNet，CSPVoVNet分析了梯度路径，使得不同层的权重能够学习到更多的信息。ELAN考虑了如何设计一个更高效的网络结构：通过控制最短最长梯度路径，更深层能够更加高效地学习和收敛。</p>
<p><img src="/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/v2-b0bd82873eb6ae4998b9177000aebd9d_1440w.webp" alt="img"></p>
<p><strong>我们改进了ELAN结构，使用expand, shuffle, merge cardinality三种方式</strong>。</p>
<p>expand即指提高channels数量（假设提高为g倍），使用组卷积来减少计算量。但是组卷积会使得特征层的不同组之间的信息无法交流，因此参考shufflenet网络，使用通道重排技术进行信息交互。假设group=g，那么对g组特征层使用通道重排技术，然后将其相cat。这时候，此时的每一个group的特征层的channels数量和输入特征层相同（因为输出通道数扩大了g倍），因此我们将g组特征层相加起来，得到新的特征层，这就是merge操作。改进的E-ELAN操作如图2d。（另外，读到后面可以知道，不是所有模型都使用E-ELAN，图1的结构图是YOLOv7的基础版，是没有使用E-ELAN，而是使用ELAN）</p>
<h3 id="基于concatenation的模型的缩放策略"><a href="#基于concatenation的模型的缩放策略" class="headerlink" title="基于concatenation的模型的缩放策略"></a>基于concatenation的模型的缩放策略</h3><p>模型缩放是调整模型的尺寸，如增大模型提高精度，减小模型提高速度，来获得不同尺寸的模型以适应不同实际工程。如scaled-YOLOv4，它通过缩放stages的数量进行缩放模型。</p>
<p>对于常用的网络如PlainNet或者ResNet，缩放模型后，模型的输入通道数和输出通道数不会发生改变，那么可以独立分析缩放的影响。（如YOLOX和YOLOv5通过控制CSP_Block中残差块的数量进行缩放，这种不会改变输出通道数）。<strong>但是基于concatenation的模型，增加卷积个数后，下一个层的入度将会改变。</strong>如图3a和b，添加了深度后，模块输出的通道数一样会改变。</p>
<p><img src="/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/v2-933dbca99e1b3195d97cf3e9535e2ca4_1440w.webp" alt="img"></p>
<p>因此，如果对于要使用cat的模型，添加了更多的卷积后，模型的输入输出通道数将会改变，那么将不好单独分析模型缩放深度和宽度的影响。因此为了解决此问题，<strong>我们提出了如图3c的模块</strong>，当模型缩放深度后（如图3c的scaling up depth），我们计算cat后的输出通道数，然后设置相应的宽度缩放因子（如图3c的scaling up width），以此来控制输出的通道数不会改变。也就是说，通过computational block控制缩放深度，通过Transition控制缩放宽度。</p>
<h3 id="3-训练时的免费午餐"><a href="#3-训练时的免费午餐" class="headerlink" title="3 训练时的免费午餐"></a><strong>3 训练时的免费午餐</strong></h3><h3 id="3-1-planned重参数卷积"><a href="#3-1-planned重参数卷积" class="headerlink" title="3.1 planned重参数卷积"></a><strong>3.1 planned重参数卷积</strong></h3><p>尽管RepConv在VGG上取得巨大成功，但是当我们直接将其应用到ResNet和DenseNet或者其他结构时，其精度会较大下降。我们使用<strong>梯度流动传播路径方法</strong>去分析如何将重参数卷积结合到不同的网络。我们也设计了相应的planned重参数卷积。</p>
<p>RepConv经常和$3<em>3$卷积、$1</em>1$卷积和恒等映射混合使用。在分析了RepConv和不同结构的结合的表现后，我们发现RepConv里面的恒等映射损害了ResNet的残差连接和DenseNet的cat操作，而这两个操作能够给不同特征层带来梯度的多样性。因此，<strong>我们设计了一个去除恒等映射的RepConv-N，如果遇到残差连接或者cat操作时，使用RepConv-N，而不是RepConv</strong>。</p>
<p>如图4，在图4g和h中，RepConv去除了恒等分支。而图4d和f，因为输出时连接了残差，所以应该使用RepConv-N。</p>
<p><img src="/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/v2-ba4eb83432276c6af04975f82a32d2a0_1440w.webp" alt="img"></p>
<h3 id="3-2-粗到细的训练loss策略"><a href="#3-2-粗到细的训练loss策略" class="headerlink" title="3.2 粗到细的训练loss策略"></a><strong>3.2 粗到细的训练loss策略</strong></h3><p><strong>深度监督</strong>是训练深层网络时经常使用的技巧。它的主要思想是在网络的中间添加一个额外的辅助头，浅层网络的权重能够作为辅助损失去指导网络。甚至对于那些容易收敛的如ResNet、DenseNet等网络，深度监督依然能够为模型在多个任务上显著地提升表现。图5的a和b是采用与不采用深度监督后的模型结构，在本文中，我们将最终对输出负责的head称为lead head，辅助训练的head称为auxiliary head</p>
<p><img src="/2023/06/18/YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors/v2-9888244b3a573b41257f5d5e2380b767_1440w.webp" alt="img"></p>
<p>然后，我们讨论了<strong>标签分配</strong>。在早期的工作中，标签分配通常是通过参考gt标签，然后给予一个硬标签。然而最近，开始考虑预测输出的质量和分布，然后基于此计算一个可信赖的软标签。比如YOLO使用预测框与真实框的IOU，作为软标签。本文将这种机制称作标签分配器（label assigner）。</p>
<p>因为本文使用了深度监督策略，那么如何为auxiliary head和lead head分配软标签呢？主流的方法如图5c所示，即分别为Lead head和auxiliary head的预测结果做标签分配和loss计算。本文使用一种新颖的方法，即通过lead head的输出同时指导lead head和auxiliary head，也就是通过lead head的输出生成粗到细的层级标签。这两种提出的方法如图5d和e所示。</p>
<p><strong>Lead head指导的标签分配器（如图5d）</strong>，主要基于lead的预测输出和ground truth进行计算，通过优化过程生成软标签。这些软标签，将会在训练时同时用于辅助头auxiliary head和导向头lead head。之所以这么做，是因为lead head的表征能力强，所以生成的软标签对源数据和目标的分布和关系，更具代表性。更进一步说，我们可以将这个过程当作<strong>一类泛化性的残差学习</strong>，浅层辅助头学习导向头已经学习过的信息，那么导向头能够更加关注学习以前未学过的残差信息。</p>
<p><strong>粗到细的导向头指导的标签分配器（如图5e</strong>），它也使用lead head的预测和gt来生成软标签，但是它是生成两类标签，如coarse label和fine label。其中，fine标签和图5d的软标签生成过程相同，coarse标签通过放宽对正样本的约束，允许更多的网格被视作正样本，这就是粗标签生成过程。这是因为auxiliary head相对于lead head的学习能力较弱，为了避免信息丢失，对于auxiliary head我们聚焦于优化其召回率。此时，lead head能够从高召回率的结果中挑选高精度的结果。但是如果额外添加的粗标签的loss权重，和精标签的相同，那么可能会损害检测器。因此，为了使得粗标签中额外的正样本的权重减少，我们对解码器做了限制（具体如何限制文中未作解释，可能是对权重参数做了调整，具体信息得看源码才能知道），使得额外的粗正样本不能完美地产生软标签。以上的机制，允许粗标签和精标签的重要性在训练时动态调整，<strong>使得精标签的优化上界始终优于粗标签</strong>。</p>
<h3 id="3-3-其他训练时的免费午餐"><a href="#3-3-其他训练时的免费午餐" class="headerlink" title="3.3 其他训练时的免费午餐"></a><strong>3.3 其他训练时的免费午餐</strong></h3><p>以下列了一些本文使用的方案，但是不是由本文最先提出的。</p>
<ol>
<li><strong>Conv-BN-Act策略</strong>。BN层直接与Conv层相连，这样在推理时，BN层能够与conv层相融合。</li>
<li><strong>YOLOR的隐式知识建模</strong>（不太了解，所以没细看）。</li>
<li><strong>EMA model</strong>。滑动平均训练策略，这是在训练时给近期数据更高权重的平均方法，用于对模型的参数做平均，以求提高测试指标并增加模型鲁棒。在推理时，我们使用了EMA模型作为最终的模型。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/" class="post-title-link" itemprop="url">YOLOv6 A Single-Stage Object Detection Framework for Industrial  Applications</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-18 17:03:19 / 修改时间：17:04:07" itemprop="dateCreated datePublished" datetime="2023-06-18T17:03:19+08:00">2023-06-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications"><a href="#YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications" class="headerlink" title="YOLOv6: A Single-Stage Object Detection Framework for Industrial  Applications"></a>YOLOv6: A Single-Stage Object Detection Framework for Industrial  Applications</h1><h2 id="YOLOv6发现以往的模型存在以下问题："><a href="#YOLOv6发现以往的模型存在以下问题：" class="headerlink" title="YOLOv6发现以往的模型存在以下问题："></a>YOLOv6发现以往的模型存在以下问题：</h2><ul>
<li>来自RepVGG的重参数化是一种尚未在检测中得到很好利用的优越技术。我们还注意到，对于RepVGG块，简单的模型缩放变得不切实际，因此我们认为小型和大型网络之间的网络设计的优雅一致性是不必要的。对于小型网络，简单的单路径架构是更好的选择，但对于大型模型，单路径架构的参数和计算成本的指数增长使其不可行</li>
<li>基于重参数化的检测器的量化也需要细致的处理，否则在训练和推理过程中由于其异构配置而导致的性能下降将难以处理。</li>
<li>以前的工作往往不太关注部署，其延迟通常在V100等高成本机器上进行比较。当涉及到真正的服务环境时，存在硬件差距。通常，像Tesla T4这样的低功耗gpu成本更低，并且提供相当好的推理性能。</li>
<li>考虑到架构差异，标签分配和损失函数设计等高级领域特定策略需要进一步验证;</li>
<li>对于部署，我们可以容忍训练策略的调整，以提高精度性能，但不增加推理成本，例如知识蒸馏。</li>
</ul>
<h2 id="本文的主要工作"><a href="#本文的主要工作" class="headerlink" title="本文的主要工作"></a>本文的主要工作</h2><ul>
<li>我们重新设计了一系列不同规模的网络，为不同场景的工业应用量身定制。</li>
<li>不同规模的架构不同，以实现最佳的速度和精度权衡，其中小模型具有简单的单路径主干，而大模型构建在高效的多分支块上。</li>
<li>我们为YOLOv6注入了一种自蒸馏策略，同时执行分类任务和回归任务。同时，我们动态调整来自老师和标签的知识，帮助学生模型在所有训练阶段更有效地学习知识。</li>
<li>我们广泛验证了标签分配、损失函数和数据增强技术的先进检测技术，并有选择地采用它们来进一步提高性能。</li>
<li>我们在RepOptimizer和通道式蒸馏的帮助下，对检测的量化方案进行了改革，这导致了一个永远快速和准确的检测器，在batchsize大小为32时，具有43.3%的COCO AP和869 FPS的吞吐量。</li>
</ul>
<h2 id="使用的方法-amp-模型的具体结构"><a href="#使用的方法-amp-模型的具体结构" class="headerlink" title="使用的方法&amp;模型的具体结构"></a>使用的方法&amp;模型的具体结构</h2><p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/b55cad01c6aa466c8df35624466f1a49.png" alt="img"></p>
<ul>
<li><strong>网络设计</strong>:<ul>
<li><strong>backbone</strong>:与其他主流架构相比，我们发现在相似的推理速度下，RepVGG骨干网在<strong>小型网络</strong>中具有更强的特征表示能力，但由于参数和计算成本的爆炸式增长，它难以扩展以获得更大的模型。在这方面，我们将RepBlock作为我们小型网络的构建块。对于<strong>大型模型</strong>，我们修改了一个更有效的CSP块，命名为CSPStackRep块。</li>
<li><strong>neck</strong>:YOLOv6的颈部在YOLOv4和YOLOv5之后采用PAN拓扑。我们用RepBlocks或CSPStackRep Blocks增强颈部以获得RepPAN。</li>
<li><strong>head</strong>:我们简化了解耦头，使其更高效，称为高效解耦头。</li>
</ul>
</li>
<li><strong>标签分配</strong>:我们通过大量实验评估了标签分配策略的最新进展，结果表明<strong>TAL</strong>更有效，更适合训练。</li>
<li><strong>损失函数</strong>:主流无锚目标检测器的损失函数包含分类损失，anchor回归损失和对象损失。对于每一种损失，我们系统地用所有可用的技术进行实验，最终选择<strong>VariFocal loss</strong>作为我们的分类损失，<strong>SIoU/GIoU loss</strong>作为我们的回归损失</li>
<li><strong>行业便利的改进</strong>:我们引入了额外的常见做法和技巧来提高性能，包括<strong>自蒸馏</strong>和<strong>更多的训练epoch</strong>。分类和anchor回归分别由教师模型监督。由于DFL，anchor回归的精馏成为可能。此外，通过余弦衰减动态衰减软、硬标签信息的比例，帮助学员在训练过程中有选择地获取不同阶段的知识。此外，我们在评估中遇到了没有增加额外灰色边界的性能受损问题，对此我们提供了一些补救措施。</li>
<li><strong>量化和部署</strong>:为了解决基于再参数化的量化模型的性能下降问题，我们使用<strong>RepOptimizer</strong>训练YOLOv6，以获得ptq友好的权重。我们进一步采用QAT与通道智能蒸馏和图优化来追求极致的性能。</li>
</ul>
<h2 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h2><p>​    单阶段物体探测器通常由以下几个部分组成：主干、颈部和头部。主干网主要决定了特征表示能力，而其设计由于计算成本较大，对推理效率的影响很大。颈部用于将低级的物理特征与高级的语义特征进行聚合，然后在所有层次上建立金字塔形特征映射。头部由几个卷积层组成，并根据颈部组装的多层次特征来预测动态检测结果。从结构的角度来看，它可以分为基于锚头和无锚头，或者是参数耦合头和参数解耦头。 </p>
<p>​    在YOLOv6中，基于硬件友好的网络设计的原则，我们提出了两个缩放的可再参数化的骨干和颈，以适应不同大小的模型，以及一个有效的解耦与混合通道策略的头。YOLOv6的整体架构如图所示。</p>
<p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/b55cad01c6aa466c8df35624466f1a49-16867300125633.png" alt="img"></p>
<h3 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a><strong>Backbone</strong></h3><p>​    多分支网络通常比单路径网络具有更好的分类性能，但它往往伴随着并行性的降低，并导致推理延迟的增加。相反，像VGG这样的普通单路网络具有高并行性和更少的内存占用的优点，从而获得了更高的推理效率。最近在RepVGG中，提出了一种结构重参数化方法，将训练时间的多分支拓扑与推理时间的平面架构解耦，以实现更好的速度精度权衡。</p>
<p>​     <strong>受上述工作的启发，我们设计了一个高效的可重新参数化的骨干，称为EffificientRep。对于小模型，训练阶段骨干的主要成分是RepBlock</strong>，如下图所示在推理阶段，每个RepBlock转换为3×3卷积层（表示为RepConv），具有ReLU激活函数，3×3卷积在主流gpu和cpu上得到了高度优化，并且它具有更高的计算密度。因此，高效的代表骨干网充分利用了硬件的计算能力，从而显著降低了推理延迟，同时提高了表示能力。</p>
<p>​    然而，<strong>随着模型容量的进一步扩大，单路网络中的计算成本和参数数量呈指数级增长。</strong>为了更好地实现计算负担和准确性之间的权衡，我们修改了CSPStackRep块来构建中大型网络的主干。如图所示，<strong>CSPStackRepBlock</strong>由三个1×1卷积层和一堆由两个RepVGGBlock或RepConv（分别在训练或推理时）组成，具有残差连接。此外，采用跨阶段部分（CSP）连接，在不增加计算成本的情况下提高性能。</p>
<p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/17095d3e9d894251b1542f04500d7653.png" alt="img"></p>
<h3 id="Neck"><a href="#Neck" class="headerlink" title="Neck"></a><strong>Neck</strong></h3><p>​    <strong>采用YOLO v4和YOLO v5的PAN结构，将RepBlock（用于小型模型）或CSPStackRep块替换为YOLOv5中使用的CSPBlock)，</strong>并相应地调整宽度和深度。YOLOv6的颈部被表示为Rep-PAN。 </p>
<h3 id="Head"><a href="#Head" class="headerlink" title="Head"></a><strong>Head</strong></h3><p>​    <strong>Effificient decoupled head：</strong> YOLOv5的检测头是一个耦合头，分类和定位分支共享参数，而FCOS和YOLOX的检测头将两个分支解耦，并在每个分支中额外引入两个3×3卷积层来提高性能。在YOLOv6中，我们采用了一种混合信道策略来构建一个更有效的解耦头。具体来说，我们将中间的3个3×3卷积层的数量减少到只有一个。头部的宽度由主干和颈部的宽度乘数共同缩放。这些修改进一步降低了计算成本，以实现更低的推理延迟。 </p>
<p>​    <strong>Achor-free：</strong> Achor-free检测头因其更好的泛化能力和解码预测结果的简单性而脱颖而出。其后处理的时间成本大大降低了。无锚点探测器有两种类型的无锚点检测器：基于锚点的和基于关键点的。<strong>在YOLOv6中，我们采用了基于锚点的范式，其框回归分支实际上预测了从锚点到边界框四边的距离。</strong></p>
<h3 id="Label-Assignment"><a href="#Label-Assignment" class="headerlink" title="Label Assignment"></a>Label Assignment</h3><p>​    标签分配负责在训练阶段为预定义的锚点分配标签。先前的工作提出了各种标签分配策略，从简单的基于iou的策略和内部地面真值方法到其他更复杂的方案。</p>
<p>​    <strong>SimOTA</strong> OTA认为目标检测中的标签分配是一个最优的传输问题。它从全局的角度为每个地面真实对象定义了正/负的训练样本。SimOTA是OTA的一个简化版本，它减少了额外的超参数并保持了性能。在YOLOv6的早期版本中，使用了SimOTA作为标签分配方法。然而，在实践中，<strong>我们发现引入SimOTA会减慢培训过程。而且经常陷入不稳定的训练。因此，我们希望有一个替代SimOTA。</strong> </p>
<p>​    <strong>Task alignment learning</strong> 任务对齐学习（TAL）首次在TOOD中提出，其中设计了一个统一的分类分数和预测框质量的统一度量。用此度量替换IoU以分配对象标签。在一定程度上，缓解了任务（分类和预测框回归）的错位问题。TOOD的另一个主要贡献是关于任务状头（T-head）。T-head堆栈卷积层来构建交互式特性，在此之上使用了任务对齐预测器（TAP）。PP-YOLOE用轻量级ESE注意取代T-head的层注意，形成ET-head。<strong>然而，我们发现ET-head会恶化我们模型的推理速度，它没有精度增益。因此，我们保留了我们的高效解耦头的设计。</strong></p>
<p>​    <strong>此外，我们观察到TAL比SimOTA带来更多的性能改善，稳定训练。因此，我们在YOLOv6中采用TAL作为默认的标签分配策略。</strong></p>
<h3 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h3><p>​    对象检测包含两个子任务：分类和定位，对应于两个损失函数：分类损失和预测框回归损失。对于每个子任务，近年来都有各种不同的损失函数。在本节中，我们将介绍这些损失函数，并描述我们如何为YOLOv6选择最佳的损失函数。 </p>
<h4 id="Classifification-Loss"><a href="#Classifification-Loss" class="headerlink" title="Classifification Loss"></a><strong>Classifification Loss</strong></h4><p>​     提高分类器的性能是优化检测器的关键部分。Focal Loss改进了传统的交叉熵损失，解决了正负样本或硬易样本之间的类不平衡问题。为了解决训练和推理之间质量估计和分类使用不一致的问题，Quality Focal Loss（QFL）进一步扩展了Focal Loss，并将分类评分和定位质量联合表示出来进行分类监督。<strong>而VariFocal Loss (VFL)来源于Focal Loss，但它不对称地处理正样本和负样本。通过考虑不同重要程度的正样本和负样本，它平衡了来自两个样本的学习信号。Poly Loss将常用的分类损失分解为一系列加权多项式基。它在不同的任务和数据集上调整多项式系数，通过实验证明了其优于交叉熵损失和焦点损失。</strong></p>
<p>​    我们评估了YOLOv6上的所有这些高级分类损失，并最终采用了VFL 。</p>
<h4 id="Box-Regression-Loss"><a href="#Box-Regression-Loss" class="headerlink" title="Box Regression Loss"></a>Box Regression Loss</h4><p>​    预测框回归损失提供了重要的学习信号精确的定位边界框。L1损失是早期工作中原始的预测框回归损失。逐渐地，各种设计良好的预测框回归损失已经出现，如iou系列损失和概率损失。</p>
<p>​    <strong>IoU-series Loss</strong> IoU损失回归了一个预测框作为一个整体单位的四个边界。由于它与评价度量的一致性，它已被证明是有效的。IoU有许多变体，如GIoU、DIoU、CIoU、α-IoU和SIoU等，形成了相关的损失函数。我们用GIoU、CIoU和SIoU进行了实验。而SIOU应用于YOLOv6-N和YOLOv6-T，而其他的则使用GIoU。</p>
<p>​    <strong>Probability Loss</strong>  Distribution Focal Loss<strong>（DFL）将预测框位置的基本连续分布简化为一个离散的概率分布。</strong>它在不引入任何其他强先验的情况下考虑数据中的模糊性和不确定性，有助于提高<strong>预测框</strong>的定位精度，特别是在地面-真值盒的边界模糊的情况下。在DFL的基础上，DFLv2 开发了一个轻量级的子网络，以利用分布统计数据与真实定位质量之间的密切相关性，进一步提高了检测性能。<strong>然而，DFL通常比一般的预测框回归多输出17×的回归值，这导致了大量的开销。额外的计算成本明显地阻碍了对小模型的训练。而DFLv2则由于额外的子网络而进一步增加了计算负担。</strong>在我们的实验中，DFLv2在我们的模型上带来了与DFL相似的性能增益。因此，我们只在YOLOv6-M/L中采用DFL。实验细节见第3.3.3节。</p>
<h4 id="Object-Loss"><a href="#Object-Loss" class="headerlink" title="Object Loss"></a><strong>Object Loss</strong></h4><p>​    Object loss首先是在FCOS中提出的，以降低低质量的边界框的得分，以便在后处理中可以过滤掉它们。它也被用于YOLOX来加速收敛和提高网络精度。作为像FCOS和YOLOX这样的无锚框架，我们尝试在YOLOv6中使用ObjectLoss。不幸的是，它并没有带来许多积极的影响。</p>
<h3 id="Industry-handy-improvements"><a href="#Industry-handy-improvements" class="headerlink" title="Industry-handy improvements"></a>Industry-handy improvements</h3><h4 id="More-training-epochs"><a href="#More-training-epochs" class="headerlink" title="More training epochs"></a><strong>More training epochs</strong></h4><p>​    实验结果表明，训练时间越长，探测器就具有进步的性能。<strong>我们将训练从300个epochs延长到400个epochs，以达到更好的收敛性。</strong></p>
<h4 id="Self-distillation"><a href="#Self-distillation" class="headerlink" title="Self-distillation"></a><strong>Self-distillation</strong></h4><p>​    为了进一步提高模型的准确性，同时不引入太多额外的计算成本，<strong>我们采用经典的知识蒸馏技术来最小化教师模型和学生模型之间预测的KL散度。</strong>我们限制教师模型是预先训练的学生模型，因此我们称之为自我蒸馏。请注意，kl-散度通常用于度量数据分布之间的差异。然而，在目标检测中有两个子任务，其中只有分类任务可以直接利用基于kl-散度的知识精馏。由于DFL损失[20]，我们也可以在预测框回归上执行它。知识蒸馏损失可以表述为： </p>
<p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/9ef8cfa56b40480e94f5ce1b36eb5875.png" alt="img"></p>
<p>​    其中<img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/70e48551855e4598af43ca460c462046.png" alt="img">和<img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/956c6028740d47c398f7ef336d6e5ffb.png" alt="img">分别为教师模型和学生模型的类别预测，因此<img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/2421a4e7d6da4247af9d11f7fefe46dc.png" alt="img">和<img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/7628f443735a4c1e9c0a75518ea107e7.png" alt="img">为预测框回归预测。总体损失函数现在可以表述为： </p>
<p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/1879de4162f44bb080647818d6350ce4.png" alt="img"></p>
<p>​    其中，Ldet是用预测和标签计算出的检测损失。引入超参数α来平衡两个损失。在训练的早期阶段，从教师模型那里得到的软标签更容易学习。随着训练的继续，学生模型的表现将与教师模型相匹配，这样硬标签将对学生更有帮助。在此基础上，我们将余弦权值衰减应用于α，以动态调整来自教师的硬标签和软标签的信息。</p>
<h4 id="Gray-border-of-images"><a href="#Gray-border-of-images" class="headerlink" title="Gray border of images"></a><strong>Gray border of images</strong></h4><p>​    我们注意到，<strong>在评估YOLOv5 和YOLOv7 实现中的模型性能时，在每个图像周围都设置了一个半步幅的灰色边界。</strong>虽然没有添加任何有用的信息，但它有助于检测图像边缘附近的物体。这个技巧也适用于YOLOv6。 <strong>然而，额外的灰度像素明显降低了推理速度。如果没有灰色边框，YOLOv6的性能就会恶化</strong>。我们假设该问题与Mosaic augmentation中的灰色边界填充有关。实验在关闭mosaic增强在最后的epochs进行验证。在这方面，我们改变了灰度边界的面积，并将具有灰度边界的图像的大小直接调整为目标图像的大小。结合这两种策略，我们的模型可以保持甚至提高性能，而不降低推理速度。</p>
<h3 id="Quantization-and-Deployment"><a href="#Quantization-and-Deployment" class="headerlink" title="Quantization and Deployment"></a><strong>Quantization and Deployment</strong></h3><p>​    对于工业部署，通常的做法是采用量化以进一步加快运行时，而不会影响太多性能。训练后量化（PTQ）直接用一个小的校准集对模型进行量化。而量化感知训练（QAT）进一步提高了对训练集的访问的性能，这通常与蒸馏联合使用。<strong>然而，由于在YOLOv6中大量使用重新参数化块，以前的PTQ技术不能产生高性能，而在训练和推理过程中匹配假量化器时，很难合并QAT。</strong>我们在这里展示了在部署期间的陷阱和我们的解决方法。 </p>
<h4 id="Reparameterizing-Optimizer"><a href="#Reparameterizing-Optimizer" class="headerlink" title="Reparameterizing Optimizer"></a><strong>Reparameterizing Optimizer</strong></h4><p>​    RepOptimizer<strong>在每个优化步骤中提出梯度重新参数化。</strong>该技术也能很好地解决了基于再参数化的模型的量化问题。因此，我们以这种方式重建了YOLOv6的重新参数化块，并使用重新优化器对其进行训练，以获得对PTQ友好的权值。特征图的分布很窄，这大大有利于量化过程。 </p>
<p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/408b78b8e84d40c590319c35205855c0.png" alt="img"></p>
<h4 id="Sensitivity-Analysis"><a href="#Sensitivity-Analysis" class="headerlink" title="Sensitivity Analysis"></a><strong>Sensitivity Analysis</strong></h4><p>​    我们通过将量化敏感操作部分转换为浮点计算，进一步提高了PTQ的性能。为了获得灵敏度分布，我们常用了几个指标，即均方误差（MSE）、信噪比（SNR）和余弦相似度。通常，为了进行比较，可以选择输出特征映射（在激活某一层之后）来计算有量化和没有量化的这些度量。作为一种替代方法，它也可以通过开关特定层的量化来计算验证AP。</p>
<p>​    我们在使用重新优化器训练的YOLOv6-S模型上计算所有这些指标，并选择前6个敏感层，以浮动形式运行。敏感性分析的完整图表见B.2。</p>
<p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/88873d7d91d945c79f54014e725fbe13.png" alt="img"> </p>
<h4 id="Quantization-aware-Training-with-Channel-wise-Distillation"><a href="#Quantization-aware-Training-with-Channel-wise-Distillation" class="headerlink" title="Quantization-aware Training with Channel-wise Distillation"></a><strong>Quantization-aware Training with Channel-wise</strong> <strong>Distillation</strong></h4><p>​    <strong>在PTQ不足的情况下，我们建议涉及量化感知训练（QAT）来提高量化性能。为了解决在训练和推理过程中假量化器的不一致性问题，有必要在重新优化器上建立QAT。</strong>此外，在YOLOv6框架内采用了通道蒸馏（后来称为CW蒸馏），如图5所示。这也是一种自蒸馏的方法，其中教师网络是在fp32精度上的学生模型。参见第3.5.1节中的实验。 </p>
<p><img src="/2023/06/18/YOLOv6-A-Single-Stage-Object-Detection-Framework-for-Industrial-Applications/cda38d2ac2884096b174b6dc66edbb6e-168707903752712.png" alt="cda38d2ac2884096b174b6dc66edbb6e"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/" class="post-title-link" itemprop="url">RepVGG Making VGG-style ConvNets Great Again</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-06-18 17:02:39 / 修改时间：17:02:54" itemprop="dateCreated datePublished" datetime="2023-06-18T17:02:39+08:00">2023-06-18</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="RepVGG-Making-VGG-style-ConvNets-Great-Again"><a href="#RepVGG-Making-VGG-style-ConvNets-Great-Again" class="headerlink" title="RepVGG: Making VGG-style ConvNets Great Again"></a>RepVGG: Making VGG-style ConvNets Great Again</h2><p>主要贡献：提出了一种简单但功能强大的卷积神经网络结构，其网络结构，在推理时只具有3x3卷积和ReLU，在训练时具有多分支拓扑结构，通过结构重参数化技术实现训练时间和推理时间的解耦，并命名为RepVGG。</p>
<h3 id="对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下："><a href="#对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下：" class="headerlink" title="对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下："></a>对于较为复杂的网络（ResNet的残差块以及Inception的分支连接），其精度往往较好，但其本身存在的问题如下：</h3><ul>
<li>会降低模型的推理速度并且减少内存利用率</li>
<li>有些节点及算子会增加内存消耗并且对别的设备不友好。</li>
</ul>
<p>论文中提到，大部分学者提到FLOPs（浮点运算的数量）会影响推理速度，但是论文中作者做了实验发现FLOPs对模型的速度并不是强相关。</p>
<p>作者提出的RepVGG，其具有以下优点：</p>
<ul>
<li>该模型具有类似VGG的拓扑结构，没有任何分支，这意味着每一层都将其唯一前一层的输出作为输入，并将输出馈送到其唯一的后一层。</li>
<li>该模型的主体部分仅使用3 × 3的conv和ReLU。</li>
<li>模型的具体架构(包括具体的深度和层宽度)的实例化没有模型结构的自动搜索，手工细化，复合缩放，也没有其他代价较大的设计。</li>
</ul>
<h3 id="作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。"><a href="#作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。" class="headerlink" title="作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。"></a>作者认为，多分支架构可以看作为许多较浅模型的隐式集成，并且具有较好的性能水平。</h3><p>针对多分支架构的优点集中于训练上，而不希望用于推理上，故提出重参数化的方法来解耦训练时的多分支结构和推理时的简单架构，即意味着通过转换其参数将架构从一个转换到另一个。</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/4H7{5]]_TNU%XI%5PPH9KA9.png" alt="img"></p>
<p>如上图中(b)和(c)所示，即为转换之后的RepVGG和转换之前的RepVGG。其将分支看作退化的1x1卷积，进一步看作退化的3x3卷积。从而可以从(b)中的模型架构转变为(c)中的模型架构，可以用3x3卷积、BN、1x1卷积等模块进行原模型的等效替换。从而提升计算速度。</p>
<h3 id="本文的核心贡献点如下："><a href="#本文的核心贡献点如下：" class="headerlink" title="本文的核心贡献点如下："></a>本文的核心贡献点如下：</h3><ul>
<li>我们提出了RepVGG，这是一种简单的架构，与最先进的技术相比，具有良好的速度-精度权衡。</li>
<li>我们建议使用结构重参数化将训练时间的多分支拓扑与推理时间的平面结构解耦。</li>
<li>我们展示了RepVGG在图像分类和语义分割方面的有效性，以及实现的效率和易用性。</li>
</ul>
<h3 id="如何实现结构重参数化："><a href="#如何实现结构重参数化：" class="headerlink" title="如何实现结构重参数化："></a>如何实现结构重参数化：</h3><p>在上述提到，RepVGG在训练时每一层都有三个分支，分别是identify，1x1，3x3，模型训练时，输出$ y=x+g(x)+f(x) $，每一层就需要3个参数块，对于n层网络，就需要$3*n$个参数块。所以我们需要重参数化，会使得推理时模型参数量小。</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/aa1ad31949b54e76b0a282fab915478f.png" alt="img"></p>
<p>上图中的过程即为将训练好的多分支模型转换为单分支模型，从而达到推理时的高性能</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对于重参数化的实现主要存在两个问题：</span><br><span class="line">第一个问题，在每个卷积后都接上一个BN，怎么将卷积和BN融合。</span><br><span class="line">第二个问题，存在不同大小的卷积，怎么将几个不同大小的卷积融合在一起。</span><br></pre></td></tr></table></figure>
<p>对于第一个问题，在每个卷积后都接上一个BN，怎么将卷积和BN融合。</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-84cdab58644fcbcafb3c690c1669b879_1440w.webp" alt="v2-84cdab58644fcbcafb3c690c1669b879_1440w"></p>
<p>这其实就是一个卷积层，只不过权重考虑了BN的参数 我们令：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-b438e3a2ee316a6054a4e4c45443fef3_1440w.webp" alt="img"></p>
<p>最终的融合结果即为：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-cd0d2de067e4850fe4fafce70f58acf1_1440w.webp" alt="img"></p>
<h3 id="2-2-2-conv-3x3和conv-1x1合并"><a href="#2-2-2-conv-3x3和conv-1x1合并" class="headerlink" title="2.2.2. conv_3x3和conv_1x1合并"></a>2.2.2. conv_3x3和conv_1x1合并</h3><p> 这里为了详细说明下，假设输入特征图特征图尺寸为(1, 2, 3, 3)，输出特征图尺寸与输入特征图尺寸相同，且stride=1，下面展示是conv_3x3的卷积过程：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-89854f076457c9c03b733a389db96993_1440w.webp" alt="img"></p>
<p> conv_3x3卷积过程大家都很熟悉，看上图一目了然，首先将特征图进行pad=kernel_size//2，然后从左上角开始(上图中红色位置)做卷积运算，最终得到右边output输出。下面是conv_1x1卷积过程：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-88962d2f0fc8f1371d0d521c04c2a57d_1440w.webp" alt="img"></p>
<p> 同理，conv_1x1跟conv_3x3卷积过程一样，从上图中左边input中红色位置开始进行卷积，得到右边的输出，观察conv_1x1和conv_3x3的卷积过程，可以发现他们都是从input中红色起点位置开始，走过相同的路径，因此，将conv_3x3和conv_1x1进行融合，只需要将conv_1x1卷积核padding成conv_3x3的形式，然后于conv_3x3相加，再与特征图做卷积(这里依据卷积的可加性原理)即可，也就是conv_1x1的卷积过程变成如下形式：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-b7409c315f10a158331bf90fcf32efd6_1440w.webp" alt="img"></p>
<h3 id="2-2-3-identity-等效为特殊权重的卷积层"><a href="#2-2-3-identity-等效为特殊权重的卷积层" class="headerlink" title="2.2.3. identity 等效为特殊权重的卷积层"></a>2.2.3. identity 等效为特殊权重的卷积层</h3><p> identity层就是输入直接等于输出，也即input中每个通道每个元素直接输出到output中对应的通道，用一个什么样的卷积层来等效这个操作呢，我们知道，卷积操作必须涉及要将每个通道加起来然后输出的，然后又要保证input中的每个通道每个元素等于output中，从这一点，我们可以从PWconv想到，只要令当前通道的卷积核参数为1，其余的卷积核参数为0，就可以做到；从DWconv中可以想到，用conv_1x1卷积且卷积核权重为1，就能保证每次卷积不改变输入，因此，identity可以等效成如下的conv_1x1的卷积形式：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-b05e6fa96bd642c1da2d36d39a543d7a_1440w.webp" alt="img"></p>
<p>从上面的分析，我们进一步可以将indentity -&gt; conv_1x1 -&gt; conv_3x3的形式，如下所示：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-bc97e575d5007645901830109828a36f_1440w.webp" alt="img"></p>
<p> 上述过程就是对应论文中所属的下述从step1到step2的变换过程，涉及conv于BN层融合，conv_1x1与identity转化为等价的conv_3x3的形式：</p>
<p><img src="/2023/06/18/RepVGG-Making-VGG-style-ConvNets-Great-Again/v2-f5ce0b89a10aa36223275dccd6327cbe_1440w.webp" alt="img"></p>
<p> 结构重参数化的最后一步也就是上图中step2 -&gt; step3， 这一步就是利用卷积可加性原理，将三个分支的卷积层和bias对应相加组成最终一个conv<em>3x3的形式即可。</em><br>这里，大家可能既然把BN，identity，conv_1x1和conv_3x3都融合在一起了，为什么不干脆把ReLU也融合进去呢？其实也是可以将ReLU层进行融合的，<strong>但是需要进行量化</strong>，<strong>conv输出tensor的值域直接使用relu输出的值阈（同时对应计算Ｓ和Z），就可以完成conv和relu合并。无量化动作的优化是无法完成conv+relu的合并*</strong>。这里的知识请大家参考论文：<em><br><em>*<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.05877">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a>。</em></em></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凯">
      <meta itemprop="description" content="选择大于努力">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凯_kaiii">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/" class="post-title-link" itemprop="url">A New Baseline for Single-/Cross-Modality Re-ID</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-23 20:46:36 / 修改时间：21:04:48" itemprop="dateCreated datePublished" datetime="2023-04-23T20:46:36+08:00">2023-04-23</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="AGW-A-New-Baseline-for-Single-Cross-Modality-Re-ID"><a href="#AGW-A-New-Baseline-for-Single-Cross-Modality-Re-ID" class="headerlink" title="AGW: A New Baseline for Single-/Cross-Modality Re-ID"></a>AGW: A New Baseline for Single-/Cross-Modality Re-ID</h1><p>其为综述Deep Learning for Person Re-identification:A Survey and Outlook 中提出的方法</p>
<p> AGW是在BagTricks的基础之上进行设计研究的，其主要包括以下三个主要的提升组件：</p>
<ul>
<li>Non-local Attention (Att) Block</li>
<li>Generalized-mean (GeM) Pooling.</li>
<li>Weighted Regularization Triplet (WRT) loss</li>
</ul>
<h2 id="Non-local-Attention-Att-Block"><a href="#Non-local-Attention-Att-Block" class="headerlink" title="Non-local Attention (Att) Block"></a>Non-local Attention (Att) Block</h2><p> 注意力的概念在ReID的学习中起到至关重要的作用，使用强大的非局部注意力块来获得各个位置特征的加权和。公式如下：$z_i = W_z ∗ φ(x_i) + x_i $，其中$W_z$是需要学习的权重矩阵，$φ()$表示非局部的操作，$+x_i$构建了一个残差策略。详情参见《Non-local neural networks》</p>
<h2 id="Generalized-mean-GeM-Pooling"><a href="#Generalized-mean-GeM-Pooling" class="headerlink" title="Generalized-mean (GeM) Pooling."></a>Generalized-mean (GeM) Pooling.</h2><p>ReID任务可视为细粒度的实例检索，广泛使用的max-pooling或average-pooling无法捕获领域特定的鉴别特征。所以针对该问题采用可学习的池化层，称为Generalized-mean (GeM) Pooling，公式如下:</p>
<p><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/image-20230418144447925.png" alt="image-20230418144447925"></p>
<p>$p_k$是一个池化超参数，可以在反向传播过程中学习，$p_k→∞$时近似最大池化，在$p_k = 1$时近似平均池化。详情参见《Fine-tuning cnn image retrieval with no human annotation》。可视为在最低维度上，对每个元素的p次方求均值再开p次方。</p>
<h2 id="Weighted-Regularization-Triplet-WRT-loss"><a href="#Weighted-Regularization-Triplet-WRT-loss" class="headerlink" title="Weighted Regularization Triplet (WRT) loss"></a>Weighted Regularization Triplet (WRT) loss</h2><p>除了使用基于softmax的交叉熵之外，还使用了另一个加权正则化三元组损失。<br><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/20201026220254980.png" alt="在这里插入图片描述"><br>避免引入了margin参数，类似于《Multi-similarity loss with general pair weighting for deep metric learning》</p>
<h2 id="完整流程如下所示"><a href="#完整流程如下所示" class="headerlink" title="完整流程如下所示"></a>完整流程如下所示</h2><p><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70#pic_center.png" alt="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70"></p>
<p><strong>AGW在跨模态行人重识别中的效果：</strong><br><img src="/2023/04/23/AGW%20A%20New%20Baseline%20for%20Single-Cross-Modality%20Re-ID/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70#pic_center-16818009779205.png" alt="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzMxMjUzNTcz,size_16,color_FFFFFF,t_70"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">凯</p>
  <div class="site-description" itemprop="description">选择大于努力</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">31</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">凯</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
